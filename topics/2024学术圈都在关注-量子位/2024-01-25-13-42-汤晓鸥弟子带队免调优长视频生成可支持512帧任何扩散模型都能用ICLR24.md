# 汤晓鸥弟子带队：免调优长视频生成，可支持512帧！任何扩散模型都能用｜ICLR'24

文章作者: 量子位
发布时间: 2024-01-25 13:42
发布地: 北京
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247714611&idx=4&sn=4b18a6159e84800fe3161943bab43836&chksm=e8df3041dfa8b9571ee6b4523eeaa57a66988e4c9675aff224cbb0caee5e410896754a59c873#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtD9VVK9G8D61n4N6fMQfBb0hFmQpW5VC0j9GB0B4QENQgbEqobYMC1ibS3r42vR4dl6thjJDicUgF1w/300

##### 丰色 发自 凹非寺  
量子位 | 公众号 QbitAI

想要AI生成更长的视频？

现在，有人提出了一个效果很不错的**免调优** 方法，直接就能作用于预训练好的视频扩散模型。

它最长可支持**512帧** （假设帧率按30fps算，理论上那就是能生成约17秒长的作品了）。

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtD9VVK9G8D61n4N6fMQfBb0Tto0hNEQQ46AHhRnmYa5FPSNkFjWN8icSRs3DcLHfLkmNQbWNK3U6pg/640?wx_fmt=gif&from=appmsg)

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtD9VVK9G8D61n4N6fMQfBb05xNyEPLdVmeceoD1icSzMeSAX4KwuEKPEcDfg2EeELSaoS4Gbsc1Bww/640?wx_fmt=gif&from=appmsg)

可应用于任何视频生成模型，比如AnimateDiff、LaVie等等。

以及还能支持**多文本生成** ，比如可以让骆驼一会跑一会停：

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtD9VVK9G8D61n4N6fMQfBb07rKClDjba4ibqTSDMn9PnwcMkSCHQScIc47Yhzb7v19uxNEMEp7dniaQ/640?wx_fmt=gif&from=appmsg)

（提示词：”A camel **running** on the snow field.” -> “…… **standing** ……”）

这项成果来自腾讯AI Lab、南洋理工大学以及港科大，入选了ICLR 2024。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtD9VVK9G8D61n4N6fMQfBb0ibO6XIicibfKYfJdLnNUyr23qcvEYz1skKliag65zQich6yvoow8tJb882Q/640?wx_fmt=png&from=appmsg)

值得一提的是，与此前业内性能最佳的同类方法带来255%的额外时间成本相比，它仅产生**约17%的时间成本** ，因此直接可以忽略不计。

可以说是成本和性能两全了～

具体来看看。

## 通过重新调度噪声实现

该方法主要解决的是两个问题：

一是现有视频生成通常在有限数量的帧上完成训练，导致推理过程中无法生成高保真长视频。

二是这些模型还仅支持单文本生成（即使你给了“一个人睡在桌子上，然后看书”这种提示词，模型也只会响应其中一个条件），而应用到现实中其实是需要多文本条件，毕竟视频内容是会随时间不断变化的。

在此，作者首先分析视频扩散模型的时间建模机制，并研究了初始噪声的影响，提出免调优、实现更长视频推理的**FreeNoise** 。

具体而言，以VideoLDM模型为例，它生成的帧不仅取决于当前帧的初始噪声，还取决于所有帧的初始噪音。

这意味着，由于临时注意力层负责促成整个交互，所以对任何帧的噪声重新采样都会显著影响其它帧。

产生的问题就是我们要想保持原视频主要内容的同时引入新东西就很难。

在此，作者检查VideoLDM的时间建模机制发现，其中的时间注意力模块是顺序无关的，而时间卷积模块是顺序相关的。

实验观察表明，每帧噪声是决定视频整体外观的基础，而它们的时间顺序会影响建立在该基础上的内容。

受此启发，作者提出了FreeNoise，其关键思想是构建一个具有长程相关性的噪声帧序列，并通过基于窗口的融合对其进行时间关注。

它主要包括**两个关键设计** ：局部噪声去除和基于窗口的注意力融合。

通过将局部噪声混洗应用于固定随机噪声帧序列以进行长度扩展，作者实现了具有内部随机性和长程相关性的噪声帧序列。

同时，基于窗口的注意力融合使预先训练的时间注意力模块能够处理任何较长的帧。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtD9VVK9G8D61n4N6fMQfBb0h02icicCCvQqMEBRoLO54SESJVSBF4tGj7jfZTdm451yeEmYVZp76ovA/640?wx_fmt=png&from=appmsg)

并且最重要的是，重叠窗口切片和合并操作只发生在时间注意力上，而不会给VideoLDM的其他模块带来计算开销，这也大大提高了计算效率。

接下来，为了解决多文本条件问题，作者则提出了**动作注入** （Motion Injection）方法。

其核心利用的是扩散模型不同步骤在去噪过程中恢复不同级别信息（图像布局、物体形状和精细视觉细节）的特性。

在模型完成上一个动作之后，该方法就在与物体形状相关的时间步长内逐渐注入新的运动。

这样的操作，既保证多提示长视频生成，又具备很好的视觉连贯性。

## 超越此前最先进的无调优方法

首先来看长视频生成的结果。

可以看到，FreeNoise诠释“宇航服吉娃娃”和“熊猫吃披萨”这两个场景最为连贯自然。

相比之下，直接推理的（最左列）的狗有严重伪影且没有生成背景，**Gen-L-Video**
（此前最先进的无调优方法）则由于无法保持长距离的视觉一致性，存在明显内容突变。

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtD9VVK9G8D61n4N6fMQfBb0avIqyOjvvo09UM5icRk2QQ8yBsAJawewRctwAMOppLibADPaCFca7Lqw/640?wx_fmt=gif&from=appmsg)

定性结果也用数据证明了FreeNoise的效果：

其中CLIP-SIM的得分代表该方法做到了良好的内容一致性。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtD9VVK9G8D61n4N6fMQfBb0evAOE1WxQcrob67h0xHWNlO9OBX3aNFGO75dOHQroTyVQkBYaNY3NA/640?wx_fmt=png&from=appmsg)

其次是多文本条件生成效果。

可以看到该方法（中间列和最右列）可以实现连贯的视觉显示和运动：

骆驼从奔跑逐渐变为站立，**远处的山脉一直保持同样的外观** 。

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtD9VVK9G8D61n4N6fMQfBb0P2IsbwxWIrhgVKOCR9veicLsHQdhjGx5sniaar5oCpgsuesnvDZqgQVA/640?wx_fmt=gif&from=appmsg)

定性结果如下：

可以看到该方法在内容一致性、视频质量和视频文本对齐都实现SOTA，且与第二名拉开的差距几乎**达到两倍之多** 。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtD9VVK9G8D61n4N6fMQfBb0jdQ7xAm0iaIYicibmaPt2TA0lHWe5BibkwWetzR34aC0LLgxkaNup39xnw/640?wx_fmt=png&from=appmsg)

最后，再给大家展示一下FreeNoise用在潜视频扩散模型AnimateDiff、LaVie上的效果。

AnimateDiff：

第一列为原效果，第二列为应用后的效果。

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtD9VVK9G8D61n4N6fMQfBb0wJ1dhFq9Bsc5xU62I9jhU8By2jQWorH38wIFyv18JoDfMQkibpKZsbg/640?wx_fmt=gif&from=appmsg)

LaVie：  

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtD9VVK9G8D61n4N6fMQfBb0lZoRsDM64Mc0YW7jcuBJ5icZsgcNUIq98IlzmicIOJI378kmlwn3Hz9g/640?wx_fmt=gif&from=appmsg)

效果提升都是肉眼可见的～

哦对，还有生成的满打满算512帧的视频，大家觉得效果如何呢：  

## 通讯作者之一是汤晓鸥弟子

本文一共7位作者。

一作为南洋理工大学计算机科学与工程学院博士生**邱浩楠** 。

他的研究方向为AIGC、对抗性机器学习和深伪检测，本科毕业于港中文。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtD9VVK9G8D61n4N6fMQfBb07trkwYfJ9q0j9Nozc4cBedFNvyF8R4f0ibAoLo7T29wzrXtbETqCJXg/640?wx_fmt=png&from=appmsg)

通讯作者有两位：

一位是腾讯AI Lab视觉计算中心研究员**Menghan Xia** 。

他的研究方向为计算机视觉和深度学习，尤其是图像/视频的生成和翻译。

Menghan Xia博士毕业于港中文，本硕先后毕业于武汉大学的摄影测量与遥感学、模式识别与智能系统专业。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtD9VVK9G8D61n4N6fMQfBb0qnfZontelXzI6eFQ2QEX4GkmpEQFgnOibqJmmAUM9kArj4eLIrHT2Og/640?wx_fmt=png&from=appmsg)

另一位是南洋理工大学计算机科学与工程学院助理教授**刘子纬** 。

他2017年博士毕业于港中文，师从**汤晓鸥教授和王晓刚教授** 。

毕业后曾在UC伯克利做博士后、港中文担任四年研究员。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtD9VVK9G8D61n4N6fMQfBb0wkicMWW8ymayMtOZXYm1XUOnIV9aKQeDB8Dnf6fhzlEjOHjSUyTS8sw/640?wx_fmt=png&from=appmsg)

论文：  
https://arxiv.org/abs/2310.15169  
Huggingface体验demo：  
https://huggingface.co/spaces/MoonQiu/LongerCrafter

— **完** —

**点这里 👇关注我，记得标星哦～**

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg)

