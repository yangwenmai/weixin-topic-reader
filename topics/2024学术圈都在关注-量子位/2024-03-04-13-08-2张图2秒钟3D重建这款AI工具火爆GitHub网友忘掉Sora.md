# 2张图2秒钟3D重建！这款AI工具火爆GitHub，网友：忘掉Sora

文章作者: 量子位
发布时间: 2024-03-04 13:08
发布地: 北京
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247719361&idx=3&sn=b6f34723ccee2f40a1991adc29660181&chksm=e8df2eb3dfa8a7a564abb4d2b4a6e03f441c5cfe43d70c5ce3acb68a5f2bca781712dc7781cd#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtBrlCC9GsYQcNwviaoibrlBksl117j6UMnWiahgY5FPQkbOWFl6G6pxFE8DvJ2ahuBMbKj3Uibx2xK1lw/300

##### 丰色 发自 凹非寺  
量子位 | 公众号 QbitAI

**只需2张图片** ，无需测量任何额外数据——

当当，一个完整的3D小熊就有了：

  
![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtBrlCC9GsYQcNwviaoibrlBks8fSKPfDTl1YCxr1hOIvtdaAd4SH44ZZ47utVBTdmMLmLyOk0f1XVKw/640?wx_fmt=gif&from=appmsg)

这个名为**DUSt3R** 的新工具，火得一塌糊涂，才上线没多久就登上**GitHub热榜第二** 。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBrlCC9GsYQcNwviaoibrlBksCV4gFQUhrUeLlsXCoyicficwCo0A1obqGsjibgILCgSCgVtVpicibAjIm3Q/640?wx_fmt=png&from=appmsg)

有**网友实测** ，拍两张照片，真的就重建出了他家的厨房，整个过程**耗时不到2秒钟** ！

（除了3D图，深度图、置信度图和点云图它都能一并给出）

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtBrlCC9GsYQcNwviaoibrlBkslU7m8dLFCPr0uUibxlnqZ7y1tHHIkHdTGe6QaOD6tvK59Q5bQUaILiaA/640?wx_fmt=gif&from=appmsg)

惊得这位朋友直呼：

> 大伙**先忘掉sora** 吧，这才是我们真正看得见摸得着的东西。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBrlCC9GsYQcNwviaoibrlBksaR8sTJ2d3D0O7fC8JHAnOCYgWcIxWNfpejp8u6xw7ib1BQic7kiaVklOQ/640?wx_fmt=png&from=appmsg)

实验显示，DUSt3R在单目/多视图深度估计以及相对位姿估计三个任务上，均取得SOTA。

作者团队（来自芬兰阿尔托大学+NAVER LABS人工智能研究所欧洲分所）的“宣语”也是气势满满：

> 我们就是要让天下没有难搞的3D视觉任务。

所以，它是如何做到？

## “all-in-one”

对于多视图立体重建（MVS）任务来说，第一步就是估计相机参数，包括内外参。

这个操作很枯燥也很麻烦，但对于后续在三维空间中进行三角测量的像素不可或缺，而这又是几乎所有性能比较好的MVS算法都离不开的一环。

在本文研究中，作者团队引入的DUSt3R则完全采用了截然不同的方法。

它**不需要任何相机校准或视点姿势的先验信息** ，就可完成任意图像的密集或无约束3D重建。

在此，团队将成对重建问题表述为点图回归，统一单目和双目重建情况。

在提供超过两张输入图像的情况下，通过一种简单而有效的全局对准策略，将所有成对的点图表示为一个共同的参考框架。

如下图所示，给定一组具有未知相机姿态和内在特征的照片，DUSt3R输出对应的一组点图，从中我们就可以直接恢复各种通常难以同时估计的几何量，如相机参数、像素对应关系、深度图，以及完全一致的3D重建效果。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBrlCC9GsYQcNwviaoibrlBksicBybhkuXiboCPicp2MpfqEyflsYkZ1Sq9fohFibPbpXdjEIhS8eBEfL6Q/640?wx_fmt=png&from=appmsg)

（作者提示，DUSt3R也适用于单张输入图像）

具体网络架构方面，DUSt3R基于的是**标准Transformer编码器和解码器**
，受到了CroCo（通过跨视图完成3D视觉任务的自我监督预训练的一个研究）的启发，并采用简单的回归损失训练完成。

如下图所示，场景的两个视图（I1，I2）首先用共享的ViT编码器以连体（Siamese）方式进行编码。

所得到的token表示（F1和F2）随后被传递到两个Transformer解码器，后者通过交叉注意力不断地交换信息。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBrlCC9GsYQcNwviaoibrlBksia6gW84p114dBHgaLnGCdicyC2wB0KFZvsgf2icPVQziczPDrwEHkDp6Gg/640?wx_fmt=png&from=appmsg)

最后，两个回归头输出两个对应的点图和相关的置信图。

重点是，这两个点图都要在第一张图像的同一坐标系中进行表示。

## 多项任务获SOTA

实验首先在7Scenes（7个室内场景）和Cambridge
Landmarks（8个室外场景）数据集上评估DUSt3R在绝对姿态估计任务上性能，指标是平移误差和旋转误差（值越小越好）。

作者表示，与现有其他特征匹配和端到端方法相比，DUSt3R表现算可圈可点了。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBrlCC9GsYQcNwviaoibrlBksq9KDB6ib1WmGibOugoDwLJWucNTT5FO2JBh7ib4HSXh27wZSH9SyY5UGQ/640?wx_fmt=png&from=appmsg)

因为它一从未接受过任何视觉定位训练，二是在训练过程中，也没有遇到过查询图像和数据库图像。

其次，是在10个随机帧上进行的多视图姿态回归任务。结果DUSt3R在两个数据集上都取得了最佳效果。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBrlCC9GsYQcNwviaoibrlBksWMDWN35UcCH4smhDLbn2yOWwF7HNVCk8F3WiaorgwLmOicuFDmeBLe2A/640?wx_fmt=png&from=appmsg)

而单目深度估计任务上，DUSt3R也能很好地hold室内和室外场景，性能优于自监督基线，并与最先进的监督基线不相上下。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBrlCC9GsYQcNwviaoibrlBksGJj20fic8eCW5BpnVEK9csicjwTWQ0o2MCMhMYklBouhewUeSlGCBeRw/640?wx_fmt=png&from=appmsg)

在多视图深度估计上，DUSt3R的表现也可谓亮眼。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBrlCC9GsYQcNwviaoibrlBksEuib7dCB3rFtCqdJTEbwFZktWtvexwYqb2jNERjkpAAylAneIP6dKYA/640?wx_fmt=png&from=appmsg)

以下是两组官方给出的3D重建效果，再给大伙感受一下，都是仅输入两张图像：

（一）

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBrlCC9GsYQcNwviaoibrlBkswQId7xjn5ydxf8ktpxj38ricWR0MRiabhQvFOwMXQf5xQNS88yWHVVLQ/640?wx_fmt=png&from=appmsg)

（二）

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBrlCC9GsYQcNwviaoibrlBks1WL9IkZKjw2caEzto2YIw9NczofnkE1sj1YZ5bmib6GGxYubMPCFPWQ/640?wx_fmt=png&from=appmsg)

## 网友实测：两张图无重叠也行

有网友给了DUSt3R两张没有任何重叠内容的图像，结果它也在几秒内输出了准确的3D视图：

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtBrlCC9GsYQcNwviaoibrlBksWiaJvaP896UAl7CdJsXibf3otnI3tibzn2BibAloj0EZYKQGBnbkFK3kdQ/640?wx_fmt=gif&from=appmsg)

（图片是他的办公室，所以肯定没在训练中见过）

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBrlCC9GsYQcNwviaoibrlBksH4ZozttKiaxz86U4suwbSgAEicW1noqmcWV2ykcKpW3qFe5YAeCVwIAA/640?wx_fmt=png&from=appmsg)

对此，有网友表示，这意味着该方法不是在那进行“客观测量”，而是表现得更像一个AI。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBrlCC9GsYQcNwviaoibrlBksQyUxe6iatHxdAXDOAmpDOmYCncLJ4VYjI1jXRibupT3kt6L8AqfYzFAg/640?wx_fmt=png&from=appmsg)

除此之外，还有人好奇**当输入图像是两个不同的相机拍的** 时，方法是否仍然有效？

有网友还真试了，答案是**yes**!

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBrlCC9GsYQcNwviaoibrlBksF7RmdsWaEqKh9ocwxF1FoG3ndlPN5eCwCh1LOCn630x1Q76yldjmJQ/640?wx_fmt=png&from=appmsg)

传送门：  
[1]论文https://arxiv.org/abs/2312.14132  
[2]代码https://github.com/naver/dust3r  
参考链接：  
[1]https://dust3r.europe.naverlabs.com/  
[2]https://twitter.com/janusch_patas/status/1764025964915302400

— **完** —

**报名中！**

**2024年值得关注的AIGC企业 &产品**

量子位正在评选**2024年最值得关注的AIGC企业** 、 **2024年最值得期待的AIGC产品** 两类奖项，欢迎[报名评选]()！

评选报名**截至2024年3月31日![](https://res.wx.qq.com/t/wx_fed/we-
emoji/res/v1.3.10/assets/newemoji/LetMeSee.png)**

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC7IzBlicP1jwLsfiaw2A2ibBoWRgd47kXexFUOSSzXn5f9fDcza39rny2BgqyDQkDrSoLCDh3Ag7XwA/640?wx_fmt=png&from=appmsg)

**中国AIGC产业峰会**
同步火热筹备中，了解更多请戳：[Sora时代，我们该如何关注新应用？一切尽在中国AIGC产业峰会](http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247718372&idx=3&sn=b89d20b431d783c185143da7c8948372&chksm=e8df2296dfa8ab8021659abb68c594c4ebe5b2907d12777771057499c61143c2cdaa8b3269b3&scene=21#wechat_redirect)

商务合作请联络微信：18600164356 徐峰  

活动合作请联络微信：18801103170 王琳玉

  

**点这里 👇关注我，记得标星噢**

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg)

