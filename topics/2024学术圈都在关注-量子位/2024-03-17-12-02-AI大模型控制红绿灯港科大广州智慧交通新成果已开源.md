# AI大模型控制红绿灯，港科大（广州）智慧交通新成果已开源

文章作者: 量子位
发布时间: 2024-03-17 12:02
发布地: 北京
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247721173&idx=3&sn=d912ed2fde65f7cf3d1f258c825d31a4&chksm=e8dfd7a7dfa85eb1d150f7ad12a9cb6630aa0fd12374f855dcc00362e36bb2c9691c4c968332#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtBhkedL8zOhmgDubicQiax5aXCUe0wwT5nNkIaNYCHUqAWwuRMHwiaCBsEorvFqNjkSq8jDF9keqBCNw/300

##### LLMLight团队 投稿  
量子位 | 公众号 QbitAI

大模型“上路”，干起了交通信号控制（TSC）的活～

模型名为**LightGPT** ，以排队及不同区段快要接近信号灯的车辆对路口交通状况分析，进而确定最好的信号灯配置。

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtBhkedL8zOhmgDubicQiax5aXY1hQYQV7TTeXMwrfoHQkm8Y9xvaJZjsuWkoSzW0x1UAKqxQ3833V3w/640?wx_fmt=gif&from=appmsg)

该模型由香港科技大学（广州）的研究团队提出，其背后关键是一个名为**LLMLight** 的框架。

该框架向智能体提供详细的实时交通状况，并**结合先验知识构成提示** ，利用大模型卓越的泛化能力，采用符合人类直觉的推理和决策过程来实现有效的交通控制。

在**九个交通流数据集** 上的实验证明了LLMLight框架的有效性、泛化能力和可解释性。

具体来说，在真实数据集上，LLMLight在所有基准测试中始终达到了SOTA或与经典强化学习等方法同等的性能水平，并且拥有比后者更为强大的**泛化性** 。

同时，LLMLight还能在决策时提供背后的分析逻辑，这一**可解释性** 实现了信号灯控制的透明化。

TSC垂类大模型LightGPT在此任务上的决策能力显著优于GPT-4。

即便在济南、杭州、纽约等复杂路网下，也展示出突出性能。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBhkedL8zOhmgDubicQiax5aXGcicqyECBKwmJ7JF8XvF08AEtZv8CH9Ouvbia0p43hDaSeO9I93F9fKg/640?wx_fmt=png&from=appmsg)

目前，LLMLight框架、交通信号灯控制垂类大模型LightGPT已开源。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBhkedL8zOhmgDubicQiax5aXWho5icDhlhZ7ic9TnEnyPsicQuXFsQeRTIZU7EQ3eAUNb2MmbE7GFKt8w/640?wx_fmt=png&from=appmsg)

## LLM应用于TSC有何挑战？

交通信号控制（TSC）是城市交通管理的重要组成部分，旨在优化道路网络效率并减少拥堵。

现有的TSC研究主要分为两类：基于**交通工程** 和**强化学习** 的方法。

其中，交通工程方法主要侧重于制定有效的启发式算法，根据车道级交通状况属性，动态调整交通信号灯配置。然而，这些方法的设计严重依赖人力及专业领域知识。

之后，多数研究便基于深度强化学习技术来应对这一任务，并在各种交通场景中都表现出了卓越的性能。

然而，基于强化学习的方法也存在明显缺点。首先，由于他们的训练数据仅涵盖有限的交通情况，致使其表现出局限的泛化能力，特别是在转移到更大规模的交通网络或在不常见的路况下（例如，极端高流量的情况）。

此外，由于深度神经网络（DNN）的黑盒特性，基于深度强化学习的方法缺乏可解释性，这使得研究人员很难理解其在某交通状况下控制行为的背后逻辑。

而当今，大语言模型凭借其卓越的零样本学习和泛化能力，它以模仿近似人类的推理过程来解决复杂任务，彻底改变了多个领域。

例如在交通控制任务上，PromptGAT使用LLM生成人类知识，以此来帮助DNN模型理解TSC任务中的长尾场景（例如极端天气），旨在弥合现实世界与模拟之间的差距。

不过，虽然现有的研究已经开始探索利用LLM作为辅助工具来增强决策，但直接利用LLM作为TSC智能体进行类人决策的潜力还尚未探寻。

具体而言，其有两个重要挑战。

第一个挑战在于如何使LLM能够理解实时交通动态并与交通环境做有效交互。

LLM通常在大规模自然语言语料库上进行预训练，但**很少包含非文本的流量数据**
（例如传感器读数和GPS轨迹）。尽管它们具有跨多种任务和领域的泛化能力，但实时交通数据和自然语言之间存在固有差距。

如何**为信号灯控制任务选择和开发专有垂类LLM** ，则是另一个重大挑战。

首先，通才大模型往往缺乏特定领域的知识，容易出现专业领域的幻觉问题。尽管GPT-4等最先进的LLM表现出了优异的泛化能力，但它们的闭源性质和高昂成本并不利于投入到实时TSC任务及其后续优化中。

因此，训练专门为TSC任务量身定制的LLM成为了当下更优的选择。

## 如何将LLM应用于TSC？

为了应对上述挑战，研究人员提出了LLMLight框架，其旨在整合大语言模型作为智能体，实现交通信号灯控制。

首先该研究将TSC视为部分可观察的**马尔可夫博****弈** （Partially Observable Markov
Game），其中每个LLM智能体管理一个十字路口的交通灯。

在每个信号切换时间步上，智能体都会收集目标路口的交通状况，并将其转换为人类可读的文本作为实时观察。

此外，该研究还结合了信息量丰富的**任务描述** 及一条与控制策略有关的**常识知识**
，以帮助LLM理解交通管理任务。交通路口的实时状态、任务描述与控制动作空间结合，形成了指导智能体决策的知识提示。

最后，LLM控制智能体利用**思想链** (CoT) 推理来确定下一个时间片的最佳交通信号灯配置。

并且该研究还构建了一个交通信号灯控制垂类大模型LightGPT来增强LLMLight框架。一方面，提出了模仿学习微调（Imitation Fine-
tuning），让学生LLM学习GPT-4产生的高质量决策和推理轨迹。

另一方面，引入了一个由评论家模型指导的策略优化（Critic-gudied Policy Refinement）过程，使其评估和改进LLM智能体的控制。

优化后的LightGPT可以产生比GPT-4更具成本效益且更有效的控制策略，并在不同流量场景中展现出卓越的泛化能力。

一起来看具体实现方法。

### LLMLight框架的构建

LLMLight的工作流包括：

  * **交通状态观测特征构建** ：收集交通路口的交通状态观测；

  * **常识知识增强的智能体提示构建** ：组成一则整合了常识知识的提示，用于指导LLM推理出下一时间片最优的交通信号灯配置；

  * **智能体的分析推理及决策** ：LLM使用构建的提示进行分析推理决策过程，随后做出决策。其流程如下图所示：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBhkedL8zOhmgDubicQiax5aX8ELm9Ctibe4aIcfHpJcGtMmyckC2PkDOOKKlGYfia2ib19OWcT52iaAcMw/640?wx_fmt=png&from=appmsg)

研究人员将交通信号控制定义为一个部分可观察的马尔可夫博弈。基于交叉口实时交通状况的观察![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgy0cZJWST9RUmFOKicdabEqIFjS5T46ylribXTED1qavpp568mABTGBPA/640?wx_fmt=jpeg&from=appmsg)、交通场景描述![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgQia40cWD8MH1sFy0nflcoSeqEU6YN0xoYpu6Ka6YX6p7sKeGVg8jzsQ/640?wx_fmt=jpeg&from=appmsg)、任务描述![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgCOePTgK18iaKpGMn8y2SqB6uQv5dyoUWdzzxdfpzO8P4Ev7zygUhnUg/640?wx_fmt=jpeg&from=appmsg)、常识知识![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYg6c8FwcKmWkFAw52apYtV0Abdo3H2QeWbyGrvaETW2HWqJhlGmMVwbA/640?wx_fmt=jpeg&from=appmsg)以及信号灯控制动作空间![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgibFbfayHFGLuXLc6eOicibOnsAjnZCJ6zI6ufjDvEpopRuL5kxOnicjK2Q/640?wx_fmt=jpeg&from=appmsg)，以LLM智能体的策略控制目标交通路口的信号灯![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgPxnTVh9hdgOS9wk3oQuU0gYjOuVzKbZtbOwK2Jc0qHibmMUn9FiaW2icw/640?wx_fmt=jpeg&from=appmsg)。

LLM的输出为分析推理轨迹![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgLlqa952eKQnIzCBENP3vhC3P3B786GLNX0TibIRhL27h4vHfhiaKnSAA/640?wx_fmt=jpeg&from=appmsg)与调节路口信号灯的控制动作![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgyO60Z9oRp0JEkJG7fxPGnyAZSSicib0xsXVO7e3QricC5EsvSTEY9WqxQ/640?wx_fmt=jpeg&from=appmsg)。其目标为优化长期内交通路口的通行效率。其可形式地表示为：

![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgerIbNjcZEr6XVJ4tWmicrRTB2h1wpBDGIG1ibjCjoU4gm0N3BqTtiak3g/640?wx_fmt=jpeg)

具体来说，对于交通状态观测特征构建，研究人员收集了两种在现实场景中可以简单获取到的观测特征：不同车道上排队车的数量；同车道上，还未到达路口车的数量。

常识知识增强的智能体提示构建方面，除观测特征外，研究人员还向LLM提供了在处理交通信号控制任务中其他必不可少的信息，包括交通场景描述![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgQia40cWD8MH1sFy0nflcoSeqEU6YN0xoYpu6Ka6YX6p7sKeGVg8jzsQ/640?wx_fmt=jpeg&from=appmsg)、任务描述![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgCOePTgK18iaKpGMn8y2SqB6uQv5dyoUWdzzxdfpzO8P4Ev7zygUhnUg/640?wx_fmt=jpeg&from=appmsg)和控制动作空间![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgibFbfayHFGLuXLc6eOicibOnsAjnZCJ6zI6ufjDvEpopRuL5kxOnicjK2Q/640?wx_fmt=jpeg&from=appmsg)。

这使得LLM能够全面了解任务，从而做出合理的控制决策。

此外，该研究还整合了常识知识![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYg6c8FwcKmWkFAw52apYtV0Abdo3H2QeWbyGrvaETW2HWqJhlGmMVwbA/640?wx_fmt=jpeg&from=appmsg)，以缓解通用型LLM在交通控制领域知识上的局限性。

这些知识规定了智能体需要优先考虑排队长度较长的车道，而减弱对距离路口较远车辆的注意力。形式化地，该研究将智能体提示表示为：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBhkedL8zOhmgDubicQiax5aXibDtvFaMk4k8AM0dRbZPJY6l1R6e36bPwFfmlzpr3XnAl26XD89khoQ/640?wx_fmt=png&from=appmsg)

提示符模板的简要示意如下图所示：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBhkedL8zOhmgDubicQiax5aXQBVwtQpWKUIoSMoBGGfPEcEnOicJ7CyN8dUauTYNE3yNNuBSdx0oY1A/640?wx_fmt=png&from=appmsg)

在智能体的分析推理及决策方面，该研究利用上述提示LLM进行零样本（Zero-Shot）推理。

其决策过程包含两个关键步骤：分析推理及决策。

首先，LLM会对所给任务及常识知识进行理解，并评估各车道的当前交通状况。

随后，LLM选择合适的信号灯配置，以允许拥堵最严重的车道通行，从而优化交通流量，确保车辆的顺畅通过。

通过这种方式，LLMLight不仅可以制定有效的控制策略，还可以为每个决策提供其背后推理逻辑。这会极大有助于建立更具解释性和透明性的交通控制系统。

形式化地，研究人员将推理和执行行动表示为![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYg1ntiaValHsOgkVeFe88aqUticcAIGuIXVsxeIEFSkVL57YibzG87syqvw/640?wx_fmt=jpeg&from=appmsg)，LLM主干的决策过程示例如下图所示：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBhkedL8zOhmgDubicQiax5aXxQZWpgMqibeDTJk9yQyiaHHWvMDFUibfb9dy2ETBTz2tauic0RAmiaNVWFQ/640?wx_fmt=png&from=appmsg)

### LightGPT模型训练

此外，该研究还提出了一种训练方法，以专门优化用于交通信号灯控制的LLM——LightGPT。

它主要包括三个阶段：

  * **推理轨迹的收集和筛选** ：首先，该研究收集GPT-4的思维链推理轨迹进行模仿学习微调，之后筛选出与长期优化目标最相符的轨迹以确保数据质量；

  * **模仿学习微调** ：利用GPT-4的决策及其推理轨迹对学生LLM进行训练；

  * **评论家模型指导的策略优****化** ：依据评论家模型的反馈进行微调，进一步改善LLM的决策过程。

下图展示了其训练流程：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBhkedL8zOhmgDubicQiax5aXJRvUvQZtwXYal2Og0jnVAQBmqw8A9QVJcbXFibQ5hDGE8JEDq2fibKzA/640?wx_fmt=png&from=appmsg)

**推理轨迹的收集和筛选**

利用上述方法构建的提示，该研究首先让GPT-4与模拟交通环境进行交互，并收集其推理轨迹。

为了确保所收集数据的质量，研究人员筛选出与交通信号灯控制的长期目标最相符轨迹（如最小化未来的排队长度）。这种筛选操作通过与一个预训练的动作-
价值网络（Action-Value Network）的对齐来实现。

该研究通过在模拟环境中优化贝尔曼方程（Bellman Equation）来训练此网络：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBhkedL8zOhmgDubicQiax5aXRM0WUF9eickicDzbSI6JHQkjnJSPj7qlzP5rRl7mcErWTyIbNNTNlhoA/640?wx_fmt=png&from=appmsg)

其中![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgA7ibH8g9LueLfEXozhCpykADNPjfzyibibSLic6BArxJTHCzqa7ic2b0hOw/640?wx_fmt=jpeg&from=appmsg)和![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYg4Xkf4D1LblLwT963BjPfSDQbw3X6CteydwdAUsDE0Lk2m3zUsX86NQ/640?wx_fmt=jpeg&from=appmsg)是在信号灯切换时间步![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgRsialL3wNT0NmdHjMictuYO7Aic44wKibR9YhlCsqOVKYicFTUjQy8GSAWw/640?wx_fmt=jpeg&from=appmsg)时观察和控制动作，![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgITjj2EDlibuvyFAEwmfY8FaaicjNe3eaTib4sEgth6iaysB7whOibzWJMMg/640?wx_fmt=jpeg&from=appmsg)是奖励折扣因子。![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgwcrIK2TrELg5wiahafTwM6et3r69ktamTb7Fn5FfJThcVR2rDtAwcVw/640?wx_fmt=jpeg&from=appmsg)是奖励函数，其提供了在观察![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgy0cZJWST9RUmFOKicdabEqIFjS5T46ylribXTED1qavpp568mABTGBPA/640?wx_fmt=jpeg&from=appmsg)下执行动作![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgyO60Z9oRp0JEkJG7fxPGnyAZSSicib0xsXVO7e3QricC5EsvSTEY9WqxQ/640?wx_fmt=jpeg&from=appmsg)的反馈（如队列长度的负值）。![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgsciccYmd6ibYtY8MwDsDj5duqApytTJs6LopEwTJeQK0oNpBrRzzKLGQ/640?wx_fmt=jpeg&from=appmsg)是动作-
价值函数，用于估计执行![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgyO60Z9oRp0JEkJG7fxPGnyAZSSicib0xsXVO7e3QricC5EsvSTEY9WqxQ/640?wx_fmt=jpeg&from=appmsg)后获得的未来累积奖励。

随后，训练好的动作-价值函数被用作评论家模型来评估GPT-4的决策。研究人员仅保留选择可得到最高未来奖励控制动作的推理轨迹，形式化地：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBhkedL8zOhmgDubicQiax5aXqzicy76ylnNCUTo0fTw0TTYmyko6Us5dj2gIvqRGbWicLic1gianFp1wsg/640?wx_fmt=png&from=appmsg)

其中![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgIxqpqGPCQHegoNnT3iat2ltXI9zMf1iaDoZv9ia1Fj1U64fPnL5YZa4mA/640?wx_fmt=jpeg&from=appmsg)是模拟持续时间，![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgeWqJgc7LnaAGvGxdB8zHCrZBf3qrtpe1x6GmynFYZVR65DfphbibSEw/640?wx_fmt=jpeg&from=appmsg)是智能体提示，![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgvxaPpD3piccndOdbvHaBgCribjkLYzDybeyfibn2STHsBI6iblseczFLXA/640?wx_fmt=jpeg&from=appmsg)是GPT-4的推理轨迹。

**模仿学习微调**

这一阶段，首先研究人员采用了一种模仿学习过程，令学生LLM基于GPT-4的决策及其推理轨迹进行训练。

研究人员将提![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYghibcrz8iczZAdmU20K4htg9K3IzPIKs1qDSMqNYwvNPsqKHCcZxKFqaA/640?wx_fmt=jpeg&from=appmsg)视为微调指令，将包含GPT-4选择的控制动作![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgyO60Z9oRp0JEkJG7fxPGnyAZSSicib0xsXVO7e3QricC5EsvSTEY9WqxQ/640?wx_fmt=jpeg&from=appmsg)的推理轨迹![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgIEG24L3QK6JibrJ2TLZgezUwLMcLNf63m8Pj9VG6dZhYpg5e36LNI9A/640?wx_fmt=jpeg&from=appmsg)作为期望得到的回答，并以负对数似然（NLL）作为损失函数：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBhkedL8zOhmgDubicQiax5aX5ZhqAVNKMrVZdTPhGI76drkgPPceV7NqwvnxDgwdu1UibTXxnW49fXg/640?wx_fmt=png&from=appmsg)

其中![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgESQeO3x9gTfCyL8fs56BxP2hU4sS3vw2OWngTpWDHBiamyFlz8jC2DQ/640?wx_fmt=jpeg&from=appmsg)为在提示为![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYghibcrz8iczZAdmU20K4htg9K3IzPIKs1qDSMqNYwvNPsqKHCcZxKFqaA/640?wx_fmt=jpeg&from=appmsg)的情况下生成字符![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgoSF0ibVS0BHyD0AichuLpzWQwEBwG6uH46VnhXTXHJkoAr3HBB8BIKOA/640?wx_fmt=jpeg&from=appmsg)的概率。

**评论家模型指导的策略优化**

为进一步提高LLM控制策略的有效性，研究人员提出了一种策略优化方法，通过调整LLM的推理轨迹以得出更合理的控制决策。

类似的，该研究继续使用上述预训练的动作-
价值函数作为评论家模型，以评估由LLM选择的控制动作。随后，利用一种对齐微调算法来调整推理轨迹，最终引导LLM采取产生更高未来奖励的决策。

具体而言，有![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgmWGaDOdDUAK8hvEesd1eUvNNoicXXUNqplplic4TYu0c33GqUkCCvx4Q/640?wx_fmt=jpeg&from=appmsg)个在提示![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYghibcrz8iczZAdmU20K4htg9K3IzPIKs1qDSMqNYwvNPsqKHCcZxKFqaA/640?wx_fmt=jpeg&from=appmsg)下由策略![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgPxnTVh9hdgOS9wk3oQuU0gYjOuVzKbZtbOwK2Jc0qHibmMUn9FiaW2icw/640?wx_fmt=jpeg&from=appmsg)采样的推理轨迹

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBhkedL8zOhmgDubicQiax5aXtjlibTRDumLvUrWobY8ibTprXUSBpBl42u8EglHRtVcBvUdricssjg7ng/640?wx_fmt=png&from=appmsg)

评论家模型给出每个轨迹![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgyJ9D8ulcJCAeLpneLgDKA3Aic5Hukf12Rogib5tDw6gxrHOHe0JaK4Kg/640?wx_fmt=jpeg&from=appmsg)推导出的控制动作的分数

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBhkedL8zOhmgDubicQiax5aX3TrLvjpynL1ibIEFTHKJgt5uXFKWwalPkia4ZmTqMSxt5LGiaePaSVYYg/640?wx_fmt=png&from=appmsg)

接着，![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgyJ9D8ulcJCAeLpneLgDKA3Aic5Hukf12Rogib5tDw6gxrHOHe0JaK4Kg/640?wx_fmt=jpeg&from=appmsg)的字符平均对数似然值表示由![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgPxnTVh9hdgOS9wk3oQuU0gYjOuVzKbZtbOwK2Jc0qHibmMUn9FiaW2icw/640?wx_fmt=jpeg&from=appmsg)生成![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYgyJ9D8ulcJCAeLpneLgDKA3Aic5Hukf12Rogib5tDw6gxrHOHe0JaK4Kg/640?wx_fmt=jpeg&from=appmsg)的概率：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBhkedL8zOhmgDubicQiax5aX87Qu5JFEUtYykCrxZxvqibTMh3mEYCNYuib1gHaogIiaQ0jQRUFwibRP4w/640?wx_fmt=png&from=appmsg)

该研究采用带有边界约束项（RBC）的排名反馈损失进行优化，以指导LLM得出产生得分更高控制动作的推理轨迹：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBhkedL8zOhmgDubicQiax5aXeGrYq6kwoOS1IcgVpZrgLvj6DoHt63kPEsOMhyZ540Xz3JslZOO5TQ/640?wx_fmt=png&from=appmsg)  

其中

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBhkedL8zOhmgDubicQiax5aXfKSpq8PlZYp5bwGFq9n6j9ibHR2aEjFIxd5E2mvHZibfMqiaXR3icWu5vA/640?wx_fmt=png&from=appmsg)

是比![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtADB0lrW2Viby5yEs4EicrgYg1HNQWQEswJhEgBxiacPHJ8SL7n0chZiacFmb4T8GM48pBQc3diaecetbw/640?wx_fmt=jpeg&from=appmsg)评分更高的且最低的推理轨迹的概率，β是超参数。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBhkedL8zOhmgDubicQiax5aXOlQicTW6An4y24Yibia2ibd0XYXFHS4yP4iaqM7ibDw7hriafZribmHv52xtYA/640?wx_fmt=png&from=appmsg)

是用于提升产生得分更高控制动作的轨迹的对齐项。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBhkedL8zOhmgDubicQiax5aXzQGyfZKvac1LUA45vVic1W4febPFfeQj5F2YicickYA4ysRqmAEI8bPrg/640?wx_fmt=png&from=appmsg)

是用于防止性能下降的约束项。

## 该方法效果如何？

实验阶段，该研究使用了**五个真实世界流量数据集** ，其中包括了来自济南和杭州的数据。

此外，还利用了**两个在纽约更大的路网下采集的数据** ，以测试不同方法的在大型路网下的可扩展性。

为了测试在长尾情况下的泛化性，研究人员还**合成了两个额外的数据集** ，模拟了极端拥堵的路况。

该研究使用了平均旅行时间（ATT），路口平均队列长度（AQL），以及路口平均等待时间（AWT）作为评价指标。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBhkedL8zOhmgDubicQiax5aXGcicqyECBKwmJ7JF8XvF08AEtZv8CH9Ouvbia0p43hDaSeO9I93F9fKg/640?wx_fmt=png&from=appmsg)

以下是具体的实验结果。

### 总体性能比较

实验结果表明，配备了LightGPT的LLMLight在所有基准测试中始终达到了SOTA或与经典方法同等的性能水平。

尽管Advanced-
CoLight（当前最先进的强化学习方法）在杭州数据集上表现优于LLMLight（LightGPT），但它的决策需要依赖与邻近路口之间的通信。

值得一提的是，LLMLight（LightGPT）仅利用当前路口的观测特征就展现出强有竞争的结果，表明了其决策显著的有效性。

对于由通用型大模型驱动的LLMLight，研究人员观察到GPT-4表现最为出色，并展示出与最先进强化学习方法相当的效果。

同时Llama2-70B和13B分别获得第二和第三名，这表明LLM在交通信号控制任务中也遵循了规模化定律（scaling law）。

令人惊讶的是，ChatGPT-3.5的表现最不理想。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBhkedL8zOhmgDubicQiax5aXMtlJPkds6OKGx9c7Vbqxy8TsNKA26t8kowQ0Ms0zxcibv20GavN3picQ/640?wx_fmt=png&from=appmsg)

### 泛化性的比较

该研究首先测试了不同方法的可迁移性。标有“-T”的模型是在不同的道路网络上预训练得到的（例如，使用在济南预训练的模型在杭州数据集上评估可迁移性）。反之则在相同的数据集上进行训练和测试。

该研究观察到强化学习方法在迁移后性能明显下降，尤其在济南1和杭州1数据集中表现尤为明显。相反，LLMLight（LightGPT）始终表现出优越的性能，并在所有数据集上展现出优异的可迁移性。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBhkedL8zOhmgDubicQiax5aX0fW4Zasjd0ncuF0AQnNAlyuCfQBTj4WUKnibBKf41uKDnb0cbeQnffQ/640?wx_fmt=png&from=appmsg)

之后该研究分析了不同方法的可扩展性，测试它们在应用于规模更大的路网时的性能。

可以观察到，大多数强化学习方法发生了显著性能下降，甚至表现出比启发式方法Maxpressure更差的性能。虽然最先进的强化学习方法在平均旅行时间（ATT）上与LLMLight（LightGPT）相当，但值得注意的是，它们的决策会导致最高延长57.80%的等待时间（AWT）。

这一结果表明，强化学习方法侧重于优化排队车辆的总数，但可能会以牺牲少部分队列的等待时间为代价。

在实际场景中，等待时间的重要性不容忽视。相比之下，LLMLight可以同时确保最短的旅行时间和等待时间，体现了其拓展到规模更大的路网时的优良的可扩展性和适用性。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBhkedL8zOhmgDubicQiax5aXPpSmrjbEXOhycBmdscGfPG2pYkniavufce2RfNicTJNQwoNpn5wZzO1A/640?wx_fmt=png&from=appmsg)

最后该研究为了探讨了在极端拥堵情况下不同模型的性能，在济南和杭州的路网上生成了两个合成交通流数据集，其流量相比原始数据集增加了约四倍。

与可扩展性实验类似，强化学习方法也表现出显著的性能下降，表现出比Maxpressure更差的结果。

相比之下，LLMLight（LightGPT）始终表现出卓越的性能，体现了其在更加繁重的交通条件下的稳健性和实用性。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBhkedL8zOhmgDubicQiax5aXdOkd739zKNxFxsibyyhV39voHzoQTPngH1VkFEI92h8IiaMib6HmjibeJA/640?wx_fmt=png&from=appmsg)

### 可解释性分析

为了评估LLMLight的可解释性，研究人员在杭州数据集上进行了一个案例模拟。

在这个模拟场景中，北部路段出现了严重拥堵，表现为排队的车辆出现积压。

下图详细展示了LightGPT在此路况下的推理分析过程。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBhkedL8zOhmgDubicQiax5aXHzGRbXqKh5FG60RoGiaccYicPzhibouXx0pTz6Fg0P5tDw4Q6QPmZm8zw/640?wx_fmt=png&from=appmsg)

它以理解任务开始，并分析目标交叉口的交通情况以进行决策推理。随后，它明确信号灯NLSL为最优的选择。

与强化学习方法不同，LLMLight不仅在制定有效的控制策略方面表现出色，而且还能为每个决策提供其背后的详细解释。这一独特特征增强了LLMLight的透明度和可解释性，有助于研究人员更全面地理解其决策行为。

最后，研究人员表示，LLMLight的下一步研究将着眼于融合多模态信息及群体协同。

多模态大模型可以直接从端到端地提取路口的交通拥堵信息，使模型能够自行探索可用的视觉特征，进而自我优化出更优的决策。

而群体协同则能够实现临近路口、车辆和智能体之间的信息交换，从而获得全局信息，最终达到优化整体路网的交通效率的目的。

论文链接：https://arxiv.org/abs/2312.16044  
代码链接：https://github.com/usail-hkust/LLMTSCS  
主页链接：https://gungnir2099.github.io/LLMLight-Page/  
模型权重链接：https://huggingface.co/USAIL-HKUSTGZ/LLMLight-LightGPT

— **完** —

**报名中！**

**2024年值得关注的AIGC企业 &产品**

量子位正在评选**2024年最值得关注的AIGC企业** 、 **2024年最值得期待的AIGC产品** 两类奖项，欢迎[报名评选]()！

评选报名**截至2024年3月31日![](https://res.wx.qq.com/t/wx_fed/we-
emoji/res/v1.3.10/assets/newemoji/LetMeSee.png)**

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC7IzBlicP1jwLsfiaw2A2ibBoWRgd47kXexFUOSSzXn5f9fDcza39rny2BgqyDQkDrSoLCDh3Ag7XwA/640?wx_fmt=png&from=appmsg)

**中国AIGC产业峰会**
同步火热筹备中，了解更多请戳：[Sora时代，我们该如何关注新应用？一切尽在中国AIGC产业峰会](http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247718372&idx=3&sn=b89d20b431d783c185143da7c8948372&chksm=e8df2296dfa8ab8021659abb68c594c4ebe5b2907d12777771057499c61143c2cdaa8b3269b3&scene=21#wechat_redirect)

商务合作请联络微信：18600164356 徐峰  

活动合作请联络微信：18801103170 王琳玉

  

**点这里 👇关注我，记得标星噢**

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg)

