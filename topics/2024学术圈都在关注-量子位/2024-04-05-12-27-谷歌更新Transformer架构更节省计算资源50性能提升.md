# 谷歌更新Transformer架构，更节省计算资源！50%性能提升

文章作者: 量子位
发布时间: 2024-04-05 12:27
发布地: 未知地区
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247723972&idx=2&sn=6888ed3a1c2e793a4646254e166fa94c&chksm=e8dfdcb6dfa855a059386f05395e0f5cec0e3262e10796df803aa813aea26915f553198e4d17#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtC5jzKGTWS7cfYjkvzrf7LTR76YH2zVfMArM4GuYvFPaebEgMGrzRTp88XNKZ5HCTU77gVz6kdS5A/300

##### 明敏 发自 凹非寺  
量子位 | 公众号 QbitAI

谷歌终于更新了Transformer架构。

最新发布的**Mixture-of-Depths** （MoD），改变了以往Transformer计算模式。

它通过**动态分配** 大模型中的计算资源，跳过一些不必要计算，显著提高训练效率和推理速度。

结果显示，在等效计算量和训练时间上，MoD每次向前传播所需的计算量更小，而且后训练采样过程中步进速度**提高50%** 。

这一方法刚刚发布，就马上引发关注。

MoE风头正盛，MoD已经来后浪拍前浪了？

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC5jzKGTWS7cfYjkvzrf7LT7DTooGoUYrAX9sunV1zYKfmcWSxiaO5HBxs9YSSAQicxMLbdIc9ibTF7A/640?wx_fmt=png&from=appmsg)

还有人开始“算账”：

> 听说GPT-4 Turbo在Blackwell上提速30倍，再加上这个方法和其他各种加速，下一代生成模型可以走多远？

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC5jzKGTWS7cfYjkvzrf7LThuXc3hCkC9vzYWT2AaFEfVfgh8A3ookLvYNVSO5Lw6xicibIQGqWrFJQ/640?wx_fmt=png&from=appmsg)

所以MoD如何实现？

## 迫使大模型关注真正重要信息

这项研究提出，现在的大模型训练和推理中，有很多计算是没必要的。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC5jzKGTWS7cfYjkvzrf7LT54RBUK5Fsib2AYhxyAeRFvpBpAokzX5P2BqRBxGBWvppp468yiafjGfg/640?wx_fmt=png&from=appmsg)

比如预测下一个句子很难，但是预测句子结束的标点符号很简单。如果给它们分配同样的计算资源，那么后者明显浪费了。

![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtC5jzKGTWS7cfYjkvzrf7LTxRoNsezMx1Q122EytsgXDI20xxDaibofUyGcdm8ico7O8G3VnIdjianLw/640?wx_fmt=jpeg&from=appmsg)

在理想情况下， 模型应该只给需要准确预测的token分配更多计算资源。

![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtC5jzKGTWS7cfYjkvzrf7LTbwZPaqmdxE4JHTIUU5fPpmR8EsialnHv3mTpAHYmn43icKBHgUOcsPsQ/640?wx_fmt=jpeg&from=appmsg)

所以研究人员提出了**MoD** 。

它在输入序列中的特定位置动态分配FLOPs（运算次数或计算资源），优化不同层次的模型深度中的分配。

通过限制给定层的自注意力和MLP计算的token数量，迫使神经网络学会主要关注真正重要的信息。

因为token数量是事先定义好的，所以这个过程使用一个已知张量大小的静态计算图，可以在时间和模型深度上动态扩展计算量。

下图右上图中的橙色部分，表示没有使用全部计算资源。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC5jzKGTWS7cfYjkvzrf7LTwic82yOn3PYbyGXkqsl3oAjJeSa09eev0gG48lRVjWK7rrcmHmGqNlg/640?wx_fmt=png&from=appmsg)

这种方法在节省计算资源的同时，还能提高效率。

这些模型在等效的FLOPS和训练时间上与基线性能相匹配，但每次前向传播所需的FLOP更少，并且在训练后采样时提速50%。

对比来看，如果为每一个token生成一个概率分布，每个token根据最高概率被送去对应的“专家”，可能会导致负载不平衡。

如果反过来，这能保障负载平衡，但是可能导致某些token被过度处理或处理不足。

最后来看论文中使用的Expert-choice
MoD，router输出的权重被用于确定哪些token将使用transformer亏啊计算。权重较大的token将参与计算，权重较小的token将通过残差连接绕过计算，从而解决每次向前传播的FLOPs。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC5jzKGTWS7cfYjkvzrf7LTh5jz6Zb1c8iaLRggKDZgcqIEVc6EqlibYw6u7ZK8G3RyxwImryd7oVPw/640?wx_fmt=png&from=appmsg)

最后，研究团队展示了MoD在不同实验中的性能表现。

首先，他们使用相对较小的FLOP预算（6e18），以确定最佳超参数配置。

通过这些实验，作者发现MoD方法能够“拉低并向右推移”isoFLOP基线曲线，这意味着最优的MoD方法在更低的损失水平上拥有更多的参数。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC5jzKGTWS7cfYjkvzrf7LTSic60wyFgZNNP3lNd6ViaU1LKQUiaCfggFTferR9VjM3Z1lDzaiber4DWw/640?wx_fmt=png&from=appmsg)

通过isoFLOP分析，比较6e18、2e19和1e20 FLOPs的总计算预算下的模型性能。

结果显示，在更多FLOP预算下，FLOP最优的MoD仍然比基线模型有更多的参数。

存在一些MoD变体，在步骤速度上比isoFLOP最优基线模型更快，同时实现更低的损失。这表明在训练之外，MoD的计算节省仍然有效。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC5jzKGTWS7cfYjkvzrf7LTI4kfEvkOgLSQsDibZsNgxFrqibHewia2QguibwS6Ps1sXnGpibHrlE5L1Vw/640?wx_fmt=png&from=appmsg)

同时，研究团队还探讨了MoD和MoE结合的可能性——MoDE。

结果表明而这结合能提供更好的性能和更快的推理速度。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC5jzKGTWS7cfYjkvzrf7LTdpxtqFQh6iauO4ZrHADpSaycAbkIb6ttXEo8sLC6joythB7dQv9EvxA/640?wx_fmt=png&from=appmsg)

## 网友：联想到了ResNet

MoD推出后马上引发了不小关注。

有人感慨，MoE还没有弄清楚呢，MoD都已经来了！

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC5jzKGTWS7cfYjkvzrf7LTV0MLSQBpiaqR4dGicbB5QKKYMmZj58AkBzoabFywqO4TpRTXF8YmMmVA/640?wx_fmt=png&from=appmsg)

这么高效的方法，让人马上联想到了ResNet。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC5jzKGTWS7cfYjkvzrf7LTfyvBbFLFBza2rTQT3BPhVb90ZqLO73ZT7OU7ZkmWRpzbtGs5Mmxf2g/640?wx_fmt=png&from=appmsg)

不过和ResNet不同，MoD跳过连接是完全绕过层的。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC5jzKGTWS7cfYjkvzrf7LTzPAvQyTBX2KULoSVqW1IeemfrKoz1cw3z90skmvzhQYZCGqyc5CEpw/640?wx_fmt=png&from=appmsg)

还有人表示，希望这种方法是完全动态的，而不是每个层固定百分比。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC5jzKGTWS7cfYjkvzrf7LTaQfYzkZkK4YCua5SFXCptBTZBReW236aialES3tbO6pQyw5iaKWAIIBA/640?wx_fmt=png&from=appmsg)

这项研究由DeepMind和麦吉尔大学共同带来。

主要贡献者是David Raposo和Adam Santoro。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC5jzKGTWS7cfYjkvzrf7LTPicgibmAy0m0oPIAHaBB1TQSKz0FCeRZyDczWIek60owOWicsmOXpXbPA/640?wx_fmt=png&from=appmsg)  
![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtC5jzKGTWS7cfYjkvzrf7LTicibNQaPRcWwhntNvzSqMEsls95U09IFWBPotHpic9zmPBs26mwXLWM3g/640?wx_fmt=png&from=appmsg)

他们二人都是DeepMind的研究科学家。此前共同带来了神作《Relational inductive biases, deep learning, and
graph networks》。

这篇论文目前被引次数超过3500次，论文核心定义了Inductive bias（归纳偏置）概念。

论文地址：  
https://arxiv.org/abs/2404.02258

参考链接：  
[1]https://twitter.com/TheSeaMouse/status/1775782800362242157  
[2]https://twitter.com/_akhaliq/status/1775740222120087847

— **完** —

**【🔥 火热报名中】中国AIGC产业峰会  
**

**定档4月17日**

峰会已经邀请到数位代表技术、产品、投资、用户等领域嘉宾，共论生成式AI产业最新变革趋势。

最新确认嘉宾包括：**商汤科技****杨帆** 、**轻松集团****高玉石** 、**印象笔记****唐毅** 、**蚂蚁集团****李建国**
等，[了解更多嘉宾详情](http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247723923&idx=2&sn=ef85f4d2209e62f02d343b7bf88f731a&chksm=e8dfdce1dfa855f71838d3e8a8e9abdb997ddaa45547793f0a373b8baeb7211e2f11365b5f39&scene=21#wechat_redirect)。

[点击报名参会 ⬇️]()

[![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtDlvmVssxqp6Jf7C6ZIwt76oeI4gxNbx0FeUWTz1cCt7LSj2n8f0C2qxYHA6odjoROc20D0ujkVRg/640?wx_fmt=jpeg&from=appmsg)]()峰会将全程线上下同步直播，欢迎预约直播
⬇️

**  
**

**点这里 👇关注我，记得标星噢**

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg)

  

