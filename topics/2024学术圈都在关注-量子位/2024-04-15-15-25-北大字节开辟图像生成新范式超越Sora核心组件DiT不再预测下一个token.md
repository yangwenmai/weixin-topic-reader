# 北大字节开辟图像生成新范式！超越Sora核心组件DiT，不再预测下一个token

文章作者: 量子位
发布时间: 2024-04-15 15:25
发布地: 北京
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247725306&idx=3&sn=029dd5ba7e0b25916ee7858c302a3fde&chksm=e8dfc788dfa84e9ee6f72743a067908b1b99ab882f1333c0c8488e9e919ad705e97667ed128e#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtAwQ4hbsRN9u66kXjYmVpBxODQIXEySia92bRqc3vFq3t5E3hNFkXutsC23rP3tu2TpuAe2gHOcficQ/300

##### 鱼羊 发自 凹非寺  
量子位 | 公众号 QbitAI

北大和字节联手搞了个大的：

提出**图像生成新范式** ，从预测下一个token变成**预测下一级分辨率** ，效果超越Sora核心组件Diffusion
Transformer（DiT）。

并且代码开源，短短几天已经揽下1.3k标星，登上GitHub趋势榜。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAwQ4hbsRN9u66kXjYmVpBxAZKY5J5vUYnP0v9d0U9shcJiaRMr8tSpkH0EWK5b3dRW8Xt78Q5syLQ/640?wx_fmt=png&from=appmsg)

具体是个什么效果？

实验数据上，这个名为**VAR** （Visual Autoregressive
Modeling）的新方法不仅图像生成质量超过DiT等传统SOTA，**推理速度也提高了20+倍** 。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAwQ4hbsRN9u66kXjYmVpBxXZIZsica3UyFvpgc4KOF6YadEicQhvSIic4wbcbeoqDHI2rYGCo8WArfA/640?wx_fmt=png&from=appmsg)

**这也是自回归模型首次在图像生成领域击败DiT。**

直观感受上，话不多说，直接看图：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAwQ4hbsRN9u66kXjYmVpBxV9XCyqny9mLATxia5ZL1dcywnzWOYsoueCB8bmeNeRQaCGauYPTEEbQ/640?wx_fmt=png&from=appmsg)

值得一提的是，研究人员还在VAR上，观察到了大语言模型同款的Scaling Laws和零样本任务泛化。

论文代码上线，已经引发不少专业讨论。

有网友表示有被惊到，顿时觉得其他扩散架构的论文有点索然无味。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAwQ4hbsRN9u66kXjYmVpBxcg3DEUjwo0lUxZoyYObohvHA1yBLbkVYle1hGkRBfx9ajYJgiatJegw/640?wx_fmt=png&from=appmsg)

还有人认为，这是一种通向Sora的更便宜的潜在途径，计算成本可降低一个乃至多个数量级。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAwQ4hbsRN9u66kXjYmVpBxPA8RSyRquchYeib7V4icyMic3rIf8OTNX29UVbwRmts11xm3HBf1uQ3OQ/640?wx_fmt=png&from=appmsg)

## 预测下一级分辨率

简单来说，VAR的核心创新，就是用**预测下一级分辨率** ，替代了**预测下一个token** 的传统自回归方法。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAwQ4hbsRN9u66kXjYmVpBxs8SoRWwCNBuzJA4IKsQoq6LEXRQbhW072jN39SYrY0RuFyd5AgqZ4w/640?wx_fmt=png&from=appmsg)

VAR的训练分为两个阶段。

第一阶段，VAR引入了多尺度离散表示，使用VQ-VAE将连续图像编码为一系列离散的token map，每个token map有不同的分辨率。

第二阶段，主要是对VAR Transformer的训练，通过预测更高分辨率的图像，来进一步优化模型。具体过程是这样的：

从最低分辨率（比如1×1）的token map开始，预测下一级分辨率（比如4×4）的完整token map，并以此类推，直到生成最高分辨率的token
map（比如256×256）。在预测每个尺度的token map时，基于Transformer，模型会考虑之前所有步骤生成的映射信息。

在第二阶段中，之前训练好的VQ-VAE模型发挥了重要作用：为VAR提供了“参考答案”。这能帮助VAR更准确地学习和预测图像。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAwQ4hbsRN9u66kXjYmVpBxU8hibI9p9c2j9QiahGqmTwIHx3hzRJu6oB1PLgJWrZlo5jWM7kIIeZ9g/640?wx_fmt=png&from=appmsg)

另外，在每个尺度内，VAR是并行地预测所有位置的token，而不是线性逐个预测，这大大提高了生成效率。

研究人员指出，采用这样的方法，VAR更符合人类视觉感知从整体到局部的特点，并能保留图像的空间局部性。

## 符合Scaling Laws

从实验结果来看，在图像生成质量、推理速度、数据效率和可扩展性等方面，VAR都超过了DiT。

在ImageNet 256×256上，VAR将FID从18.65降到了1.8，IS从80.4提高到356.4，显著改善了自回归模型基线。

注：FID越低，说明生成图像的质量和多样性越接近真实图像。

推理速度方面，相较于传统自回归模型，VAR实现了约20倍的效率提升。而DiT消耗的时间是VAR的45倍。

数据效率方面，VAR只需要350个训练周期（epoch），远少于DiT-XL/2的1400个。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAwQ4hbsRN9u66kXjYmVpBxwJMib2VBJ0BNrpxJlstRHNG9K8HicBRcsuiaq6X5RRVj6WiboZbRCaiaXIA/640?wx_fmt=png&from=appmsg)

可扩展性方面，研究人员观察到VAR有类似于大语言模型的Scaling Laws：随着模型尺寸和计算资源的增加，模型性能持续提升。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAwQ4hbsRN9u66kXjYmVpBxtyL4ibibS4lzHaTojjKHr0ufOKmqZf80JUuY9fE9n5BDkz5I3FObp34Q/640?wx_fmt=png&from=appmsg)

另外，在图像修补、扩展和编辑等下游任务的零样本评估中，VAR表现出了出色的泛化能力。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAwQ4hbsRN9u66kXjYmVpBxdPKicEynMNHAcxkv0eJdB4IJWZ3VLezdTrwy0Yd7SfGGhOEqc9JjQeQ/640?wx_fmt=png&from=appmsg)

目前，在GitHub仓库中，推理示例、demo、模型权重和训练代码均已上线。

不过，在更多讨论之中，也有网友提出了一些问题：

> VAR不如扩散模型灵活，并且在分辨率上存在扩展问题。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAwQ4hbsRN9u66kXjYmVpBxKggwEcEfmEMBVjXibnPBpcJm5PSoX4QIxpRcnOrNvMhN3g8TVwG3FQg/640?wx_fmt=png&from=appmsg)

## 北大字节联合出品

VAR的作者们，来自字节跳动商业化技术团队和北大王立威团队。

一作田柯宇，本科毕业自北航，目前是北大CS研究生，师从北京大学信息科学技术学院教授王立威。2021年开始在字节商业化技术团队实习。

论文通讯作者，是字节跳动研究员袁泽寰和王立威。

袁泽寰2017年博士毕业于南京大学，目前专注于计算机视觉和机器学习研究。王立威从事机器学习研究20余年，是首届“优青”获得者。

该项目的项目主管，是字节跳动广告生成AI研究主管Yi jiang。他硕士毕业于浙江大学，目前的研究重点是视觉基础模型、深度生成模型和大语言模型。

参考链接：  
[1]论文：https://arxiv.org/abs/2404.02905  
[2]项目主页：https://github.com/FoundationVision/VAR

— **完** —

**报名参会倒计时 ⏰**

**4月17日，中国AIGC产业峰会**

**只需一天，感受AIGC新应用正在引领的科技新范式！**

来自产品、技术、投资等领域最主流的“玩家”代表和投资人，将在**4月17日中国AIGC产业峰会**
，与你共同探讨AIGC正在重塑的新世界。[了解更多峰会详情。](http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247724675&idx=1&sn=ac06f4e18cbd79749ca9ea5196554a99&chksm=e8dfd9f1dfa850e7b76b4be18c94cc356e272476afb5c0b5793e0f46b07fb4f9fa20d59566e2&scene=21#wechat_redirect)

欢迎报名参会 ⬇️

[![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtDLhXKy1KqSiaOvpU1HdibV8YeT1BnF0Bf63lPcbHGXrre6buWaV0CCbGb4AX2lz8Z8sjnxk0BsgQPw/640?wx_fmt=jpeg&from=appmsg)]()峰会将全程线上下同步直播，欢迎预约直播
⬇️

  

**点这里 👇关注我，记得标星噢**

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg)

