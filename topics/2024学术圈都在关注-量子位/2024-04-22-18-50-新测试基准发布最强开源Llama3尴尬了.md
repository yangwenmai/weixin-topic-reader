# 新测试基准发布，最强开源Llama 3尴尬了

文章作者: 量子位
发布时间: 2024-04-22 18:50
发布地: 北京
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247726259&idx=1&sn=1ea25d4636696a8e32cf92745413b394&chksm=e8dfc3c1dfa84ad7865182e18f5d2bc487ef6638a7b73adfa5d962d1fd15b46de1da43964a4e#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtBYugOibJeiaqqXD34iaK5Ivb2hjsl1D335Fkm1WeKYz1gq2icLheEZju3SHdP4g2P8SanKG6wI5Txgmg/300

##### 梦晨 发自 凹非寺  
量子位 | 公众号 QbitAI

如果试题太简单，学霸和学渣都能考90分，拉不开差距……

随着Claude 3、Llama 3甚至之后GPT-5等更强模型发布，业界急需一款**更难、更有区分度的基准测试** 。

大模型竞技场背后组织LMSYS推出下一代基准测试**Arena-Hard** ，引起广泛关注。

Llama 3的两个指令微调版本实力到底如何，也有了最新参考。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBYugOibJeiaqqXD34iaK5Ivb2zpkbvwUF8fbNMAxlDohbyQ3Aq1yIgzYC9eGbxMxLqYSC8dBxFic5pCg/640?wx_fmt=png&from=appmsg)

与之前大家分数都相近的MT Bench相比，Arena-Hard**区分度从22.6%提升到87.4%** ，孰强孰弱一目了然。

Arena-Hard利用竞技场实时人类数据构建，**与人类偏好一致率也高达89.1%** 。

除了上面两个指标都达到SOTA之外，还有一个额外的好处：

实时更新的测试数据包含人类新想出的、AI在训练阶段从未见过的提示词，**减轻****潜在的数据泄露** 。

并且新模型发布后，无需再等待一周左右时间让人类用户参与投票，只需花费25美元快速运行测试管线，即可得到结果。

有网友评价，**使用真实用户提示词而不是高中考试来测试，真的很重要。**

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBYugOibJeiaqqXD34iaK5Ivb2kOV8K8ArfwxicxhXzUW5NMjzg1AG2EF4ZLxLQNvOkiaiaJYmlzicPnFKug/640?wx_fmt=png&from=appmsg)

## 新基准测试如何运作？

简单来说，通过大模型竞技场20万个用户查询中，挑选500个高质量提示词作为测试集。

首先，挑选过程中确保**多样性** ，也就是测试集应涵盖广泛的现实世界话题。

为了确保这一点，团队采用BERTopic中主题建模管道，首先使用OpenAI的嵌入模型（text-embedding-3-small）转换每个提示，使用
UMAP 降低维度，并使用基于层次结构的模型聚类算法 (HDBSCAN) 来识别聚类，最后使用GPT-4-turbo进行汇总。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBYugOibJeiaqqXD34iaK5Ivb2x9su7iciccRFp9gBycyVGVCzIVKKjyHIfLoHzDzaJ5QIG0O5vhfSjfvg/640?wx_fmt=png&from=appmsg)

同时确保入选的提示词具有**高质量** ，有七个关键指标来衡量：

  * **具体性：** 提示词是否要求特定的输出？

  * **领域知识：** 提示词是否涵盖一个或多个特定领域？

  * **复杂性：** 提示词是否有多层推理、组成部分或变量？

  * **解决问题：** 提示词是否直接让AI展示主动解决问题的能力？

  * **创造力：** 提示词是否涉及解决问题的一定程度的创造力？

  * **技术准确性：** 提示词是否要求响应具有技术准确性？

  * **实际应用：** 提示词是否与实际应用相关？

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBYugOibJeiaqqXD34iaK5Ivb23iat6M1Vh1wDmCumYicDQh3l2suZRKGIqMIHdWlALV8oELTcNWzcEGgw/640?wx_fmt=png&from=appmsg)

使用GPT-3.5-Turbo和GPT-4-Turbo对每个提示进行从 0 到 7 的注释，判断满足多少个条件。然后根据提示的平均得分给每个聚类评分。

高质量的问题通常与有挑战性的话题或任务相关，比如游戏开发或数学证明。

![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtBYugOibJeiaqqXD34iaK5Ivb2G2Tt7kdXfPibnFcl2b16O3BacvtZiaGm6L8gKrGqSBibE5W0CJgAGhaTg/640?wx_fmt=jpeg&from=appmsg)

## 新基准测试准吗？

Arena-Hard目前还有一个弱点：使用GPT-4做裁判更偏好自己的输出。官方也给出了相应提示。

可以看出，最新两个版本的GPT-4分数高过Claude 3 Opus一大截，但在人类投票分数中差距并没有那么明显。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBYugOibJeiaqqXD34iaK5Ivb260JvYvHQ6VFSrk0gEyvgDGfwEl4lerc1Xh35pnZ83QlqIgkljwrGjg/640?wx_fmt=png&from=appmsg)

其实关于这一点，最近已经有研究论证，**前沿模型都会偏好自己的输出** 。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBYugOibJeiaqqXD34iaK5Ivb25eUwJZiatSgUaXG1dwYPHw49qbicA0zNBBH8jt5X7z3ac98oHDhaGYnA/640?wx_fmt=png&from=appmsg)

研究团队还发现，AI天生就可以判断出一段文字是不是自己写的，经过微调后自我识别的能力还能增强，并且**自我识别能力与自我偏好线性相关** 。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBYugOibJeiaqqXD34iaK5Ivb2KJFS2Vn4XiaFKZCkibEEukMWX9TrywDfcEt6bU1GicIzq48ojerxd6hmQ/640?wx_fmt=png&from=appmsg)

那么使用Claude 3来打分会使结果产生什么变化？LMSYS也做了相关实验。

首先，Claude系列的分数确实会提高。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBYugOibJeiaqqXD34iaK5Ivb2IOnm1P84DXoCMSS3W7SBYOjqcxDxJusw7Zods7VIKH6yTECgZz8kZg/640?wx_fmt=png&from=appmsg)

但令人惊讶的是，它更喜欢几种开放模型如Mixtral和零一万物Yi，甚至对GPT-3.5的评分都有明显提高。

总体而言，使用Claude 3打分的区分度和与人类结果的一致性都不如GPT-4。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBYugOibJeiaqqXD34iaK5Ivb2JWaSO5ibPzPsn9jiaID4OZkrwHezeLqslJHFKkpJSCKtRcVepRbuhYZA/640?wx_fmt=png&from=appmsg)

所以也有很多网友建议，**使用多个大模型来综合打分** 。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBYugOibJeiaqqXD34iaK5Ivb22NXtlADVOPbuNcgUKnFZIBJG0ZfoVsicC0YsfGR1X77M61TdBicMNJKQ/640?wx_fmt=png&from=appmsg)

除此之外，团队还做了更多消融实验来验证新基准测试的有效性。

比如在提示词中加入“让答案尽可能详尽”，平均输出长度更高，分数确实会提高。

但把提示词换成“喜欢闲聊”，平均输出长度也有提高，但分数提升就不明显。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBYugOibJeiaqqXD34iaK5Ivb2B3mQsTnibXxkuqK8mD12PkQ7bUgLVZ0kZ9a8aDrIDbsrJ4LoKBTYSjw/640?wx_fmt=png&from=appmsg)

此外在实验过程中还有很多有意思的发现。

比如GPT-4来打分非常严格，如果回答中有错误会狠狠扣分；而Claude 3即使识别出小错误也会宽大处理。

对于代码问题，Claude
3倾向于提供简单结构、不依赖外部代码库，能帮助人类学习编程的答案；而GPT-4-Turbo更倾向最实用的答案，不管其教育价值如何。

另外即使设置温度为0，GPT-4-Turbo也可能产生略有不同的判断。

从层次结构可视化的前64个聚类中也可以看出，大模型竞技场用户的提问质量和多样性确实是高。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBYugOibJeiaqqXD34iaK5Ivb27ia3iazn23ADibsia8Sq2wp0L0FSLSacdpJBvGoLUWtokiaXsIMvrozlicIA/640?wx_fmt=png&from=appmsg)

这里面也许就有你的贡献。

Arena-Hard GitHub：  
https://github.com/lm-sys/arena-hard  
Arena-Hard HuggingFace：  
https://huggingface.co/spaces/lmsys/arena-hard-browser  
大模型竞技场：  
https://arena.lmsys.org

参考链接：  
[1]https://x.com/lmsysorg/status/1782179997622649330  
[2]https://lmsys.org/blog/2024-04-19-arena-hard/

— **完** —

**点这里 👇关注我，记得标星哦～**

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg)

