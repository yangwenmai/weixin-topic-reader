# 一次预测多个token，Meta新模型推理加速3倍，编程任务提高17%

文章作者: 量子位
发布时间: 2024-05-03 12:33
发布地: 北京
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247727655&idx=3&sn=7118b86d70594f54289700aabeee1767&chksm=e8dfcd55dfa8444312acc903fc9ec94906779e6bbe8b856649b86443fadbffc87c14b8bd868d#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtCLqq3cRRB68fqibTDMHC4oQNNwMOKQYTsnibD3v53JGxoq5NwtGufwD9ib8bm0kg1OrOYpG2F3AddBw/300

##### 梦晨 西风 发自 凹非寺  
量子位 | 公众号 QbitAI

**“预测下一个token”** 被认为是大模型的基本范式，**一次预测多个tokens** 又会怎样？

Meta AI法国团队推出“基于多token预测的更快&更好大模型”。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCLqq3cRRB68fqibTDMHC4oQOq1tsCxgfMJibrhguWFnxIAJQtYga8ic4ibMQd5bQ7VfqEgCyCFT7ec6Q/640?wx_fmt=png&from=appmsg)

多token预测模型，**在编程类任务上表现尤其突出** 。

与单token预测相比，13B参数模型在HumanEval上多解决了12%的问题，在MBPP上多解决了17%。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCLqq3cRRB68fqibTDMHC4oQebOxXAZwyQGtEGXskh4qRRyLfpxjDibOXcEiaYjh1KdIY5vh3m5D4OXA/640?wx_fmt=png&from=appmsg)

**小型算法推理任务** 上，多token预测也在分布外泛化方面带来了令人印象深刻的收益。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCLqq3cRRB68fqibTDMHC4oQrebdTqdiaw6SFJPibr0U4UkSGSdFB1tl1xEfYU4ZuzT6pPssIFwC3qng/640?wx_fmt=png&from=appmsg)

不过在自然语言任务上，多token预测方法并不能显著提高7B模型在数学选择题上的表现了。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCLqq3cRRB68fqibTDMHC4oQ0N50w8DNPpEyIurTz9YfYG1hbleF9R45uHoichs6Od8YicuKW5Z4xiaPQ/640?wx_fmt=png&from=appmsg)

另外一个好处是，即使batch size较大，使用4-token预测训练的模型，**推理速度也可提高3倍** 。

## 多token预测更适合编程

具体来说，团队设计了一种新的多token预测架构，通过n个独立的输出头并行预测n个未来token。

使用大量文本数据进行模型训练，包括代码和自然语言数据集。

再通过实验比较多token预测和单token预测在多个下游任务上的性能。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCLqq3cRRB68fqibTDMHC4oQvUppjdUGJibqKwl0vcicTZMQ9EZXXVLXicRn4xk0h9GjFxGzetzWTBMvQ/640?wx_fmt=png&from=appmsg)

为啥多token预测在编程任务和小型算法推理任务上提升更明显？

团队猜测可能有两个原因:

第一，编程语言的逻辑结构更严谨，知识的内在联系更紧密。一个关键节点可能影响到后续整个代码块的走向。多Token预测能更好捕捉这种长距离依赖。

第二，相比自然语言，编程语言的词汇量更小。因此即便每次预测多个Token，难度也没那么大。反而能迫使模型从局部细节中抽身，着眼全局优化。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCLqq3cRRB68fqibTDMHC4oQhWCX5FlfAhiaLgqibxawVAtWPNI0J5EPsXzL3rcib2McWaV3Xd5m5wkgg/640?wx_fmt=png&from=appmsg)

除了在token层面的实验，团队还在**更细粒度的字节级模型** 上做了尝试。

他们发现，用8字节预测替代下一个字节预测后，模型在MBPP上的Pass@1指标暴增67%，在HumanEval上也提升了20%。

而且推理速度还能再快6倍，简直不要太香。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCLqq3cRRB68fqibTDMHC4oQ966YGhibgOuv6pllpribCFIXW6P6RibUSQGzpUV5zphiad7vIhdFyFVMqg/640?wx_fmt=png&from=appmsg)

对于背后原理，团队认为**多token预测缓解了训练时Teacher Forcing和推理时自回归生成之间的分布差异** 。

也就是说，在训练的时候，模型看到的都是标准答案，生成的时候却得靠自己。好比人类在家做练习册时有答案，考试时却啥也没有，就会不适应。

而多token预测相当于训练时就逼着模型多想几步，这样到了考场上，才能应对自如。

**从信息论的角度，团队还给出了一个更精确的论证。**

传统的下一个Token预测，目标是最小化当前位置的信息熵。而2-Token预测实际上最小化的是当前和下一位置的信息熵之和。

数学推导表明，后者其实隐含了更大的互信息权重，也就是更看重当前Token和未来Token的相关性。这就是为什么多Token预测更”有远见”。

**不过在这篇论文中，还有几个未解决的问题。**

比如没有探讨如何自动选择最佳的预测token数量n，作者提出，未来可以研究使**用损失权重调整或动态调整n来解决最佳n的选择问题** 。

此外最佳的词表大小也可能与单token预测时不同。

总之，看过这篇论文之后，大家都更期待Llama-4了。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCLqq3cRRB68fqibTDMHC4oQLsHPiavQXbwr358KbW8fwwHib70AiboEC7sKApwIdVEW9dfUkwicfONeAw/640?wx_fmt=png&from=appmsg)

论文地址：  
https://arxiv.org/abs/2404.19737

— **完** —

**点这里 👇关注我，记得标星哦～**

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg)

  

