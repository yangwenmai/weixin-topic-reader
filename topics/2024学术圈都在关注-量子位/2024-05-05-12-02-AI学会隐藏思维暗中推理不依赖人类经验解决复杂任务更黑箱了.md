# AI学会隐藏思维暗中推理！不依赖人类经验解决复杂任务，更黑箱了

文章作者: 量子位
发布时间: 2024-05-05 12:02
发布地: 北京
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247727676&idx=3&sn=ae35fb03e10e36fc469a53ea50e321a4&chksm=e8dfcd4edfa84458bdcb3e208bdfe7461d05796b3ff0b1a437df140b445a34681c6125b7051c#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtDwlBP4ZKIdDhtdI9zVLuxdq0o9hicPxQVs7T8vd3hpvXM4jnRq5kP51dYEAUS8cIdlbx2nicQCPH2Q/300

##### 梦晨 发自 凹非寺  
量子位 | 公众号 QbitAI

AI做数学题，真正的思考居然是**暗中****“心算”** 的？

纽约大学团队新研究发现，即使**不让****AI写步骤，全用无意义的“……”代替** ，在一些复杂任务上的表现也能大幅提升！

一作Jacab Pfau表示：**只要花费算力生成额外token就能带来优势，具体选择了什么token无关紧要** 。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDwlBP4ZKIdDhtdI9zVLuxdHtsPjBeiaMyqH7pfAEvLMuiaBkKtxd3EmLsKiaS7Y2zyJnd5xib6V4mqNg/640?wx_fmt=png&from=appmsg)

举例来说，让Llama 34M回答一个简单问题：**自然常数e的前6位数字中，有几个大于5的？**

AI直接回答约等于瞎捣乱，只统计前6位数字居然统计出7个来。

让AI把验证每一数字的步骤写出来，便可以得到正确答案。

让AI把步骤隐藏，替换成大量的“……”，依然能得到正确答案！

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDwlBP4ZKIdDhtdI9zVLuxdlNEugiaxLQmzHjw2icCTSfyCQYnTvia6SLYdThC8LdVZY2xcK8eYcGUDw/640?wx_fmt=png&from=appmsg)

这篇论文一经发布便掀起大量讨论，被评价为**“我见过的最玄学的AI论文”** 。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDwlBP4ZKIdDhtdI9zVLuxdqo9kib6wV3UIw7SibdrzdpNeqklnveDmT5oWLyANSyDpudz525ajn9lw/640?wx_fmt=png&from=appmsg)

那么，年轻人喜欢说更多的“嗯……”、“like……”等无意义口癖，难道也可以加强推理能力？

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDwlBP4ZKIdDhtdI9zVLuxdoWvBA33vvBcYabgFV6ddpBIcLvYNibHPicBQORTAHycQRuzzPy3RX4sw/640?wx_fmt=png&from=appmsg)

## 从“一步一步”想，到“一点一点”想

实际上，纽约大学团队的研究正是从思维链（Chain-of-Thought，CoT）出发的。

也就是那句著名提示词**“让我们一步一步地想”** （Let‘s think step by step）。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDwlBP4ZKIdDhtdI9zVLuxddlMG9aN2cYlvc3jYT1qab7WSfj5qFKBUxjtkNZnme0icyTjQ5Fvy4pA/640?wx_fmt=png&from=appmsg)

过去人们发现，使用CoT推理可以显著提升大模型在各种基准测试中的表现。

目前尚不清楚的是，这种性能提升到底源于模仿人类把任务分解成更容易解决的步骤，还是额外的计算量带来的副产物。

为了验证这个问题，团队设计了两个特殊任务和对应的合成数据集：3SUM和2SUM-Transform。

**3SUM** 要求从一组给定的数字序列中找出三个数，使得这三个数的和满足特定条件，比如除以10余0。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDwlBP4ZKIdDhtdI9zVLuxd58iaDH3Q2ibMvoAibNwnIFxUOrGCukMYd3QUMQyDVuCvE7iaF9roXoIibrw/640?wx_fmt=png&from=appmsg)

这个任务的计算复杂度是O(n3)，而标准的Transformer在上一层的输入和下一层的激活之间只能产生二次依赖关系。

也就是说，当n足够大序列足够长时，**3SUM任务超出了Transformer的表达能力** 。

在训练数据集中，把与人类推理步骤相同长度的“...”填充到问题和答案之间，也就是AI在训练中没有见过人类是怎么拆解问题的。‍‍‍‍‍‍‍‍‍

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDwlBP4ZKIdDhtdI9zVLuxd8I5YiclicbRPs4icQ3rUamZDsoF3ibVibCP80ldXhqdl7FWFV8q0iaul55tw/640?wx_fmt=png&from=appmsg)

在实验中，不输出填充token“…...”的Llama 34M表现随着序列长度增加而下降，而**输出填充token时一直到长度14还能保证100%准确率**
。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDwlBP4ZKIdDhtdI9zVLuxdgfIf0aNCODPXaicQiaXKdEib94VAUrhYh9vfHOoS68E2Mf8MlKyDoZibfQ/640?wx_fmt=png&from=appmsg)

**2SUM-Transform** 仅需判断两个数字之和是否满足要求，这在 Transformer 的表达能力范围内。

但问题的最后增加了一步“对输入序列的每个数字进行随机置换”，以防止模型在输入token上直接计算。

结果表明，**使用填充token可以将准确率从 78.7%提高到93.6%** 。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDwlBP4ZKIdDhtdI9zVLuxdOflqJiccgAmKibxQCCKfibVUEibVpG1B3ORNghdRqpGRBT6xibYBmK1kgow/640?wx_fmt=png&from=appmsg)

除了最终准确率，作者还研究了填充token的隐藏层表示。实验表明，冻结前面层的参数，只微调最后一个Attention层，**随着可用的填充token数量增多，预测的准确率递增**
。

这**证实了填充token的隐藏层表示确实包含了与下游任务相关的隐性计算** 。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDwlBP4ZKIdDhtdI9zVLuxdoFTRNNTTAGKN2dLlVrS7zqEtgZy5D6a9t8OKDtW2n6mD9kRBibv7C3Q/640?wx_fmt=png&from=appmsg)

## AI学会隐藏想法了？

有网友怀疑，这篇论文难道在说“思维链”方法其实是假的吗？研究这么久的提示词工程，都白玩了。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDwlBP4ZKIdDhtdI9zVLuxdsX5Wy31ZGDyBtTuLSkicaxJaibPTdfX9ibj2d2pAkWhaWQgTiaHcJfjjibw/640?wx_fmt=png&from=appmsg)

团队表示，从理论上讲**填充token的作用仅限于TC 0复杂度的问题范围内**。

TC0也就是可以通过一个固定深度的电路解决的计算问题，其中电路的每一层都可以并行处理，可以通过少数几层逻辑门（如AND、OR和NOT门）快速解决，也是Transformer在单此前向传播中能处理的计算复杂度上限。

而**足够长的思维链，能将Transformer的表达能力扩展到TC 0之外**。

而且让大模型学习利用填充token并不容易，需要提供特定的密集监督才能收敛。

也就是说，**现有的大模型不太可能直接从填充token方法中获益** 。

但这并不是当前架构的内在局限性，如果在训练数据中提供足够的示范，它们应该也能从填充符号中获得类似的好处。

这项研究还引发了一个令人担心的问题：大模型有能力进行无法监控的暗中计算，对AI的可解释性和可控性提出了新的挑战。

换句话说，**AI可以不依赖人类经验，以人们看不见的形式自行推理** 。

这既刺激又可怕。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDwlBP4ZKIdDhtdI9zVLuxdmZkibONxqIb9xQMLoUicRGVgDt26ZPAb66PFUFPkbib2iceqV6N3UQcgbQ/640?wx_fmt=png&from=appmsg)

最后有网友开玩笑提议，让Llama 3首先生成1千万亿点点点，就能得到AGI的权重了（狗头）。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDwlBP4ZKIdDhtdI9zVLuxd4aYG3qicLL9iamWATByTv3fPGdfWJ6YjjcQgVBPicFKuT9NHcMwibl2Vsw/640?wx_fmt=png&from=appmsg)

论文：  
https://arxiv.org/abs/2404.15758

参考链接：  
[1]https://x.com/jacob_pfau/status/1783951795238441449  
[2]https://x.com/johnjnay/status/1784261779163349110

— **完** —

**点这里 👇关注我，记得标星哦～**

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg)

