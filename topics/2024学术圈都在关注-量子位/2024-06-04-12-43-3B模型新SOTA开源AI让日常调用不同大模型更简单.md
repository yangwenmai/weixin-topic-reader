# 3B模型新SOTA！开源AI让日常调用不同大模型更简单

文章作者: 量子位
发布时间: 2024-06-04 12:43
发布地: 北京
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247732015&idx=3&sn=595be2caab9860b2ebd8e2140ff0917d&chksm=e8dffc5ddfa8754b3b05a64e851884e5bc23324da86d719457f6027d7aaa46c01bb1db1922b6#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtA3QOia8KQeKGdukOosQPw8ibAFpGzDA1qGbbIMMxt4uWTibG5I220ftOBFlZHLXhDTFwzVwDC04zeyg/300

##### NEXA AI 投稿  
量子位 | 公众号 QbitAI

大模型，大，能力强，好用！

但单一大模型在算力、数据和能耗方面面临巨大的限制，且消耗大量资源。

而且目前最强大的模型大多为闭源，对AI开发的速度、安全性和公平性有所限制。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtA3QOia8KQeKGdukOosQPw8ibTb6VfshYPvuTJsrZTXPKtUrJO24DnOOXP2Tu5R5MibH9bib7scJqEFGw/640?wx_fmt=png&from=appmsg)

AI大模型的未来发展趋势，需要怎么在单一大模型和多个专门化小模型之间做平衡和选择？

针对如此现状，两位斯坦福校友创办的NEXA AI，提出了一种新的方法：

**采用functional token整合了多个开源模型，每个模型都针对特定任务进行了优化。**

他们开发了一个名叫Octopus v4的模型，利用functional token智能地将用户查询引导至最合适的垂直模型，并重新格式化查询以实现最佳性能。

介绍一下，Octopus v4是前代系列模型的演化，擅长选择和参数理解与重组。

此外，团队还探索了使用图作为一种多功能数据结构，有效地协调多个开源模型，利用Octopus模型和functional token的能力。

通过激活约100亿参数的模型，Octopus v4在同级别模型中实现了74.8的SOTA MMLU分数。

## Octopus系列模型

这里要重点介绍一下**Octopus-V4-3B** 。

它拥有30亿参数，开源，是Nexa AI设想中的语言模型图的主节点。

该模型专为MMLU基准测试话题定制，能够高效地将用户查询转换成专业模型可以有效处理的格式。

它擅长将这些查询正确引导至相应的专业模型，确保精确且有效的查询处理。

Octopus-V4-3B具备以下特点：

  * **紧凑尺寸：** Octopus-V4-3B体积紧凑，使其能在智能设备上高效、迅速地运行。

  * **准确性：** 利用functional token设计准确地将用户查询映射到专业模型，提高了其精度。

  * **查询重格式化：** 帮助将自然人类语言转换为更专业的格式，改善了查询描述，从而获得更准确的响应。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtA3QOia8KQeKGdukOosQPw8ibmanMKl8wFBrBhibpjKFlgH9XwKrWXicJVNY8eZSnLtlZyxxPQGoQdjEA/640?wx_fmt=png&from=appmsg)

Nexa AI把语言模型作为图中的节点整合，并提供了针对实际应用定制的系统架构。

此外，讨论了使用合成数据集对Octopus模型进行训练的策略，强调了这种语言模型图在生产环境中的系统设计。

## 从Octopus v2提取的用于分类的语言模型

研究人员在Octopus v2论文中介绍了一种名为functional token的分类方法。

Octopus v2模型有效地处理了这一任务：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtA3QOia8KQeKGdukOosQPw8ibjGhtXWxTYsLJMJaV4QLfrQtyL9E2jdQx0xcdSTjvNZapHtOXt3micaw/640?wx_fmt=png&from=appmsg)

#### 图中的语言模型作为节点

考虑一个定义为：G=(N，E)。

其中N代表图中的各种节点，E代表连接这些节点的边。

节点分为两种类型：

**一，主节点Nm，** 它们通过将查询定向到合适的工作节点Nω并传递执行任务所需的信息来协调查询。

**二，工作节点，** 接收来自主节点的信息并执行所需的任务，使用Octopus模型来促进进一步的协调。

节点信息传输过程如下图所示。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtA3QOia8KQeKGdukOosQPw8ibFYlTGLAUfAibl9K19OicuJdGEtM3LcMic1uMmicUwx9MpSDAFdXdgbBRJw/640?wx_fmt=png&from=appmsg)

为了处理用户查询q并生成响应y，研究人员将概率建模为：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtA3QOia8KQeKGdukOosQPw8ibzqGK7StP5KJO4Tr2vcq9JNBOtGakXRpQT9kZRzUnib6IEaSkppMtyCw/640?wx_fmt=png&from=appmsg)

对于只涉及一个工作节点的单步任务，该过程可以定义为：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtA3QOia8KQeKGdukOosQPw8ib7oEicP8XIn6ykuJWeAQqGRGvwvVpGmwMBpJBr1GQicIcmpgVMRYyUohA/640?wx_fmt=png&from=appmsg)

这里，P(Nω,ph|q;Nm)使用Octopus v2模型为𝑁m选择最佳的相邻工作节点并重新格式化查询为𝑞ℎ，这是重构的查询。

概率P(y|qh;Nω)由位于工作节点的语言模型计算。

对于多步任务，通常在多代理工作流中，该过程涉及多个节点之间的几次顺序交互，如下：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtA3QOia8KQeKGdukOosQPw8ibglgTBAv2IqlicNdMOnQLOLu7lLqLrCxt29t1PyRFhSs1TgKLwiaRianSQ/640?wx_fmt=png&from=appmsg)

#### 使用图进行多步骤操作的任务规划

多步骤任务规划中，所有功能列在上下文中提交给语言模型，生成基于用户查询的计划。

**传统方法在处理长功能描述时有局限性** ，尤其是参数少于10B的模型。

基于图的方法确保只考虑与特定节点相关的邻近节点，显著减少了选择的复杂性。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtA3QOia8KQeKGdukOosQPw8ibBtib71nXdwnN95x7lQmD9KIRFZ8vbaLgP2slHib3O9QLqmp2grrxmfaQ/640?wx_fmt=png&from=appmsg)

#### 语言模型图的系统设计

以下详细说明了复杂图中每个节点代表一个语言模型的系统架构，利用多个Octopus模型进行协调。

在准备生产部署时，整合一个负载均衡器以有效管理系统需求至关重要。

然后，研究团队将系统划分为几个可管理的组件，强调核心方法：

**首先是工作节点部署。**

每个工作节点Nω对应一个单独的语言模型。

团队建议为这些节点采用无服务器架构，特别推荐使用Kubernetes进行基于内存使用和工作负载的强大自动缩放。

**其次是主节点部署。**

主节点应使用不超过10B参数的基础模型（实验中使用3B模型），以便在边缘设备上部署。

每个工作节点与一个Octopus模型接口，以增强协调。

如Octopus v2所示，可以集成一个紧凑的Lora模型以扩展functional token的能力。

建议使用单一基础模型并补充多个Loras，每个工作节点一个。

推荐使用开源的LoraX库来管理这种配置的推理操作。

**再者是通讯。**

工作节点和主节点分布在不同设备上，不限于单一设备。

因此，互联网连接对于节点之间的数据传输至关重要。

虽然主节点可能位于智能设备上，工作节点则托管在云端或其他设备上，结果返回智能设备。

为了支持数据缓存需求，包括聊天历史存储，推荐使用Redis，一个高性能的内存数据库，促进分布式缓存。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtA3QOia8KQeKGdukOosQPw8ibdqib3YQLeGOtBBBzROho1wCkR5icGcBTQicj33lmfhsgkQvcNAUpwuNCg/640?wx_fmt=png&from=appmsg)

## 实验

研究人员详细介绍了框架进行的实验，通过多节点协作提高语言模型性能。

采用了17种不同的模型进行MMLU任务，Octopus v4模型将用户查询定向到相关的专业模型并适当重格式化。

MMLU包含57个独特的任务，分为17个整合组。

专业模型根据基准得分、人气和用户推荐从Hugging Face精选。

并非所有任务都有专门模型，例如人文学科和社会科学目前无专门模型，但Llama3模型通过系统提示调整模拟专业能力。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtA3QOia8KQeKGdukOosQPw8ib9LdLSlAoWLaH8yeIPnE3qrlhZPiaJgbarJhJiavE6hFT9afgoiaibOSkdQ/640?wx_fmt=png&from=appmsg)

## 未来工作与讨论

当前，NEXA AI的GitHub 项目专注于开发语言模型的图形框架，目前处于起始阶段。

团队计划通过整合多种垂直特定模型并加入Octopus v4模型来增强这一框架，以多代理能力为特征。

未来版本将在此存储库中展示更强大的图形表示。

GitHub Repo将由Nexa AI维护，团队今后旨在为多种垂直领域开发紧凑、专门化的模型。

与更大模型的缩放法则、数据集相比，NEXA AI的框架无限制，并且可以创建一个大型图。

此外，团队**正在开发 多模态模型Octopus 3.5**，能够处理视觉、音频和视频数据；完成开发后，将被纳入图形框架。

论文链接：https://arxiv.org/pdf/2404.19296  
GitHub： https://github.com/NexaAI/octopus-v4  
试用Octopus v4：https://huggingface.co/NexaAIDev/Octopus-v4

— **完** —

  

投稿请发邮件到：

**ai@qbitai.com**

标题注明【投稿】，告诉我们：

你是谁，从哪来，投稿内容‍

附上论文/项目主页链接，以及联系方式哦

我们会（尽量）及时回复你

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtC5nGy7YMGhQ0ZJeyibWyL0KVCtiaLEPMyd4Bszuo0bFIOxZOvdmqdxnOosYXyu5aI7MXpyUrUWfz6g/640?wx_fmt=gif&tp=webp&wxfrom=5&wx_lazy=1)

  

**点这里 👇关注我，记得标星哦～**

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

