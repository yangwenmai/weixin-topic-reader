# 多模态模型学会打扑克：表现超越GPT-4v，全新强化学习框架是关键

文章作者: 量子位
发布时间: 2024-06-04 12:43
发布地: 未知地区
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247732015&idx=4&sn=5ca6ed3594bd47e3c77ce415d84b8f0d&chksm=e8dffc5ddfa8754b5008cc038198b5d38b2c98bea25b40cf7e4a3e923c6d25fbfb9d073863e5#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtBXIHcMRMgAfZlUVQmDxMKy0iamJtZ9oKBLuYyzLv0ZuybudRyiazWHziag4AyQZqBeAfI9y0ib5PP6OQ/300

##### Simon Zhai 投稿  
量子位 | 公众号 QbitAI

只用强化学习来微调，无需人类反馈，就能让多模态大模型学会做决策！

这种方法得到的模型，已经学会了看图玩扑克、算“12点”等任务，表现甚至超越了GPT-4v。

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtBXIHcMRMgAfZlUVQmDxMKyalFppGk621iaoKuIZcIZWpWMC1C1C4RvNeUPtnvsdicKWUY6CuzmZ80Q/640?wx_fmt=gif&from=appmsg)

这是来自UC伯克利等高校最新提出的微调方法，研究阵容也是相当豪华：

  * 图灵奖三巨头之一、Meta首席AI科学家、纽约大学教授LeCun

  * UC伯克利大牛、ALOHA团队成员Sergry Levine

  * ResNeXt一作、Sora基础技术DiT作者谢赛宁

  * 香港大学数据科学学院院长、UC伯克利教授马毅

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBXIHcMRMgAfZlUVQmDxMKyP4GJJRrvEiax4LQ7bXXLY0smu9OZQWBWMVLesJ4xYHp6pv4LOh6SxVg/640?wx_fmt=png&from=appmsg)

该方法名为RL4VLM，论文预印本已经上线，相关代码也已在GitHub中开源。

RL4VLM提出了一种新的算法框架，直接使用强化学习方法对多模态大模型进行微调。

其中奖励信息直接来源于环境当中，摆脱了RLHF中对于人类反馈的需要，从而直接赋予了多模态模型决策能力。

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtBXIHcMRMgAfZlUVQmDxMKyrFGGeUiaT3L4HdUQPdKSyGYHdia4obTN1p6gJx36ncqvcbGuZDbsszMg/640?wx_fmt=gif&from=appmsg)

对于RL4VLM的意义，参与了这项工作的马毅教授这样说：

> 一方面希望大家对模型真实性能有更客观清醒的认识；  
> 另一方面，也希望能建立一个平台，支持探索如何进一步提升模型性能。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBXIHcMRMgAfZlUVQmDxMKy7yhsTDBiciaqK7uib8Q8rzkuVK4eg5fO8rno3gMKn2IbpACHbH7hibibh3A/640?wx_fmt=png&from=appmsg)

那么，用这种方法微调出来的多模态大模型，都能让智能体学会哪些能力呢？

## 多模态决策能力超GPT-4v

为了评估训练出的多模态大模型给智能体带来的能力，作者一共使用了两类物种评测任务：

  * 第一类任务（a-d） 主要考验模型利用图像中的细粒度视觉信息做决策的能力，包括对于数字的识别能力和利用识别的数字进行逻辑推理的能力

  * 第二类任务（e）主要考察多模态大模型在具身智能环境中的视觉语义推理能力。

具体来说，这五个任务分别是：

  * a.数轴（Numberline）：模型需要通过输出“+” 或者 “-”，将当前数字移动到目标数字

  * b.简易12点（EZPoint）：模型需要识别两张牌，并用加号和乘号运算“12点”

    * c.24点（Point24）: 模型需要识别四张牌，并用加减乘除和括号运算“24点”

  * d.21点（Blackjack）：模型需要通过牌面上的信息来决定“要牌”或者“停牌”

  * e.ALFWorld：一个标准具身智能环境

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBXIHcMRMgAfZlUVQmDxMKy9RUxTibD4dZwiboib3FJfTUUEoc9A7ic1ZmBSxS4mDARthUHM4efawJ2hg/640?wx_fmt=png&from=appmsg)

其中任务a-d为作者的原创任务，任务e的ALFWorld是微软等于2020年提出的开源具身智能任务集。

实验结果表明，直接使用强化学习微调7B的多模态模型之后，能使其在两类决策问题上的表现超过商用模型GPT-4v
Gemini，同时也能超过传统的监督微调（SFT）方法。

而在ALFWorld的具身智能任务中，作者的模型也取得了最高的平均分，特别是在单物体拾取任务上表现尤为突出。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBXIHcMRMgAfZlUVQmDxMKyJylA0AsvL2mIicH3VsJYvVGlyJd8LkEiac5ao6iczhlEqBG8r0gmyIeew/640?wx_fmt=png&from=appmsg)

## 先生成思维链，再做决策

这套VLM智能体主要解决的是需要视觉识别和语言理解的任务，它的工作流程是这样的：

首先，对于每一个任务，系统会直接将该任务的当前状态，以图片和文字描述的形式输入多模态大模型，并要求模型输出一段思维链之后，再以文字形式输出要执行的动作。

最后将，动作信息会被输入进对应的环境并获得奖励值，该奖励值会被用来进行强化学习训练。

例如下图中，智能体在执行玩21点的任务时，系统直接要求多模态模型根据目前的状态，在输出思维链之后选择“停牌”
（stand）或者“拿牌”（hit），然后直接将对应的动作输入到环境中，得到奖励函数值以及下一个状态。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBXIHcMRMgAfZlUVQmDxMKybbchB74gtLcPaYsUYFUfYsbpgIuQStH3NuiaAmP14gDA9k6ZZCuqRvQ/640?wx_fmt=png&from=appmsg)

为了能用直接将强化学习运用到多模态模型的训练中，需要对模型的输入和输出做一些调整，以适应RL训练框架中。

具体来说，作者将任务图像o和任务描述的文本v-in合并后，直接作为当前任务的状态s，即：

> s = [o, v-in]

在获得了多模态模型的文字输出v-out以后，该框架直接将其中文字形式的动作(“action: {act}”) 转化为可与环境交互的动作指令a。

接下来把a输入到环境当中，就能获得奖励函数r，以及操作后的下一个状态。

在获得了来自环境的奖励函数r之后，文章利用PPO直接对整个多模态模型进行微调。

而从提示词上看，这项研究采取了如下的提示过程作为多模态模型的输入，并且给出了期望的输出形式：

（其中蓝色的部分是让模型生成思维链提示过程， 红色的部分是告诉模型以文字形式输出动作a）

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBXIHcMRMgAfZlUVQmDxMKyIg5SGQmLvM3ecfKUeGp0qDyic3ZjwXwX0Rx93JnUoIhl1NmvIeFdxYg/640?wx_fmt=png&from=appmsg)

消融实验结果表明，如果这一过程中不采用思维链，则任务成功率会出现大幅下降。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBXIHcMRMgAfZlUVQmDxMKykRibnnAqXc1kvpicprdXJ3pS3UWbQia44cTXwnibdHq78rceaVxtT5x4HA/640?wx_fmt=png&from=appmsg)

论文地址：  
https://arxiv.org/abs/2405.10292  
GitHub：  
https://github.com/RL4VLM/RL4VLM

— **完** —

  

投稿请发邮件到：

**ai@qbitai.com**

标题注明【投稿】，告诉我们：

你是谁，从哪来，投稿内容‍

附上论文/项目主页链接，以及联系方式哦

我们会（尽量）及时回复你

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtC5nGy7YMGhQ0ZJeyibWyL0KVCtiaLEPMyd4Bszuo0bFIOxZOvdmqdxnOosYXyu5aI7MXpyUrUWfz6g/640?wx_fmt=gif&tp=webp&wxfrom=5&wx_lazy=1)

  

**点这里 👇关注我，记得标星哦～**

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

