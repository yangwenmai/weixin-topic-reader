# GPT-4o差点没及格！首个多任务长视频评测基准，它有亿点难

文章作者: 量子位
发布时间: 2024-06-21 12:56
发布地: 北京
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247734853&idx=4&sn=b9ed3f3a16509c7acc74d08c32277185&chksm=e8dfe137dfa86821007f91dc8538e545abe8f4246c1ff8e9a0cd14f25b98b4d92ab28e97971f#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtBF88ult7g9icAe9orbWZpkTkic1elX2eJoCCMzd40VTBqHYxfhvt7kia59XlIR0T8iaeMunLQCmDfIng/300

##### MLVU团队 投稿  
量子位 | 公众号 QbitAI

**难度大升级** 的多任务长视频理解评测基准**MLVU** 来了！

由智源联合北邮、北大和浙大等多所高校推出。

究竟有多难呢？最终排名第一的**GPT-4o** 单选正确率还**不足65%** 。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBF88ult7g9icAe9orbWZpkTWUM6x4G722vG2GH1XOPiaGNOicbicpYko01icNPV1DXp8HEuqAmEuLBDDg/640?wx_fmt=png&from=appmsg)

而且研究发现，大部分模型的性能都会**随着视频时长增加显著下降** 。

研究进一步证明，提升上下文窗口，提升图像理解能力，以及使用更强大的LLM Backbone对长视频理解的性能具有显著的提升作用。

目前相关论文及数据集已公开，具体细节下面一起看看吧~

## MLVU的构建过程

当前流行的Video Benchmark主要针对**短视频** 设计，大部分视频的长度都在**1分钟以内** 。

且现有评测基准往往专注在**特定领域的视频** （例如电影、第一视角）和**特定的视频评测任务** （例如Captioning，Temporal
Perception，Action Understanding）。

此外，现有部分长视频理解评测任务往往**只和局部帧** 有关，或者**针对经典电影** 进行问答，这导致MLLMs可以直接凭借text
prompt正确回答而无需对视频进行分析。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBF88ult7g9icAe9orbWZpkT5k4pm4qTyrCfdic2debktvUjqho3IKTUd0kT4o8LCicpQsrPbBE0qLkw/640?wx_fmt=png&from=appmsg)

针对以上不足，新基准MLVU从以下**3个层面** 进行构建：

### 时长和来源更丰富

MLVU的视频时长覆盖了**3分钟到超过2小时** ，平均视频时长**12分钟** ，极大扩展了当前流行的Video Benchmark的时长范围。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBF88ult7g9icAe9orbWZpkTB61mpGtzrWOmGX7yj8SNZhib21WU42o0B8AicJpKeRXQ6yaMJNjQdnDw/640?wx_fmt=png&from=appmsg)

另外，MLVU的大部分任务标注过程中进行了**片段-问题对应标注** 。

例如，Video Summarization任务分段标注了视频的前3分钟，前6分钟……

这意味着，MLLMs可以灵活地在MLVU上选择测试**不同时长情况下的长视频理解能力** 。

同时，MLVU收集了包括电影、电视剧、纪录片、卡通动画片、监控视频、第一视角视频和游戏视频等多个类型的长视频，覆盖了长视频理解的多个领域范围。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBF88ult7g9icAe9orbWZpkTdDbdX1qZmia2183GxpoibBic9RxNe8cULRXGhiajH2BZgicN92LSDNNWpuA/640?wx_fmt=png&from=appmsg)

### 任务类别更全面

团队针对长视频理解设计了**9类** 不同的任务，并进一步将任务分为三类：全面理解、单细节理解、多细节理解。

  * **全面理解任务** ：要求MLLMs理解和利用视频的全局信息来解决问题

  * **单细节理解任务** ：要求MLLMs根据问题定位长视频中的某一细节，并利用该细节来解决问题

  * **多细节理解任务** ：要去MLLMs定位和理解长视频中的多个相关片段来完成和解决问题

此外，还包括了**单项选择题** 和**开放生成式** 问题，全面考察MLLMs在不同场景下的长视频理解能力。

以下为9大任务的示例：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBF88ult7g9icAe9orbWZpkTFd7k4ib7RBjGWn32FUUFuVicKJf5EcsiapE7SGqVxS9wfveDHJ6mUzxkA/640?wx_fmt=png&from=appmsg)

### 问题设置与答案标注更合理

为了突出新旧基准变化，直接以**情节问答** （Plot Question Answering）任务为例。

假如以电影、电视的角色作为问题线索来对MLLMs进行提问，旧基准的**常见问题** 有两种。

一是挑“经典”下手，这导致MLLMs在没有对视频进行分析的情况下，直接使用了自有知识回答问题。

另一部分试图避免这个问题，但由于长视频的复杂性，仅仅利用代词和描述性语句来指代情节细节非常困难。

他们的**问题非常宽泛** 或者**需要在问题中额外指定具体的时间片段** 而不是让MLLMs自己根据题目寻找对应细节。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBF88ult7g9icAe9orbWZpkTCdBnnia9Oq8uA4oMAHeEjsQGOaEm3yKcL6hTnGRxquc6LOvKRXdNG7A/640?wx_fmt=png&from=appmsg)

MLVU通过精细的**人工标注** 克服了这些问题。

在所有的情节问答任务中，MLVU均使用**“具有详细细节的代词”**
来指代情节中的人物、事件或背景，避免了问题泄露带来的潜在影响，MLLMs需要根据问题提供的线索识别和定位相关片段才能进一步解决问题。

此外，MLVU的Plot QA问题具备丰富的多样性，增强了评测的合理性和可靠性。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBF88ult7g9icAe9orbWZpkT9ntcVxdjbdPxFkss4HMTZgB22vFfiaqWk5x2PY8DjIafvyZ0ZwoQMYQ/640?wx_fmt=png&from=appmsg)

## 模型在MLVU上的表现

团队在MLVU上对**20个流行的MLLM** 进行了评测，包括开源模型和闭源模型。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBF88ult7g9icAe9orbWZpkTiadUNrF9FSZJ4vro9iaicjAMq6Z61HQA0vSFpNFDvyJTbX4bic0s0DLc5A/640?wx_fmt=png&from=appmsg)

实验结果表明，尽管GPT-4o在所有任务中均取得了第1名，但它的**单选平均准确率只有64.6%** 。

且所有模型都在需要**细粒度理解能力** 的任务上（单细节、多细节理解任务）表现糟糕。

此外，大部分模型的性能都会随着视频时长增加**显著下降** 。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBF88ult7g9icAe9orbWZpkTErgp6tmtYmXsPic24ISFSdZVaibN6K308h0wu2p6xEFhmbdmnX8EmC4w/640?wx_fmt=png&from=appmsg)

另一明显结论是，开源模型和闭源模型之间存在**较大的差距** 。

开源模型中**单项选择题性能最强的InternVL-1.5** 单选平均准确度仅有50.4%；**开放生成式题目最强的LLaMA-Vid**
得分仅有4.22，均远远落后于GPT-4o的64.6%和5.80。

不过研究发现，**提升上下文窗口** ，**提升MLLM的图像理解能力** ，以及**使用更强大的LLM Backbone**
对长视频理解的性能具有显著的提升作用。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBF88ult7g9icAe9orbWZpkToZfQKHo9bu4Gxic2IHO3VICiaNP1LcNypsYDg6WRg3rzqZ50bhWdAayA/640?wx_fmt=png&from=appmsg)

这揭示了未来MLLMs在提升长视频理解能力的重要改进方向。

论文：  
https://arxiv.org/abs/2406.04264  
项目链接：  
https://github.com/JUNJIE99/MLVU

— **完** —

  

投稿请发邮件到：

**ai@qbitai.com**

标题注明【投稿】，告诉我们：

你是谁，从哪来，投稿内容‍

附上论文/项目主页链接，以及联系方式哦

我们会（尽量）及时回复你

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtC5nGy7YMGhQ0ZJeyibWyL0KVCtiaLEPMyd4Bszuo0bFIOxZOvdmqdxnOosYXyu5aI7MXpyUrUWfz6g/640?wx_fmt=gif&tp=webp&wxfrom=5&wx_lazy=1)

  

**点这里 👇关注我，记得标星哦～**

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

  

