# 无需人类或GPT-4打标签！南大&旷视研究院无监督范式大幅降低视觉大模型对齐成本

文章作者: 量子位
发布时间: 2024-06-23 11:35
发布地: 北京
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247735118&idx=4&sn=25b864b55a8adfdd9ab4173c205a3adf&chksm=e8dfe03cdfa8692aaeb7eb4c6b39db093bc698ec9da1a6b283b4814ddebe4c3844c716e17b34#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtCic5s5xNYGvHXwg5FOje4o0TQ0qIGibicaOMWIvUf4VicPcdNQ6clhdbqpLK9f2RTFgUhMm7L3gicV9CA/300

##### 旷视研究院 投稿  
量子位 | 公众号 QbitAI

不用打标签，也能解决视觉大模型的偏好对齐问题了。

南大与旷视研究院的研究人员，推出了适用于VLM的无监督范式。

对比偏好对齐前后，可以发现模型的输出发生了显著的变化。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCic5s5xNYGvHXwg5FOje4o0PtfsfoiauVicBvsZn2nqCFuHEua2NiaKiaavibxDLmhXIz26kYY1C8jot7w/640?wx_fmt=png&from=appmsg)

目前的视觉大模型已经比较成熟，但作者发现它们在用户体感方面仍然有所欠缺。

于是团队经过研究，通过构造偏好样本对的方式解决了视觉语言模型的偏好对齐问题，并提出了Self-Supervised Visual Preference
Alignment（SeVa）范式。

该范式基于LLaVa-1.5-7B/13B完成，整个过程无需GPT-4或者是人类参与打标签，目前项目已经开源！

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCic5s5xNYGvHXwg5FOje4o0d0AP1r3rF7WiaK8icuDtATRHlibZy8HOszrzJq9Ye6KJM5xcXSubaXrwQ/640?wx_fmt=png&from=appmsg)

## 构建正负样本对比数据集

目前视觉大模型基本上在流程上已经非常成熟——预训练+指导监督微调（SFT）+对齐（可选）。

去年下半年开始，工业界和学术界主要聚焦在多模态大模型的数据（数据构造，配比，打标签）和模型结构（Connector，打开模型权重等）的设计上，目标是提升VLM的理解能力（传统QA+多模态benchmark）。

但是，研究团队发现部分开源大模型，虽然在跑分时有不错的性能，但在用户体感方面会比较欠缺——不遵循指令，产生幻觉回答，违背3H准则（helpfulness,
harmless, honest）等问题纷纷出现。

研究团队认为，多模态对齐的一大难点，在于偏好数据的构造。

主要的原因是，纯NLP领域的偏好数据非常昂贵且稀缺（一般需要GPT-4或者人类的参与），Vision-
Language领域的偏好数据还没有形成一个成熟的pipeline（数据构造方式，数据质量，数据的效果都还没完全得到验证）。

因此，本文首次提出一套自动化构造偏好数据的pipeline用于Alignment的训练。作者通过严格的实验，从多个角度展示了该pipeline对多模理解和用户友好性的提升。

研究当中，作者发现VLM对于图像层面的扰动非常敏感，也就是说，轻微的图像增广就会使得VLM对同一个Question产生错误且不同的回答。

具体来说，作者将多种图像层面的扰动分别作用于LLaVA-1.5的测试阶段，并在3个常规的多模态benchmark上运行，得到的结果如下：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCic5s5xNYGvHXwg5FOje4o0r2FEzicy0q5GbUbQOcbTbzxPIJQ0XRaHHjeKmdNmDgMNwN4HkUEA9Tg/640?wx_fmt=png&from=appmsg)

因此SeVa将原始图像产生的回答作为正样本，将增广后的图像产生的回答作为负样本，用于构造DPO的数据集并训练。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCic5s5xNYGvHXwg5FOje4o0OHmthQsanCUPTF2EELYgKZ3AFib47HwXGjBEYlkPypE1ZkNicF0FYeEg/640?wx_fmt=png&from=appmsg)

###### **△** SeVa的6行伪代码实现

如果以流程图的形式来展示，SeVa的工作流如下：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCic5s5xNYGvHXwg5FOje4o0ibzb8ZLqch2VELEq5MZibK7P1lSz9r49XUQDIqSBs77BvqSxsmWEf5Og/640?wx_fmt=png&from=appmsg)

具体来说，作者使用LLaVA665k
数据集中的TextVQA和OCRVQA来构造DPO数据，基于7B和13B的LLaVA-v1.5模型，使用其pretrained+SFT作为DPO的初始化权重，结合LoRA训练语言模型，r默认在512/1024。

实验结果表明，仅仅使用8k构造的无监督的数据能够显著提高VLM的指令遵循能力、降低幻觉，并且在多模态等benchmark上提升明显。

而且构造过程轻而易举、成本低廉，不需要任何人类或者是GPT-4的标注。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCic5s5xNYGvHXwg5FOje4o0vK3jNXqy8CsdVxTOsmY0TNgKw5SiaoVcyGypuTY9gVrD2eydHYDozJw/640?wx_fmt=png&from=appmsg)

另外，作者还系统阐述了在DPO训练中用到的偏好分布与对比损失之间的关系。他们的形式在一定程度上是一致的，但是核心区别在于负样本的定义。

和对比学习统一之后的好处是，可以轻易的通过对比学习的思路，在DPO中添加更多由SeVa构建的负样本对，从而推导出一个更加通用的DPO形式。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCic5s5xNYGvHXwg5FOje4o0AY0SQ8PE8w2JH8ibY8g7H5stKtKoA08MwicDRuoVMVd54egqqtX7LbFw/640?wx_fmt=png&from=appmsg)

## 让视觉模型更符合人类偏好

在9个benchmark上，SeVa几乎都能够做到稳定的提升，特别是在GPT-4评估的MMVet,和LLaVA-
bench上提升显著，在用于评估幻觉的指标POPE、SHR上也有稳定的性能提升。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCic5s5xNYGvHXwg5FOje4o08pHKxG8xUzZLBaicLmD2ANtnpjK51rRAYpjT5iaBTjU2tZ57U8nl1Osw/640?wx_fmt=png&from=appmsg)

进一步实验表明，SeVa
DPO的范式比SFT在微调VLM上具有更大的优势，例如训练时间更短、数据量更少、pipeline无需监督等，另外再性能上也有所提升。

换句话说，该实验也证明了Preference Alignment在某些情况会远远超过SFT的效率。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCic5s5xNYGvHXwg5FOje4o0LSw4UVXxUJJ0hFl4dhOaCBVp4BDiaDVniahxTPvrLCKQUxdUQs8WTkLw/640?wx_fmt=png&from=appmsg)

而且，经过DPO之后，SeVa的输出会更加的与模型得到的Question更加的接近。

同时，SeVa每次回答的一致性也更高，对于不同temperature的扰动拥有更强的鲁棒性。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCic5s5xNYGvHXwg5FOje4o0H3rh3LHMovJ9wPm8TMvXHwsndZgicKVEtIwmYgK98qjBXtju2oSMm4g/640?wx_fmt=png&from=appmsg)

通过可视化，作者还发现，SeVa的输出结果比原始LLaVA（未经过DPO训练）更加的优质（在win-lose的比例上明显占优）。

同时，经过DPO之后，SeVA产生了普遍比LLaVA更长更详细的回答。以上两个方面的可视化也解释了为什么SeVa能够更加的与人类的偏好对齐。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCic5s5xNYGvHXwg5FOje4o0h9hbtWHmOyswJGBpzg4qN9MTSJO0TiaFI0hU6V2xbibictMRELt01D27Q/640?wx_fmt=png&from=appmsg)

另外，本文还进行了诸多关于SeVa的细化和分析，有很多有意思的结论：

  * SeVa能够被视作一种特殊的对比学习方法。

  * SeVa构造的数据进行DPO训练后，模型会产生更长token的输出，并且抗干扰能力更强。

  * 正负样本之间的margin很重要，过大或过小都会sup-optimal。

  * 对齐过程中的LoRA参数非常关键。

论文地址：  
https://arxiv.org/abs/2404.10501  
GitHub：  
https://github.com/Kevinz-code/SeVa

— **完** —

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCic5s5xNYGvHXwg5FOje4o0H8FCAC42V9icPgXP13rSTjgPAqzicutIptiax3vwBcMzvufrwgWnkDbZA/640?wx_fmt=png&from=appmsg)

  

**点这里 👇关注我，记得标星哦～**

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg)

