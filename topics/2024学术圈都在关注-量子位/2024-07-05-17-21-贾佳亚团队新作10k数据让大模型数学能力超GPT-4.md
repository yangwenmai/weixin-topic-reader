# 贾佳亚团队新作：10k数据让大模型数学能力超GPT-4

文章作者: 量子位
发布时间: 2024-07-05 17:21
发布地: 北京
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247737621&idx=3&sn=8f39ae67d2d51bbf6e6869d94402e3f4&chksm=e8df9667dfa81f711728e276fa67341c931230cd10b20d9b4878a594e6a317ffb486ffd3fa4e#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtATxwIsb9Sj7SqvUibjrcOhG2hoooAvW5JWZkeebVcvPYomZLx8sYWnLWBkggXvs606JaF7Lw6SUwQ/300

##### 港中文贾佳亚团队 投稿  
量子位 | 公众号 QbitAI

**只要10k数据** ，就能让大模型的数学成绩增长5.6%。  

港中文贾佳亚团队推出了基于推理步骤的大模型优化策略，能够像老师教学生一样优化大模型。

利用这种方法，72B Qwen模型的数学成绩超越了GPT-4、Gemini1.5-Pro和Claude3-Opus等一众闭源模型。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtATxwIsb9Sj7SqvUibjrcOhGiclD1ANOlicyFXp53zIsrLJm31L9LrbecrXiaZFrwKnQicrkvtLhJ9oO0g/640?wx_fmt=png&from=appmsg)

老师在纠正学生错误时，不会只告诉学生最终答案错了，还会告知具体哪个步骤错了，以此快速纠正其错误。

贾佳亚团队正是学习了这一特点，将斯坦福团队推出的DPO（直接偏好优化）进一步细化，形成了逐步应用的策略**Step-DPO** 。

该方法让Qwen-72B模型在多个数据集上进步明显，同时也获得了更强的长链条推理任务能力。

## 像教育学生一样训练大模型

如何强化推理能力，一直是大语言模型领域的重要问题之一。

常见的思维链策略通过在输入提示词部分添加“Let’s think step by
step.”，来使模型在输出中完成逐步推理，但对于复杂的问题，仅通过修改提示词不足以引导模型正确解决问题。

由于复杂问题涉及的推理过程较长，有时包含数十个推理步骤，一旦其中任一步骤出错，就难以得到正确的结果。

此外，现有方案旨在通过监督式微调（SFT）阶段增加问答数据以实现更好的对齐。

然而，当SFT数据达到一定数量时，模型经常出现幻觉，性能也随之趋于饱和。

一个潜在的原因是，随着偏好输出的概率上升，非偏好输出的概率也会随之增加。

为了抑制幻觉，提升模型的事实性，斯坦福大学提出了直接偏好优化方法，其工作原理是创建基于人类偏好对的数据集，每个偏好对都包含一个输入提示、偏好输出以及非偏好输出。

然后对语言模型直接进行微调，最大限度地提高生成的可能性，并减少输出的可能性。

因此，DPO的优化目标为：

![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtATxwIsb9Sj7SqvUibjrcOhGkeSZPgrPBSj7Ntl6QvZSKMrwD3t5uc9C7z4xP4v5xmoaGa1FSSc6dw/640?wx_fmt=jpeg)

其中πθ与πref分别表示当前微调模型以及参照模型。

但在长链条推理任务中，DPO无法准确判断推理过程中的错误步骤，从而无法聚焦关键出错步骤。

如下图所示，基于DPO的模型在训练过程中无法准确判断推理步骤正确与否。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtATxwIsb9Sj7SqvUibjrcOhGwIazFXGS2R7DXt5picuVXzOUbbQ7bmvYWnVc41wh5JV363ZdvmOVGNg/640?wx_fmt=png&from=appmsg)

因此，作者提出了基于推理步骤的直接偏好优化——**Step-DPO** 。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtATxwIsb9Sj7SqvUibjrcOhG0TXTTk44VYX3iaQFnrVWHWqPSX3c73RKCsHddk4Dpy74ejqF5KBeEzA/640?wx_fmt=png&from=appmsg)

就像老师在纠正学生错误时，不会只告诉学生最终答案错了，**还会告知具体哪个步骤错了** ，以此快速纠正其错误。

与此类似，Step-DPO不再像DPO从整体上对比答案，而是**将每个推理步骤视为一个基本单元**
，并且对比单个推理步骤，从更精细的角度提升模型的多步推理分析能力。

Step-DPO的优化目标为：

![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtATxwIsb9Sj7SqvUibjrcOhGPokph3sGLb5NocnOgKbUVnd0VsA8RcKiaIcug2CUUBaJsclUsudttUw/640?wx_fmt=jpeg)

除此之外，作者还提出基于模型自生成的数据处理流程。如图所示，该流程包含以下三个步骤：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtATxwIsb9Sj7SqvUibjrcOhGttEprxErd4dLqlll1VIiapOLeV2x9Q5zj8UE7AiadcibNU65M6EYfHTrQ/640?wx_fmt=png&from=appmsg)

第一步是**错误收集** 。

首先，给定一组数学问题D0=(x,y∧)，其中x是数学问题，y∧是其真实答案。

然后，使用初始模型πref来得到每个数学问题x的答案。

在进行模型推理之前，需要添加思维链（CoT）前缀作为提示，以确保模型的推理结果被结构化为多个推理步骤，每个步骤均以“Step i：”开始。

经过模型推理可得到每个数学问题x的推理结果y，然后选择与真实答案y∧不一致的那些结果，并汇总得到数据集D1：

![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtATxwIsb9Sj7SqvUibjrcOhGL0xYA6KRQpb1kc9aHfHYcc8EwMpkb08cwlhqxIic0ny4gdjTB6LAw9A/640?wx_fmt=jpeg)

第二步是**错误步骤定位** 。

每个错误推理结果y都呈现为一系列推理步骤的序列y=s1,s2,…,sn，随后需要人工或利用GPT-4验证每个推理步骤的正确性，直到找到第一个错误步骤sk，并记录其步骤编号。

然后将sk选为错误的推理步骤slose，从而得到D2：

![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtATxwIsb9Sj7SqvUibjrcOhGI1ZRPag2wqHneSlibcXXFn9KutLS0lh1PURQwcWCN0yzzATKAibmpx4g/640?wx_fmt=jpeg)

最后是**错误步骤修正** 。

为了获得D2中每个样本对应的正确推理步骤，需要对模型πref进行推断，使用提示x和前面的正确推理步骤s1~k-1来采样多个输出ycont，此过程可以表示为：

![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtATxwIsb9Sj7SqvUibjrcOhGB5vs80oFC2wJgFXy5icHgMFvDFm9J8Mmm6xlhDM62stNclITufexOyg/640?wx_fmt=jpeg)

随后保留ycont中那些与真实答案一致的输出，并将其中的第一个推理步骤作为swin，最终得到数据集D：

![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtATxwIsb9Sj7SqvUibjrcOhGu1nJtDSfwTKJYtfExX5M4tqycUgMVdZxCr9Dunkaf5nY2tibg37QQFg/640?wx_fmt=jpeg)

下图展示了一个数据样本示例。值得一提的是，该数据准备流程无需大量的人工介入，人类或GPT-4只需要判断给定推理步骤是否正确，而无需亲自撰写答案来修正错误。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtATxwIsb9Sj7SqvUibjrcOhGKwlDCT8Bk3oATATIElgfMmgp7f6gUGzicickYdEOJ0sWpMVJ25R1Ym1Q/640?wx_fmt=png&from=appmsg)

## 10k数据带来数学能力大幅提升

Step-DPO可以在SFT模型或现有的开源Instruct模型上进行微调，仅通过10K数据以及数百个训练步数，即可取得大幅度的数学能力提升。

如下图所示，在Qwen2-7B-Instruct模型的基础上进行Step-DPO可在MATH测试集上**获得5.6%准确率的提升** 。

在Qwen2-72B-Instruct模型的基础上进行Step-
DPO，可在MATH和GSM8K测试集的准确率分别达到70.8%和94.0%，**超过一系列闭源模型**
如Gemini-1.5-Pro、GPT-4-1106，以及Claude-3-Opus。

除此之外，在难度较高的包含数学**竞赛题** 的Odyssey-MATH榜单上也有显著提升。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtATxwIsb9Sj7SqvUibjrcOhGGy8YreicSibOarOy7wMsv99Aoo3X6NB3NfYCrkYsslSIgG5JicM9vKPug/640?wx_fmt=png&from=appmsg)

经过Step-DPO之后，模型更加鲁棒，减少幻觉的产生，在推理过程中也不容易出错。如以下两个例子所示。

> 假设h(x)=f-1(x)，如果h(2)=10，h(10)=1，h(1)=2，求f(f(10))。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtATxwIsb9Sj7SqvUibjrcOhGw5icIibfnjYTfAcNKGjEnhZicHUaCFiaQ35zEWZAa7uu2UDJJcc2Cym5Rw/640?wx_fmt=png&from=appmsg)

> t的平方根大于2且小于3.5，满足这一条件的整数t有多少个？

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtATxwIsb9Sj7SqvUibjrcOhGokAXwb6YKuW6Biae54fe3ADmbfW2EF56IRQ840vJnt6bLZnSCNcHJmw/640?wx_fmt=png&from=appmsg)

即便是下图这道数学竞赛题，经过Step-DPO之后的模型也可以做对。

> 在所有非增函数f:{1,2,…,10}→{1,2,…,10}中，有些函数有固定点，另一些没有，这两种函数的数量相差多少？

![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtATxwIsb9Sj7SqvUibjrcOhGLEURjxEaKb2CkkqxO4FbWdicukGw6aF2Hz1D4pFcgwwcIqCa3sEdGAg/640?wx_fmt=jpeg&from=appmsg)

目前，该项目的代码，数据，模型，Demo均已公开至GitHub和Hugging Face，同时支持在线体验。

论文地址：  
https://arxiv.org/abs/2406.18629  
GitHub：  
https://github.com/dvlab-research/Step-DPO  
在线Demo：  
http://103.170.5.190:7870/  
模型（HF）：  
https://huggingface.co/collections/xinlai/step-dpo-6682e12dfbbb2917c8161df7  
数据（HF）：  
https://huggingface.co/datasets/xinlai/Math-Step-DPO-10K

— **完** —

  

投稿请发邮件到：

**ai@qbitai.com**

标题注明【投稿】，告诉我们：

你是谁，从哪来，投稿内容‍

附上论文/项目主页链接，以及联系方式哦

我们会（尽量）及时回复你

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtC5nGy7YMGhQ0ZJeyibWyL0KVCtiaLEPMyd4Bszuo0bFIOxZOvdmqdxnOosYXyu5aI7MXpyUrUWfz6g/640?wx_fmt=gif&tp=webp&wxfrom=5&wx_lazy=1)

  

**点这里 👇关注我，记得标星哦～**

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

