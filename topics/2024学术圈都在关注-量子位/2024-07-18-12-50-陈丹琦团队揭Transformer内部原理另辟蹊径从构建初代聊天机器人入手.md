# 陈丹琦团队揭Transformer内部原理：另辟蹊径，从构建初代聊天机器人入手

文章作者: 量子位
发布时间: 2024-07-18 12:50
发布地: 北京
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247739578&idx=3&sn=678fb4e523cda9e2de1d5cb1a3b7e4af&chksm=e8df9fc8dfa816de96350e2275be05f315b8a85179af664a817e22eccb9bb133a380aff3295d#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtAJicpw7icQOibpwhPgHO3CLWtricLG9biaicZN8ZmwXJCkta05Bva8cTS52klicDAco3F0YO8984ZfQ5Ftw/300

##### 一水 发自 凹非寺  
量子位 | 公众号 QbitAI

好家伙！为了揭秘Transformer内部工作原理，陈丹琦团队直接**复现** ——

**第一个经典聊天机器人** ELIZA。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAJicpw7icQOibpwhPgHO3CLWt3AtopA8atX09icPibQ94VP2uW1j1JnMmgghFqMSzgG1Usa7XKykGLJ7w/640?wx_fmt=png&from=appmsg)

ELIZA编写于**20世纪60年代** ，主要用于心理治疗，在当时似乎已经能“听懂”人说话。

比如下面这个例子：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAJicpw7icQOibpwhPgHO3CLWt47XBFw7ibVAdWxW4SgXkfLlsEOUPeGZLVzkQMgC3pu7VNt0ScOzObSQ/640?wx_fmt=png&from=appmsg)

可以看出，ELIZA的对话方式“有点狡猾”，像极了看似认真实则敷衍的好闺蜜好兄弟~

由于表现出了早期语言模型行为，且**算法简单** ，团队通过成功“复现”ELIZA揭开了Transformer的神秘面纱。

他们在研究中发现：

  * Transformer模型倾向于使用注意力机制来识别和复制序列中的**特定模式** ，而非严格按照词的位置来复制

  * 即使没有特别为记忆设计的工具，模型也能**通过自己的计算过程** 来实现记忆效果

更多详情接下来一睹为快。

## 复现经典聊天机器人ELIZA

动手前第一步，先来简单了解下ELIZA算法。

ELIZA同时使用**本地模式匹配** 和**两种长期记忆机制** （循环遍历响应和记忆队列）。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAJicpw7icQOibpwhPgHO3CLWtiauFQVLaiciasOKGzkg7c1zxPnKqq48Qch5n95apAF1j34cpNmq2MJdWg/640?wx_fmt=png&from=appmsg)

简单来说，本地模式匹配是指ELIZA有一套**关键词和规则** ，当它看到用户说的话里包含这些关键词时，就能按照规则给出回应。

而且，ELIZA会记住以前是怎么回答类似问题的，然后换着花样给出不同的回答。

甚至它还有自己的小本本（记忆队列），可以把用户说过的重要事情记下来。当用户提到以前的事情时，ELIZA就可以翻翻笔记本，然后根据记得的内容给出回应。

摸清了上述原理，团队通过**4个子任务** 来实现ELIZA算法。

**其核心** 是使用一组模式匹配规则（称为分解模板）和相应的转换规则（称为重组规则）来生成响应。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAJicpw7icQOibpwhPgHO3CLWtFMPXG7lHajUvlrasicic6jlNvNuPSticNq39diaHvKTX9r36NrAjMGzhXA/640?wx_fmt=png&from=appmsg)

第一步，将输入分成多个段落。

这里输入的是**对话历史** ，包括用户的输入（标记为“u：”）和ELIZA的响应（标记为“e：”）。

在多轮对话中，用户输入和ELIZA响应会形成**一个连续的序列**
，然后Transformer模型使用自注意力机制来处理这些输入。它能够通过注意力权重来识别对话中的重要部分，并据此生成响应。

接下来，团队利用无星号正则表达式（Star-Free Regular Expression）来构建ELIZA的**模板匹配机制** 。

左侧为**分解模板** ，告诉我们机器人如何识别用户说的话。比如，如果规则是“你 0 我”，那么“你讨厌我”和“你觉得我怎么样”都会被识别。

右侧为**重组规则** ，告诉机器人如何回应。比如，如果规则是“你 0
我”，那么机器人可能会回应“你为什么认为我讨厌你？”这里的“0”会被替换成用户实际说的话。

模型尝试**将每个用户输入与一个“分解模板”匹配。** 这个过程是并行进行的，意味着模型会同时比较每个用户输入与所有可能的模板，以找到最合适的匹配。

**第三步**
，模型识别出得分最高的模板。在选择转换规则时，模型不仅考虑模板的匹配度，还会考虑这个模板在对话中较早匹配的次数。这可能有助于模型更准确地理解对话的上下文。

最后，在识别出匹配的模板后，模型需要生成一个合适的响应。

这一过程涉及到**两种主要的复制机制**
：基于内容的注意力（感应头）和基于位置的注意力。前者通过识别输入序列中的模式来复制词，而后者则依赖于词在输入中的具体位置。

为了模拟ELIZA的长期记忆功能，团队还引入了循环遍历重组规则和记忆队列机制。

**比如前者**
，一种方法是通过计算模板被匹配的次数，并使用模运算来选择重组规则（模块化前缀和）；另一种方法是通过检查模型之前的输出来决定下一次的回应（中间输出）。

**再比如后者** ，一种实现记忆队列的方法是使用一个自动机，它可以通过增加或减少状态来跟踪队列中的记忆（Gridworld
automaton）；另一种方法是通过分析模型之前的输出来确定何时从记忆队列中检索记忆（中间输出）。

通过以上几个步骤，团队成功复现了ELIZA模型。

## 实验结论

为了测试效果，团队用新模型生成了**合成的ELIZA数据集** ，这些数据集包括多轮对话，每轮对话最多包含512个词。

然后，基于这些合成数据，团队使用GPT-2从头训练了新的Transformer模型。

新模型包含**8层解码器** ，每层有12个注意力头，隐藏维度为768。

通过观察模型在学习过程中的表现，团队进一步分析Transformer模型在处理对话任务时的行为和学习机制。

研究显示，Transformer模型能够快速学会识别**正确的重组规则** ，但需要更长时间来正确实施转换。特别是在多轮对话和内存队列示例中，准确性略低。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAJicpw7icQOibpwhPgHO3CLWtpA5icX8DgJQDaxKRgib0gCxDXHBQ8nZF8g80cdkD6SQVKicZw2iaJUrDicg/640?wx_fmt=png&from=appmsg)

另外，团队进一步分析了模型的错误，发现模型在**精确复制方面** 存在困难，尤其是当需要复制的标记数量较多时。同时，模型在**处理内存队列**
时也遇到了挑战，尤其是当前回合与目标内存之间的距离较远时。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAJicpw7icQOibpwhPgHO3CLWtdlwU8OhibSJgeBqSSic5QZN3XVwUtasBwibABFdLBo5uktjrPIuQkIklw/640?wx_fmt=png&from=appmsg)

最重要的是，研究发现Transformer模型倾向于根据**对话内容的相似性** （Induction
Head）来选择回答，而非严格按照**词出现的位置** 来复制；而且，通过**调整数据属性** 可以影响模型学习的机制。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAJicpw7icQOibpwhPgHO3CLWtQNcE6dRICUpCoCvjjO1ILYzS2ibuCn0bPJ2YuvmLEAibd1VGRjFWCniaw/640?wx_fmt=png&from=appmsg)

事实上，除了上述具体发现，该项目最大贡献是给**研究自动可解释性**
提供了新思路。自动可解释性是指系统能够自动生成解释其决策过程的能力，这对于提高人工智能系统的透明度和可信度非常重要。

在本研究中，团队通过模仿ELIZA这样的经典聊天机器人，采用了一种结构化和系统化的方法来分析模型的行为。

这种方法包括生成特定的数据集、设计特定的模型架构和训练策略。

这一切实现了：

> 为大语言模型研究提供一个**受控的理想化环境** 。

目前相关研究已公开，具体可进一步查阅论文。

论文：  
https://arxiv.org/abs/2407.10949  
GitHub：  
https://github.com/princeton-nlp/ELIZA-Transformer  
参考链接：  
https://x.com/danfriedman0/status/1813168885631263126  

— **完** —

**量子位年度AI主题策划** 正在征集中！

欢迎投稿专题 **一千零一个AI应****用** ，**365行AI落地方案**

或与我们分享你在**寻找的AI产品** ，或发现的**AI新动向**

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDpTavEwUl8aOlFLGHaPnaKXJcMUeJtGXVLliac6P6XxYHIKhnz0NPUgVvlrXAvJC33ibh8aYDdyudA/640?wx_fmt=png&from=appmsg)

  

**点这里 👇关注我，记得标星哦～**

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg)

