# 提示词用上“过去式“，秒破GPT4o等六大模型安全限制！中文语境也好使

文章作者: 量子位
发布时间: 2024-07-19 13:06
发布地: 北京
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247739705&idx=2&sn=a96618dcae33470608400007f535369a&chksm=e8df9e4bdfa8175d273120c2676bd7e2ed7878bab45dd05dfa689112f14cbdc5cd2f5d37fde9#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtBUg5QBzVKyuqmSzKDYlG0sicI4ZS9qqfxGe8G2pDWcWFYm67YEqfsIN4nILUs68vVwibaCT3onalDQ/300

##### 克雷西 发自 凹非寺  
量子位 | 公众号 QbitAI

只要在提示词中**把时间设定成过去** ，就能轻松突破大模型的安全防线。

而且对GPT-4o尤其有效，原本只有1%的攻击成功率直接飙到88%，几乎是“有求必应”。

有网友看了后直言，这简直是有史以来最简单的大模型越狱方式。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBUg5QBzVKyuqmSzKDYlG0sYUECt2cZTcDpKXsrcr248OvlBviae5TWK9gEkEZzq2dOYJbHfsW1XjA/640?wx_fmt=png&from=appmsg)

来自洛桑联邦理工学院的一篇最新论文，揭开了这个大模型安全措施的新漏洞。

而且攻击方式简单到离谱，不用像“奶奶漏洞”那样专门构建特殊情境，更不必说专业对抗性攻击里那些意义不明的特殊符号了。

只要把请求中的时间改成过去，就能让GPT-4o把燃烧弹和毒品的配方和盘托出。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBUg5QBzVKyuqmSzKDYlG0sJKEtfNQx9ajWKUmr9zS7WEHqPqiae8n2HGW2NTicbzTVxuMQ0tgpOXnw/640?wx_fmt=png&from=appmsg)

而且量子位实测发现，把提示词改成中文，对GPT-4o也一样有效。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBUg5QBzVKyuqmSzKDYlG0sCH2mDFq6bAT2TcgtQaiaOJ9ibaWO8p5ffxDeXtbPzs5aktDbcZhycwrg/640?wx_fmt=png&from=appmsg)

有网友表示，实在是想不到突破大模型漏洞的方式竟然如此简单……

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBUg5QBzVKyuqmSzKDYlG0sJ4Hwndgglzp9RM0Dp1swiaH5rw63uPaCgwURyWaGw6afbibeTKTzKKyw/640?wx_fmt=png&from=appmsg)

当然这样的结果也说明，现有的大模型安全措施还是太脆弱了。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBUg5QBzVKyuqmSzKDYlG0sHw8LFnmejW7TOiaJYKflGVV1P6chELVxKeeLwDNz728sGVbrFUsE4iaw/640?wx_fmt=png&from=appmsg)

## GPT-4o最易“破防”

实验过程中，作者从JBB-Behaviors大模型越狱数据集中选择了100个有害行为，涉及了OpenAI策略中的10个危害类别。

然后作者用GPT-3.5 Turbo，把把这些有害请求对应的时间改写成过去。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBUg5QBzVKyuqmSzKDYlG0sza8nTuJUQZ4ucibmawQficwFLGfhn1Gzu1qdNZuUIZ5MwicJht9LlG5iag/640?wx_fmt=png&from=appmsg)

接着就是用这些修改后的请求去测试大模型，然后分别用GPT-4、Llama-3和基于规则的启发式判断器这三种不同方式来判断越狱是否成功。

被测试的模型则包括Llama-3、GPT-3.5 Turbo、谷歌的Gemma-2、微软的Phi-3、GPT-4o和R2D2（一种对抗性训练方法）这六种。

结果显示，**GPT-4o的越狱成功率提升最为明显**
，在使用GPT-4和Llama-3进行判断时，原始成功率均只有1%，使用这种攻击的成功率则上升到了88%和65%，启发式判断器给出的成功率也从13%升到了73%。

其他模型的攻击成功率也提高不少，尤其是在使用GPT-4判断时，除了Llama-3，其余模型的成功率增长值都超过了70个百分点，其他的判断方法给出的数值相对较小，不过都呈现出了增长趋势。

对于**Llama-3的攻击效果则相对稍弱** 一些，但成功率也是增加了。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBUg5QBzVKyuqmSzKDYlG0spVKWIvzkIhFSsyicSZ71NUe0ZfP6qLyqhmYuHoQKkq94XDoPBBMjv8A/640?wx_fmt=png&from=appmsg)

另外**随着攻击次数的增加，成功率也是越来越高** ，特别是GPT-4o，在第一次攻击时就有超过一半的成功率。

不过当攻击次数达到10次后，对各模型的攻击成功率增长都开始放缓，然后逐渐趋于平稳。

值得一提的是，Llama-3在经历了20次攻击之后，成功率依然不到30%，相比其他模型体现出了很强的鲁棒性。

同时从图中也不能看出，不同判断方法给出的具体成功率值虽有一定差距，但整体趋势比较一致。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBUg5QBzVKyuqmSzKDYlG0sJA3WSSERpgY5ahibl2p2e0sCO2H0aolMJGtV8mrJVrI5bUnKmCp2XzA/640?wx_fmt=png&from=appmsg)

另外，针对10类不同的危害行为，作者也发现了其间存在攻击成功率的差别。

不看Llama-3这个“清流”的话，恶意软件/黑客、经济危害等类型的攻击成功率相对较高，错误信息、色情内容等则较难进行攻击。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBUg5QBzVKyuqmSzKDYlG0suuicXztvzSqQYyrQEmITp7cCer3EbmfqYm1nC20N6s7xQY214x3cNnw/640?wx_fmt=png&from=appmsg)

当请求包含一些与特定事件或实体直接相关的关键词时，攻击成功率会更低；而请求偏向于通识内容时更容易成功。

基于这些发现，作者又产生了一个新的疑问——既然改成过去有用，那么改写成将来是不是也有用呢？

进一步实验表明，确实也有一定用处，不过相比于过去来说，将来时间的效果就没有那么明显了。

以GPT-4o为例，换成过去后接近90个百分点的增长，再换成将来就只有60了。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBUg5QBzVKyuqmSzKDYlG0sB8KbMl5zg4W9a1iapwIxDgI1FzSMSkzXNc9uR5ib2ONllfsyuugiaDgQg/640?wx_fmt=png&from=appmsg)

对于这样的结果，网友们除了有些惊讶之外，还有人指出为什么不测试Claude。

作者回应称，不是不想测，而是免费API用完了，下一个版本会加上。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBUg5QBzVKyuqmSzKDYlG0s5nKSU1yAKiavaurKBJEQiaY36btH023wJwTdjWXp4RLMT5YheuyKnXNA/640?wx_fmt=png&from=appmsg)

不过有网友自己动手试了试，发现这种攻击并没有奏效，即使后面追问说是出于学术目的，模型依然是拒绝回答。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBUg5QBzVKyuqmSzKDYlG0seR7n2Z78RGQTyl60jJ23tIEKamlwVdrDsuYVxo22ddb6qY1scRicgibg/640?wx_fmt=png&from=appmsg)

###### **△** 来源：Twitter/Muratcan Koylan

这篇论文的作者也承认，Claude相比于其他模型会更难攻击，但他认为用复杂些的提示词也能实现。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBUg5QBzVKyuqmSzKDYlG0sy72C6HWicJiaH0ZDcpybFAgP6hEsiaL2rrxYdD2x1nRWdYtjXNjbHHBxg/640?wx_fmt=png&from=appmsg)

因为Claude在拒绝回答时非常喜欢用“I apologize”开头，所以作者要求模型不要用“I”来开头。

不过量子位测试发现，这个方法也未能奏效，无论是Claude 3 Opus还是3.5 Sonnet，都依然拒绝回答这个问题。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBUg5QBzVKyuqmSzKDYlG0sIoDcPiaUVwMl2osM6OuuBvO4sXVANZX2Kol5GLc6l9AEl5B7fkKq8Mg/640?wx_fmt=png&from=appmsg)

###### **△** 左：3 Opus，右：3.5 Sonnet

还有人表示，自己对Claude 3 Haiku进行了一下测试（样本量未说明），结果成功率为0。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBUg5QBzVKyuqmSzKDYlG0s1QKzpLPjbbFvfvnJ5emu3lhRJIv8iblesic8Z5BGbHQXBNFptI7vuubA/640?wx_fmt=png&from=appmsg)

总的来说，作者表示，虽然这样的越狱方式比不上对抗性提示等复杂方法，但明显更简单有效，可作为探测语言模型泛化能力的工具。

## 使用拒绝数据微调或可防御

作者表示，这些发现揭示了SFT、RLHF和对抗训练等当前广泛使用的语言模型对齐技术，仍然存在一定的局限性。

按照论文的观点，这可能意味着模型从训练数据中学到的拒绝能力，过于依赖于特定的语法和词汇模式，而没有真正理解请求的内在语义和意图。

这些发现对于当前的语言模型对齐技术提出了新的挑战和思考方向——仅仅依靠在训练数据中加入更多的拒绝例子，可能无法从根本上解决模型的安全问题。

作者又进行了进一步实验，使用拒绝过去时间攻击的示例对GPT-3.5进行了微调。

结果发现，只要拒绝示例在微调数据中的占比达到5%，攻击的成功率增长就变成了0。

下表中，A%/B%表示微调数据集中有A%的拒绝示例和B%的正常对话，正常对话数据来自OpenHermes-2.5。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBUg5QBzVKyuqmSzKDYlG0svBQrQ0NcODGEgYasA8BnNAy2L1tjNVEjYTe9U8fncYk4eTJYvRQcrw/640?wx_fmt=png&from=appmsg)

这样的结果也说明，如果能够对潜在的攻击进行准确预判，并使用拒绝示例让模型对齐，就能有效对攻击做出防御，也就意味着在评估语言模型的安全性和对齐质量时，需要设计更全面、更细致的方案。

论文地址：  
https://arxiv.org/abs/2407.11969  
参考链接：  
[1]https://x.com/maksym_andr/status/1813608842699079750  
[2]https://x.com/MatthewBerman/status/1813719273338290328

— **完** —

**量子位年度AI主题策划** 正在征集中！

欢迎投稿专题 **一千零一个AI应****用** ，**365行AI落地方案**

或与我们分享你在**寻找的AI产品** ，或发现的**AI新动向**

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDpTavEwUl8aOlFLGHaPnaKXJcMUeJtGXVLliac6P6XxYHIKhnz0NPUgVvlrXAvJC33ibh8aYDdyudA/640?wx_fmt=png&from=appmsg)

  

**点这里 👇关注我，记得标星哦～**

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg)

