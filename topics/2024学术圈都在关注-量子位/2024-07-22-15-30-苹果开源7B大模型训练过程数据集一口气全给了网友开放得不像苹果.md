# 苹果开源7B大模型，训练过程数据集一口气全给了，网友：开放得不像苹果

文章作者: 量子位
发布时间: 2024-07-22 15:30
发布地: 未知地区
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247739937&idx=2&sn=44c7290cf64203012f6ead9551f7fbdb&chksm=e8df9d53dfa8144544f2d9ccefbeff848ca1c220f10dd089c7839a5b8dd52b44ef9bdd736fcc#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtCC9fA3gicM8wc0mTXzumJYrC2dMh7cibQ6ElibXHnj6zez7a7aVN9PPXxibmFkgb41vX04hk9icpzYRyg/300

##### 一水 发自 凹非寺  
量子位 | 公众号 QbitAI

苹果最新杀入开源大模型战场，而且比其他公司更开放。

推出**7B模型** ，不仅效果与**Llama 3 8B** 相当，而且一次性开源了**全部训练过程和资源** 。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCC9fA3gicM8wc0mTXzumJYrlZRsQ3y1FYC2pEDP7icJv736qJhHiaBvNTibbCvSH9Sibiakdia1gI2Iz9ZQ/640?wx_fmt=png&from=appmsg)

要知道，不久前Nature杂志编辑Elizabeth Gibney还**撰文批评** ：

> 许多声称开源的AI模型，实际上在数据和训练方法上并不透明，无法满足真正的科学研究需求。

而苹果这次竟然来真的！！

就连NLP科学家、AutoAWQ创建者也发出惊叹：

> Apple发布了一个击败Mistral 7B的模型，但更棒的是他们完全开源了所有内容，**包括预训练数据集** ！

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCC9fA3gicM8wc0mTXzumJYrkIPX1JTTKOD4SQCVaiaMiaJQF5E65hib1hBu4TvW7IVAIK2dDlJ3LAkrQ/640?wx_fmt=png&from=appmsg)

也引来网友在线调侃：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCC9fA3gicM8wc0mTXzumJYrQWk1PWNktdXIelJnF99sQCqgLHWtssAN3FJ8VjJqWEUvK7xtnKWxzA/640?wx_fmt=png&from=appmsg)

至于这次开源的意义，有热心网友也帮忙总结了：

> 对于任何想要从头开始训练模型或微调现有模型的人来说，**数据管理过程** 是必须研究的。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCC9fA3gicM8wc0mTXzumJYrME17t8kib0MAaOqKA1ydcaQ9jibmA607Dt3D9py6rpv1j2KMXhNOoEVQ/640?wx_fmt=png&from=appmsg)

当然，除了OpenAI和苹果，上周Mistral AI联合英伟达也发布了一个12B参数小模型。

HuggingFace创始人表示，**「小模型周」** 来了！

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCC9fA3gicM8wc0mTXzumJYr9QvmmkwIjCFicruiaproJ7N5C8BAjzp82WUIVyaKYRvfLt5RvaecycCA/640?wx_fmt=png&from=appmsg)

卷！继续卷！所以苹果这次发布的小模型究竟有多能打？

## 效果直逼Llama 3 8B

有多能打先不说，先来看Hugging Face技术主管刚“拆箱”的**模型基础配置** 。

总结下来就是：

  * 7B基础模型，在开放数据集上使用**2.5T tokens** 进行训练

  * 主要是英文数据，拥有**2048** tokens上下文窗口

  * 数据集包括DCLM-BASELINE、StarCoder和ProofPile2

  * MMLU得分接近Llama 3 8B

  * 使用PyTorch和OpenLM框架进行训练

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCC9fA3gicM8wc0mTXzumJYrE5ibVhtHVic0EMRsGH13Dibtxv16JFUldDpNWg7nJXFtpdhQu1H4icAG6A/640?wx_fmt=png&from=appmsg)

具体而言，研究团队先是提出了一个语言模型**数据比较新基准** ——DCLM。

之所以提出这一基准，是因为团队发现：

> 由机器学习 (ML) 模型从较大的数据集中**自动过滤和选择高质量数据** ，可能是构建高质量训练集的关键。

因此，团队使用DCLM来设计高质量数据集从而提高模型性能，尤其是在多模态领域。

其**思路** 很简单：使用一个标准化的框架来进行实验，包括固定的模型架构、训练代码、超参数和评估，最终找出哪种数据整理策略最适合训练出高性能的模型。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCC9fA3gicM8wc0mTXzumJYrj7x11hnfKq9sxfbmS2M3NoddCrjX2FsVV1UcO7bH6IvdMKqiawK4QCw/640?wx_fmt=png&from=appmsg)

基于上述思路，团队构建了一个**高质量数据集DCLM-BASELINE** ，并用它从头训练了一个7B参数模型——DCLM-7B。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCC9fA3gicM8wc0mTXzumJYrDiaoERbl3ibXbFib2wpz8RkA5Rp49DJSJxeiaqqV1ibHQROzD6p4LXf1MEA/640?wx_fmt=png&from=appmsg)

DCLM-7B具体表现如何呢？

结果显示，它在MMLU基准上5-shot**准确率达64%** ，可与Mistral-7B-v0.3（63%）和Llama 3
8B（66%）相媲美；并且在53个自然语言理解任务上的平均表现也可与Llama 3 8B相媲美，而所需计算量仅为后者的1/6。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCC9fA3gicM8wc0mTXzumJYreNiaicTIECupibiaWxtDYoIRqyAdOU2m1QLibW0e5FWyqrvRKlLjDic4qkBQ/640?wx_fmt=png&from=appmsg)

与其他同等大小模型相比，DCLM-7B的MMLU得分超越Mistral-7B，接近Llama 3 8B。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCC9fA3gicM8wc0mTXzumJYriaoLApbKU6W9ETHD7VBy8YouojqPicGKDFTwiam5eFOiaTafR8v3jRPRHg/640?wx_fmt=png&from=appmsg)

最后，为了**测试新数据集效果** ，有业内人士用卡帕西的llm.c训练了GPT-2 1.5B，来比较DCLM-Baseline与FineWeb-
Edu这两个数据集。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCC9fA3gicM8wc0mTXzumJYrGXRw4jDFW4hicMiaeWrSHmThGwp3Kmxnan7TP9oXuT8ibDxsqmiaubE0dw/640?wx_fmt=png&from=appmsg)

结果显示DCLM-Baseline取得了**更高的平均分** ，且在ARC（小学生科学问题推理）、HellaSwag（常识推理）、MMLU等任务上表现更好。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCC9fA3gicM8wc0mTXzumJYriac1QsnDPY63vaFROfsE1hvBKsSbT5EjpYtBSAJicTibK7iahib36ucZfDQ/640?wx_fmt=png&from=appmsg)

## “小”模型成新趋势

回到开头，“小”模型最近已成新趋势。

先是HuggingFace推出了小模型家族**“SmolLM”** ，其中包含135M、360M和1.7B型号模型。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCC9fA3gicM8wc0mTXzumJYr9ShWsQ0YmGkmE8mR8LDBKp2LXRsqIaicg0ZCVZyGLelbfT9nF77VpoQ/640?wx_fmt=png&from=appmsg)

它们在广泛的推理和常识基准上优于类似大小的模型。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCC9fA3gicM8wc0mTXzumJYrIsTMEib8SRFs0GMI029GbvPhAjohmlYAXApzdvS2zB35tjOjmkzKXcg/640?wx_fmt=png&from=appmsg)

然后OpenAI突然发布了**GPT-4o mini** ，不仅能力接近GPT-4，而且价格大幅下降。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCC9fA3gicM8wc0mTXzumJYryTEfZeOdHuKEy8fBlzKDUILvgfvLhgMzce4Tjs2J6P6AFU2ZGJCINQ/640?wx_fmt=png&from=appmsg)

就在GPT-4o mini**发布同日** ，Mistral AI联合英伟达发布了12B参数小模型——**Mistral NeMo** 。

从整体性能上看，Mistral NeMo在多项基准测试中，击败了Gemma 2 9B和Llama 3 8B。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCC9fA3gicM8wc0mTXzumJYrE8s7r2HWoEzMgzcrKJzKBkWatib7MGR3fOBZQ8veyt2kpx8NAvlJfHw/640?wx_fmt=png&from=appmsg)

所以，为啥大家都开始卷小模型了？

原因嘛可能正如smol AI创始人提醒的，虽然模型变小了，但在能力相近的情况下，小模型**大大降低了成本** 。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCC9fA3gicM8wc0mTXzumJYrib9Uyelhzy4ImZNQNA3mFzFkkpYHnzU15uXictGQQxq2F6wG5lMnXXzA/640?wx_fmt=png&from=appmsg)

就像他提供的这张图，以GPT-4o mini为代表的小模型整体比右侧价格更低。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCC9fA3gicM8wc0mTXzumJYry3MbG4dMKAh7uicsJllPRVbDlPKjY43IUHr4KpNcX2IVmRJYZvibKW4g/640?wx_fmt=png&from=appmsg)

对此，我等吃瓜群众be like:

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtCC9fA3gicM8wc0mTXzumJYrSER2JYP8pyuF1M8W5Fd3oZ0HOwwBUicExgEDIq9xqLUJM7Wh45ZI9DQ/640?wx_fmt=gif&from=appmsg)

所以，你更看好哪家呢？（欢迎评论区讨论留言）

模型地址：  
https://huggingface.co/apple/DCLM-7B  
GitHub:  
https://github.com/mlfoundations/dclm  
数据集地址：  
https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0

参考链接：  
[1]https://x.com/Yuchenj_UW/status/1813260100192334108  
[2]https://x.com/casper_hansen_/status/1814269340100751382  
[3]https://x.com/_philschmid/status/1814274909775995087  
[4]https://x.com/LoubnaBenAllal1/status/1813252390692303069

— **完** —

**量子位年度AI主题策划** 正在征集中！

欢迎投稿专题 **一千零一个AI应****用** ，**365行AI落地方案**

或与我们分享你在**寻找的AI产品** ，或发现的**AI新动向**

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDpTavEwUl8aOlFLGHaPnaKXJcMUeJtGXVLliac6P6XxYHIKhnz0NPUgVvlrXAvJC33ibh8aYDdyudA/640?wx_fmt=png&from=appmsg)

  

**点这里 👇关注我，记得标星哦～**

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg)

