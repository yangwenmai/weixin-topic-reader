# Meta「分割一切」进化2.0！一键跟踪运动物体，代码权重数据集全开源，网友：真正的OpenAI

文章作者: 量子位
发布时间: 2024-07-30 14:37
发布地: 未知地区
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247741474&idx=2&sn=78aab794f8951a5e587c66cd14ae90b5&chksm=e8df8750dfa80e46f8d8778a02955068671c58c51442befd834db98782f5d1bf2aa683d61141#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtAVxtpdrlOzTYDqkeBWUeV8myeAAcJfJx1ia4ibS1TYOelqSjPI0dnzsichV5HKhy8nqah6ib0B1xjMfg/300

##### 明敏 发自 凹非寺  
量子位 | 公众号 QbitAI

又是发布即开源！

Meta“分割一切AI”二代**SAM2** 在SIGGRAPH上刚刚亮相。

相较于上一代，它的能力从图像分割拓展到**视频分割** 。

可实时处理任意长视频，视频中没见过的对象也能轻松分割追踪。

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtAVxtpdrlOzTYDqkeBWUeV8AEGTlVA9PqS0bNtyhr3z6ichyGcQ686O3DjBBwztZkPu5yJsMVuNN1Q/640?wx_fmt=gif&from=appmsg)

更关键的是，模型**代码、权重以及数据集** 通通开源！

它和Llama系列一样遵循Apache 2.0许可协议，并根据BSD-3许可分享评估代码。

网友yygq：我就问OpenAI尴尬不尴尬。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAVxtpdrlOzTYDqkeBWUeV8ib8LAq4EpJVOOwSibu7rlM72jFvibQxMWoaIKib1b3iavHY1iavtF4tcBGibg/640?wx_fmt=png&from=appmsg)

Meta表示，此次开源的数据集包含51000个真实世界视频和600000个时空掩码（masklets，spatio-temporal
masks)，规模远超此前最大同类数据集。

可在线试玩的demo也同步上线，大家都能来体验。

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtAVxtpdrlOzTYDqkeBWUeV8HEfOSbyV7A4ibojXhFeiaOIxhjJnjhnZNvhuiatvypH4wKbNsXlpsewiaw/640?wx_fmt=gif&from=appmsg)

## 在SAM之上加入记忆模块

相较于SAM一代，SAM2的能力升级主要有：

  * 支持任意长视频实时分割

  * 实现zero-shot泛化

  * 分割和追踪准确性提升

  * 解决遮挡问题

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtAVxtpdrlOzTYDqkeBWUeV8JjsatevCLOLzN6JF0WKEibwUPs4ZJLJVk4uacPibWeC6IGqb0KT3RQtA/640?wx_fmt=gif&from=appmsg)

它进行交互式分割的过程主要分为两步：选择和细化。

在第一帧中，用户通过点击来选择目标对象，SAM2根据点击自动将分割传播到后续帧，形成时空掩码。

如果SAM2在某些帧中丢失了目标对象，用户可以通过在新一帧中提供额外的提示来进行校正。

如果在第三帧中需要需要恢复对象，只需在该帧中点击即可。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAVxtpdrlOzTYDqkeBWUeV86icJ1ChJTIvKgsssQ1j8b2bBLIqrOTia6Pk93VXdXfVbVYthrDRyZIOw/640?wx_fmt=png&from=appmsg)

SAM2的核心思路是**将图像视作单帧视频** ，因此可以从SAM直接扩展至视频领域，同时支持图像和视频输入。

处理视频唯一的区别在于，模型需要依赖内存来回忆处理过的信息，以便在当前时间步长上准确分割对象。

与图像分割相比，视频分割中，物体的运动、变形、遮挡和光线等都会发生强烈变化。同时分割视频中的对象需要了解实体跨越空间和时间的位置。

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtAVxtpdrlOzTYDqkeBWUeV89ssh3R9YicwZoYE6aeNYfb9iadJuNy8hEqvsrYHsdYWunfS1AhvX9S4w/640?wx_fmt=gif&from=appmsg)

所以Meta主要做了三部分工作：

  * 设计一个可提示的视觉分割任务

  * 在SAM基础上设计新模型

  * 构建SA-V数据集

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAVxtpdrlOzTYDqkeBWUeV8QPUTYHgcK7WOVhE6ecLRXt54nKY5FEkfqx3chRPg8WKQFcI49t1JsA/640?wx_fmt=png&from=appmsg)

首先，团队设计了一个视觉分割任务，将图像分割任务推广到视频领域。

SAM被训练成以图像中的输入点、框或掩码来定义目标并预测分割掩码(segmentation mask)。

然后训练SAM在视频的任意帧中接受prompt来定义要预测的时空掩码(masklet)。

SAM2根据输入提示对当前帧上的掩码进行即时预测，并进行临时传播，在所有帧上都可生成目标对象的掩码。

一旦预测到初始掩码，就可以通过任何帧中向SAM2提供额外提示来进行迭代改进，它可以根据需要重复多次，直到获取到所有掩码。

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtAVxtpdrlOzTYDqkeBWUeV8ALoNuUhrwYbHgvRDplHl8hW8mT2SNicNnXlEal8ibmAiaibvr9G8gU4MVw/640?wx_fmt=gif&from=appmsg)

通过引入**流式记忆** （streaming memory），模型可以实时处理视频，还能更加准确分割和跟踪目标对象。

它由记忆编码器、记忆库和记忆注意力模块组成。让模型一次只处理一帧图像，利用先前帧信息辅助当前帧的分割任务。

分割图像时，内存组件为空，模型和SAM类似。分割视频时，记忆组件能够存储对象信息以及先前的交互信息，从而使得SAM2可以在整个视频中进行掩码预测。

如果在其他帧上有了额外提示，SAM2可以根据目标对象的存储记忆进行纠错。

记忆编码器根据当前预测创建记忆，记忆库保留有关视频目标对象过去预测的信息。记忆注意力机制通过条件化当前帧特征，并根据过去帧的特征调整以产生嵌入，然后将其传递到掩码解码器以生成该帧的掩码预测，后续帧不断重复此操作。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAVxtpdrlOzTYDqkeBWUeV8f5YQMicInCV6iaP8vhGjMficffAcbeHSENjdA3ds5JQnne4SltQ0s9O9w/640?wx_fmt=png&from=appmsg)

这种设计也允许模型可以处理任意时长的视频，不仅对于SA-V数据集的注释收集很重要，也对于机器人等领域应有有影响。

如果被分割对象比较模糊，SAM2还会输出多个有效掩码。比如用户点击了自行车的轮胎，模型可以将此理解为多种掩码，可能是指轮胎、可能是指自行车全部，并输出多个预测。

在视频中，如果在一帧图像中仅有轮胎可见，那么可能需要分割的是轮胎；如果视频后续帧中很多都出现了自行车，那么可能需要分割的是自行车。

如果还是不能判断用户到底想分割哪个部分，模型会按照置信度进行选择。

此外，视频中还容易出现分割对象被遮挡的情况。为了解决这个新情况，SAM2还增加了一个额外的模型输出“遮挡头”（occlusion
head），用来预测对象是否出现在当前帧上。

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtAVxtpdrlOzTYDqkeBWUeV8BAmgL95Y3aZdFFyrIIBsPzCI7xcuv0VEcxPMIicYeu5SHTZNIFDxib5A/640?wx_fmt=gif&from=appmsg)

此外，在数据集方面。

SA-V中包含的视频数量是现有最大同类数据集的4.5倍，注释量则是53倍。

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtAVxtpdrlOzTYDqkeBWUeV8fcf9suPoCtWg6ibuL0ogubcI0uxmdnuD0H6DO1BBaOQvrxu9478DfxQ/640?wx_fmt=gif&from=appmsg)

为了收集到如此多的数据，研究团队构建了一个数据引擎。人工会利用SAM2在视频中注释时空掩码，然后将新的注释用来更新SAM2。多次重复这一循环，就能不断迭代数据集和模型。

和SAM相似，研究团队不对注释的时空掩码进行语义约束，而是更加关注完整的物体。

这一方法让收集视频对象分割掩码速度也大幅提升，比SAM快8.4倍。

## 解决过度分割、超越SOTA

对比来看，使用SAM2可以很好解决过度分割的问题。

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtAVxtpdrlOzTYDqkeBWUeV8ib1daAkZ1WhJUlfPA2ANLr8cypWhOplOhVRGZRGduib2XwG6LbEPDSPw/640?wx_fmt=gif&from=appmsg)

实验数据显示，和半监督SOTA方法比较，SAM2各项性能都表现不错。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAVxtpdrlOzTYDqkeBWUeV8M0k4LjnnZiabibo7V2qyhyES8GibF4V3tolWB0riboqRBx5mzLHyxjCayw/640?wx_fmt=png&from=appmsg)

不过研究团队也表示，SAM2还有不足，

比如可能会跟丢对象。如果相机视角变化大、在比较拥挤的场景里，就容易出现这类情况。所以他们设计了实时交互的模式，支持手动修正。

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtAVxtpdrlOzTYDqkeBWUeV8DRg6aqyvrpz6RUzN6Y5ia3icXfLic7ad1LtfnynBretykB1rqGYOxl2vw/640?wx_fmt=gif&from=appmsg)

以及目标对象移动过快，可能会细节上有缺失。

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtAVxtpdrlOzTYDqkeBWUeV8rc3MOCLrHV3NowibntBzF2G1H6wQ9Cpq8oXnPqhzaKdE3TXQexdLuGQ/640?wx_fmt=gif&from=appmsg)

最后，模型不仅开源支持免费使用，并已在Amazon SageMaker 等平台上托管。

值得一提的是，有人发现论文中提到SAM2训练是在256块A100上耗时108小时完成，对比SAM1则花了68小时。

> 从图像分割扩展到视频领域，成本如此低？

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAVxtpdrlOzTYDqkeBWUeV83xJNO4ef12zE60O4rvGLNG2yGG5k7yibaa08icAfliaxJW9FicOYrNQ4mw/640?wx_fmt=png&from=appmsg)

参考链接：  
[1]https://ai.meta.com/blog/segment-anything-2/  
[2]https://x.com/swyx/status/1818074658299855262

— **完** —

**量子位年度AI主题策划** 正在征集中！

欢迎投稿专题 **一千零一个AI应****用** ，**365行AI落地方案**

或与我们分享你在**寻找的AI产品** ，或发现的**AI新动向**

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDpTavEwUl8aOlFLGHaPnaKXJcMUeJtGXVLliac6P6XxYHIKhnz0NPUgVvlrXAvJC33ibh8aYDdyudA/640?wx_fmt=png&from=appmsg)

  

**点这里 👇关注我，记得标星哦～**

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg)

