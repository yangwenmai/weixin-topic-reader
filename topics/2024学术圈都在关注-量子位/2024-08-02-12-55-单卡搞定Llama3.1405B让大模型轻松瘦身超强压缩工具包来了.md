# 单卡搞定Llama 3.1 405B，让大模型轻松瘦身！超强压缩工具包来了

文章作者: 量子位
发布时间: 2024-08-02 12:55
发布地: 北京
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247742288&idx=6&sn=8b1492da94f2c6c5fbf18916fa60fde3&chksm=e8df8422dfa80d3405a71aebb52e0081ee0f91f565da18b1cb3bfe4f33cc5eafc2d4b8bab9fb#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtDIIVYDBc9AglPlgJNd7ov1dYYlneicfZvpul4ickHmlHTia20qtibz0ycnuPNcVqQF4hxhTJhn1icP8vw/300

##### 模型工具链团队 投稿  
量子位 | 公众号 QbitAI

单卡搞定Llama 3.1（405B），最新大模型压缩工具来了！

最近Llama-3.1登上开源顶峰，但其最强的405B版本模型900多GB的内存需求，对资源构成了****更加苛刻的挑战。

北航、商汤、南洋理工等团队联合推出的大模型压缩工具与基准**LLMC** ，能很好解决这一问题。

**它使得一张80G A100即可完成Llama 3.1 405B的校准和评估，从而实现以超低成本进行量化。**

它支持多种压缩算法、模型和推理后端，具有强大的扩展性和全方位的评估能力。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDIIVYDBc9AglPlgJNd7ov1x15ibPiblpNMn1v9CibfRCFxoeYfpkTO3UeWgzbyc2ZA72icm6wMSolAkg/640?wx_fmt=png&from=appmsg)

目前，研究团队已将使用方法放在GitHub主页上，戳文末链接即可获取。

## Llama3.1 更大也更难压缩

低比特量化是解决资源受限问题的通用技术之一。为此，相关研究人员运用了LLMC对Llama 3.1进行了量化压缩。

结果如表1所示，采用LLMC中的某些算法，例如QuaRot和AWQ等算法在70B和405B参数量的模型上，均能有效保持量化精度。而最为简单的“四舍五入”（Naive）算法在这些大规模模型上则表现出显著的精度下降，特别是在激活被量化的情况下。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDIIVYDBc9AglPlgJNd7ov1wu8NvFL0YyK7ugeErGu4SfibR80DRCklJyg1t0WurxtpHSO0BdLOreQ/640?wx_fmt=png&from=appmsg)

该研究团队发现，Llama 3.1系列模型的量化精度下降现象，源于其激活张量中存在一些相比其他模型更显著的离群值或异常值（outliers）。随着Llama
3.1模型体积的增大，这些离群值的现象更加严重。离群值是指在数据中某些数值与其他数值相比差异较大的点，是影响量化精度的关键因素之一。

借助LLMC工具，研究团队对Llama
3.1系列模型（8B、70B、405B）的第一个block的4层（q_proj、o_proj、gate_proj、down_proj）输入激活张量进行了可视化（如图1-3所示）。每个子图底部展示了该层激活值的所有token的Kurtosis值的平均值和标准差。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDIIVYDBc9AglPlgJNd7ov1a7AbGEZSqFrZicOnyeeOIicrKLhTxjf4O9x0156kHrv09iaFicclrG4f9w/640?wx_fmt=png&from=appmsg)  
![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDIIVYDBc9AglPlgJNd7ov1oYhB6vUdHFDZyFnh4CkiarAY4K7EGo18twiaUHSKf6Zur9xO8sJoc8Og/640?wx_fmt=png&from=appmsg)  
![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDIIVYDBc9AglPlgJNd7ov1ibVEDX2y8f019bOdLAickoB8QDquibvl5gALAbJ8EvScvXNupLktDLQxQ/640?wx_fmt=png&from=appmsg)

由图1-3可以发现，在Llama 3.1系列的模型中，激活张量的一些channel中存在outlier，而且在越大的模型中，这个现象更明显。

因此，可以合理推断：**Llama 3.1 405B模型虽然变强了，但也变得更加“异常”，更难被量化** 。

LLMC工具中支持一系列关于抑制大模型异常值的量化算法，包括AWQ、SmoothQuant、OS+、QuaRot等。由表1可以看到，这些方法通过有效抑制outlier，大大提升了Llama
3.1的量化精度。例如，在405B模型W8A8量化上，SmoothQuant、OS+、QuaRot几乎可以取得与浮点模型相当的精度表现。

## LLMC：一站式大模型瘦身工具包

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDIIVYDBc9AglPlgJNd7ov12tqeYkfCNf51XDXjfpicDBAcMWwYOU5M8K5LPk2C2YsScIvlRXYrHzQ/640?wx_fmt=png&from=appmsg)

###### **△** LLMC框架图

**支持多种算法** 。LLMC 支持多种压缩算法，包括 16
种不同的量化方法，涵盖仅权重、权重激活和混合精度量化。这种多样性允许对不同方法进行公平比较和深入分析。当然除了量化，目前还支持各种类型的稀疏以及相关算法。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDIIVYDBc9AglPlgJNd7ov1k80ZRMWDxJJkI9BmYKLbdAOQYOgyGvG8CMN3CibfFqXFhUCnicyibIAXg/640?wx_fmt=png&from=appmsg)

###### **△** LLMC目前支持的部分硬件友好压缩算法分类

**精度高度对齐** 。LLMC团队进行了若干对齐实验，比较了几种已建立的量化算法（LLMC与原始论文/代码）。

实验设置与原始论文中的设置或其开源代码的默认设置相同（如表3所示）。

这些实验结果总结在表4-6中。表中的结果表明，LLMC工具在性能上几乎与文献中报道的原始量化算法一致。通过这些实验，证明了LLMC不仅有效，而且在重现现有量化方法的结果方面也是可靠的。这确保了该工具对LLM量化研究的贡献是可信且有价值的。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDIIVYDBc9AglPlgJNd7ov1FnIAGthfoA75eSGZEjZAL6rl2c7wzlgld5rZhAUpwPRibTlRfxW8FzA/640?wx_fmt=png&from=appmsg)  
![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDIIVYDBc9AglPlgJNd7ov1vH5OTxRY0bjy3u6TdjjicVa5ZOMpGFQQx9SKCQ56hzUNxibiatFiaRHpjQ/640?wx_fmt=png&from=appmsg)

**以超低成本进行量化** 。LLMC工具包旨在实现资源高效利用，并且能够以最低的硬件要求运行大型模型。得益于单block级别的运行机制，仅需要一台80G
A100即可完成Llama 3.1 405B的校准和评估，从而实现以超低成本进行量化。

**多后端兼容性** 。LLMC支持多种量化设置和模型格式，兼容多个后端和硬件平台，例如LightLLM、TRT-LLM、PPL-LLM、vLLM、MLC-
TVM和llama.cpp，具有高度的通用性。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDIIVYDBc9AglPlgJNd7ov151zCCwDgkoXicvEOuPRj0xBSJtlS97XkibXwk0DyLneSz0GuOiat7HzJA/640?wx_fmt=png&from=appmsg)

**高扩展性**
。该工具包高度模块化和可扩展，能够轻松适配从整数量化到浮点量化，从密集模型到专家混合（MoE）模型，从LLM到视觉语言模型（VLM），从量化到稀疏化。这种模块化设计确保用户可以扩展和自定义工具包，以满足他们的需求。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDIIVYDBc9AglPlgJNd7ov17q0J5s4BZicap3mFJibPAVjicwAuBZHzgQrNMsbmfbeKhg4pS4pdAR7ng/640?wx_fmt=png&from=appmsg)  
![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDIIVYDBc9AglPlgJNd7ov1ubYaIPnXTLag9Dctfrmfqa7nQw1lp5tacuYfcDJ8DNOvwJ3tgq1Uqw/640?wx_fmt=png&from=appmsg)

**多样化评估**
。LLMC能够对压缩模型进行综合评估，提供详细的性能指标和分析，例如困惑度（PPL）、数据可视化分析、峰度值（Kurtosis）、误差和异常值分布。这种全面的评估功能可确保用户能够就其模型的最佳压缩策略做出明智的决策。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDIIVYDBc9AglPlgJNd7ov1avSbk1xS2ODuw9MztsJk1Qg4lib0YnXcQSzbmFejIfBeicBbWHBMDOqQ/640?wx_fmt=png&from=appmsg)

LLMC团队发布了多功能的大模型压缩工具包LLMC，支持多种压缩算法、模型和推理后端，具有强大的扩展性和全方位的评估能力。

该工具包允许用户仅使用单个GPU即可对千亿参数LLM进行压缩，这极大地方便了LLM量化的应用。配备这款强大的工具包，未来的大模型研究人员以及普通用户可以为他们的应用程序有效地集成合适的算法和对应后端平台所需要的格式，从而普及大模型的压缩应用。

工具地址：https://github.com/ModelTC/llmc  
论文地址：https://arxiv.org/abs/2405.06001

— **完** —

  

投稿请发邮件到：

**ai@qbitai.com**

标题注明【投稿】，告诉我们：

你是谁，从哪来，投稿内容‍

附上论文/项目主页链接，以及联系方式哦

我们会（尽量）及时回复你

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtC5nGy7YMGhQ0ZJeyibWyL0KVCtiaLEPMyd4Bszuo0bFIOxZOvdmqdxnOosYXyu5aI7MXpyUrUWfz6g/640?wx_fmt=gif&tp=webp&wxfrom=5&wx_lazy=1)

  

**点这里 👇关注我，记得标星哦～**

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

