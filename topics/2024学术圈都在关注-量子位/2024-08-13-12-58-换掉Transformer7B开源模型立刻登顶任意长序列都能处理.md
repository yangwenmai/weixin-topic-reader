# 换掉Transformer，7B开源模型立刻登顶！任意长序列都能处理

文章作者: 量子位
发布时间: 2024-08-13 12:58
发布地: 北京
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247743677&idx=3&sn=6b33b501f5380d3b8b167bdad85c9703&chksm=e8df8fcfdfa806d952cc699a53faf51dc9d1d1c6ad224d503106123035b6cff13fdae12117a7#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtDErT6bj1B9syQriaNSnh1XicUJYuwZ81ibjPzWXX0UCWqY880gZdu5F5Dtktfva4vaHX0hWq8VR7aZA/300

##### 明敏 发自 凹非寺  
量子位 | 公众号 QbitAI

只是换掉Transformer架构，立马性能全方位提升，问鼎同规模开源模型！

（注意力机制不存在了）

这就是最新**Falcon Mamba 7B** 模型。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDErT6bj1B9syQriaNSnh1XicBBwfKuwvmCQSTfe5GLqibm7ibvib41HQgryibyNanylJGXLmgVlJSgpbAA/640?wx_fmt=png&from=appmsg)

它采用**Mamba状态空间语言模型架构** 来处理各种文本生成任务。

通过取消传统注意力机制，有效提升了模型处理长序列时计算效率低下的问题。

它可以处理**无限长** 序列，但内存需求不增加。

无论上下文多长，**生成每个token的时间基本一样** 。

由此，Falcon
Mamba模型性能全方位提升，打败一众Transformer架构模型，如Llama-3.1(8B)、Mistral(7B)以及Falcon-2(11B)。

![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtDErT6bj1B9syQriaNSnh1XicG3Vvx5rTPMibq1XQIxWJ6mllFVZQlkvmE7cA86SywzgM3yRjXib9qf6Q/640?wx_fmt=jpeg&from=appmsg)

如上成果由阿联酋阿布扎比技术创新研究所(TII)带来，他们正是Falcon模型的开发团队。

该系列共包含四个模型：基础版本、指令微调版本、4bit版本和指令微调4bit版本。  

最新模型遵循TII Falcon License 2.0开放协议，它在Apache 2.0协议下。

围观网友直呼：游戏规则要改变了！

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDErT6bj1B9syQriaNSnh1XicJ0m6AiaESRzTbr9f6ZO3cM0EvVUib3UjV6h5QdMKiaNMfOf9FBGRkW48A/640?wx_fmt=png&from=appmsg)

## 全球首个开源SSLM

在性能上，Falcon Mamba 7B全方位超越一众开源模型。

![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtDErT6bj1B9syQriaNSnh1XicfNHChticYAwic9e2LvaVGNbJRc4bsLeroAtHbz1VVVTgWlIOkiab0fbeQ/640?wx_fmt=jpeg&from=appmsg)

[它基于第一代Mamba。](http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247708081&idx=2&sn=0e66bd0ff20c6a969d24bf2176bf8c1a&chksm=e8df1ac3dfa893d5f83d58c5a0e0cc10b094a8ffd104a2fb818263642206639762139e3f6bae&scene=21#wechat_redirect)

Mamba是一种**状态空间模型** （SSM，State Space
Model）。它结合了RNN和CNN的特点，通过引入一种选择机制，它允许模型根据当前的输入有选择地传播或忘记信息，从而提高处理文本信息的效率。

同时，它设计了一种硬件感知的并行算法，以递归模式运行，避免了GPU内存层级之间IO访问，提高计算效率。

最后它还简化了架构，将SSM架构和Transformer中的MLP块结合为单一的块。

从Transformer换到Mamba，能够让Falcon模型可以处理任意长序列，但无需增加内存。尤其适合单个A10 24GB GPU。

研究还讨论了两种不同的处理序列方法。

并行预填充方法适用于GPU并行处理，对内存需求较高；顺序填充方法适用于SSM模型，可以处理任意长度序列，从而不会受到内存限制。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDErT6bj1B9syQriaNSnh1Xic0r22HicTutejRLIO8RPicNM4bAnfvxSLLxsTzXY7iasEOLa6CKygXyymw/640?wx_fmt=png&from=appmsg)

为了确保大规模训练稳定，Falcon Mamba模型使用了额外的RMS标准化层。

RMS标准化层能够简化LayerNorm的计算过程，可减少计算量。

模型使用了5500GT数据训练，这些数据主要来自RefedWeb数据集以及公开数据。训练过程基本匀速，在训练后期增加了一小部分高质量策划数据，这有助于模型在最后阶段的优化。

在H100上，批大小为1、提示词长度为1-130k生成token的测试中，Falcon Mamba能够在**生成新token时保持稳定的吞吐量**
，这意味着它的性能不受文本长度影响，可以稳定处理长序列，不会出现性能下降情况。

![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtDErT6bj1B9syQriaNSnh1XicYkvVbOzlcoUjaT4vNxPbSXhMsqMBFTOolXDsuabLb7vrUCEvWDNBEA/640?wx_fmt=jpeg&from=appmsg)  
![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtDErT6bj1B9syQriaNSnh1Xicbzg0rgH3eMu8PEOvUounMXUxeWEiad9MKVm58bsUVYqMtZzICCrMLwg/640?wx_fmt=jpeg&from=appmsg)

Falcon Mamba支持多种Hugging Face API，包括AutoModelForCausalLM、pipline。

还推出了一个指令调优版本，通过额外50亿个token进行微调，可以让模型准确性更高。

在Hugging Face、GitHub上都可访问最新模型~

参考链接：  
https://huggingface.co/blog/falconmamba#hardware-performance

— **完** —

**量子位年度AI主题策划** 正在征集中！

欢迎投稿专题 **一千零一个AI应****用** ，**365行AI落地方案**

或与我们分享你在**寻找的AI产品** ，或发现的**AI新动向**

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDpTavEwUl8aOlFLGHaPnaKXJcMUeJtGXVLliac6P6XxYHIKhnz0NPUgVvlrXAvJC33ibh8aYDdyudA/640?wx_fmt=png&from=appmsg)

  

**点这里 👇关注我，记得标星哦～**

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg)

