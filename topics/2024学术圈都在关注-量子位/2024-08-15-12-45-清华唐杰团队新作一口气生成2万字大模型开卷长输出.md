# 清华唐杰团队新作：一口气生成2万字，大模型开卷长输出

文章作者: 量子位
发布时间: 2024-08-15 12:45
发布地: 北京
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247743973&idx=4&sn=3accf681b282ff8937468739690a9928&chksm=e8df8e97dfa80781b85170ece72bc0ff09ee4885be2b353f55f0388fe42e93a6dea9e8f659ae#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtDUsgECyb6Dsiadj16wugkDicd6pCiaK3CgBzCqd35iaVLCakYCS381E8xpAtKjUk2PX4Zf1hUh0xVl0Q/300

##### 明敏 发自 凹非寺  
量子位 | 公众号 QbitAI

一口气生成**2万字** ，大模型输出也卷起来了！

清华&智谱AI最新研究，成功让GLM-4、Llama-3.1输出长度都暴增。

相同问题下，输出结果直接从1800字增加到7800字，**翻4倍** 。

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtDUsgECyb6Dsiadj16wugkDicCjSBYDasxMPib3iadZMjtgBu2icgULh4q5nBdsVtlyukLgSK3Bvy8ic7Fw/640?wx_fmt=gif&from=appmsg)

要知道，目前大模型的生成长度普遍在2k以下。这对于内容创作、问题回答等都存在影响，可能导致模型回答问题不全面、创造性降低等。

该研究由智谱AI创始人、清华大学教授李涓子和唐杰共同领衔。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDUsgECyb6Dsiadj16wugkDicWtCJxoZUcyA4EFAToZqtIdMQG02r9icWw2Cq5Mnj0icMiaiceAOQbtIzVQ/640?wx_fmt=png&from=appmsg)

论文及代码都已放在GitHub上开源。

有网友已经抢先体验。LongWriter-llama3.1-8b可生成万字长文《罗马帝国衰落史》，在MacBook Pro 2018（32GB）上就能运行。

> 输出内容很准确，可以得A++。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDUsgECyb6Dsiadj16wugkDicm2ueo2NPy4yXyrxJeqFVibpRj4Hwsehwth1atSich24E1MxdjhX6qrow/640?wx_fmt=png&from=appmsg)

## 9B模型搞定万字输出

本项研究主要包括3方面工作。

  * 分析文本生成长度限制因素

  * 提出AgentWrite

  * 扩展LLM输出窗口大小

首先，研究人员构建了一个测试工具LongWrite-Ruler。通过测试多个大模型，他们发现所有模型在生成**超过2000字** 的文本时都遇到了困难。

进一步分析用户和大模型的交互日志，研究人员发现只有超过1%的用户请求明确提到**要生成超过2000字** 的文本。

为此，他们改变了模型在监督式微调（SFT）阶段使用的**数据集的最大输出长度** 。

结果发现，模型的最大输出长度与SFT数据集中的最大输出长度呈**显著正相关** 。

所以得出结论，现有模型在输出长度上受限主要是因为**SFT数据集中缺少长输出样本** 。

即使模型在预训练阶段见过更长的序列，但是SFT阶段缺乏长文本样本，还是会影响输出长度。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDUsgECyb6Dsiadj16wugkDic1ARUEibkEnBzPZXgq86nmsA1TNknCtFj4yKj8ov8w7W1GIs8p5ju1jg/640?wx_fmt=png&from=appmsg)

为了克服这个限制，研究人员提出了**AgentWrite** 。

这是一个基于Agent的pipline。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDUsgECyb6Dsiadj16wugkDic8H6qhQicYySd8iaxibibJNfwE7D4SiaiaqZCkrPet3f9RETTFvKia5zUp98xA/640?wx_fmt=png&from=appmsg)

它允许将超长文本生成任务分解为多个子任务，每个子任务处理其中的一段。

具体流程是AgentWrite先根据用户指令制定出一个详细的写作计划，计划包括每个段落的主要内容点和目标词数。根据计划，AgentWrite依次提示模型生成每个段落的内容。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDUsgECyb6Dsiadj16wugkDicpOC6luPXMTlacf0AOQLyDsbibKT2Sp90R7E2ZuN73J9iadguL4gY4L0g/640?wx_fmt=png&from=appmsg)

在AgentWrite基础上，团队利用GPT-4o生成了6000个长输出SFT数据，输出长度在2k到32k词之间，构成了数据集LongWriter-6k。并将这些数据添加到训练过程中。

为了验证方法的有效性，团队还提出了一个LongBench-
Write。其中包含了多样化的用户写作指令，输出长度规格分别为0-500词、500-2000词、2000-4000词以及4000词以上。

评估结果显示，使用AgentWrite后模型输出长度明显增加。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDUsgECyb6Dsiadj16wugkDicV4n6JkuvgGlAjUCPW8LO7h2rLWYep7a3vAFMdrtwCLccicRBiagETibSQ/640?wx_fmt=png&from=appmsg)

通过直接偏好优化（DPO），GLM-4-9B在一众模型中实现了最佳性能。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDUsgECyb6Dsiadj16wugkDicdibmruGiciaaxa542GyQxsZc5ScXxsPZajEtEv7XhQD3bVCPwyawPOgFw/640?wx_fmt=png&from=appmsg)

手速快的网友已经抢先实测。

Reddit上一位网友让LongWriter-llama3.1-8b生成罗马帝国衰败史，整体需要22分钟（与硬件有关），平均每秒生成3.34个token。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDUsgECyb6Dsiadj16wugkDicD3F658TRcanbZMibZDBjicy4raoh1LXOoJrNv1VShQ68WKTCr3kovVLQ/640?wx_fmt=png&from=appmsg)

生成内容比较公式化，回答不同问题的结构、节奏相似。

> 无论如何这是个好的开始，带来的提升很明显。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDUsgECyb6Dsiadj16wugkDiciamgibgmZprSNqreZpxahRIXou1wrM37XCX76J8UAVFXhWr5CDI8KImw/640?wx_fmt=png&from=appmsg)

研究团队也表示未来将进一步扩展模型的输出长度和输出质量，同时也会开始研究如何在不牺牲生成质量的情况下提高效率。

参考链接：  
https://github.com/THUDM/LongWriter

— **完** —

**量子位年度AI主题策划** 正在征集中！

欢迎投稿专题 **一千零一个AI应****用** ，**365行AI落地方案**

或与我们分享你在**寻找的AI产品** ，或发现的**AI新动向**

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtDpTavEwUl8aOlFLGHaPnaKXJcMUeJtGXVLliac6P6XxYHIKhnz0NPUgVvlrXAvJC33ibh8aYDdyudA/640?wx_fmt=png&from=appmsg)

  

**点这里 👇关注我，记得标星哦～**

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg)

  

