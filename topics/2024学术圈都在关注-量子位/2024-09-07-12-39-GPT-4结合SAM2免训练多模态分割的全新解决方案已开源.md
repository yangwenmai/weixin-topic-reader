# GPT-4结合SAM2：免训练多模态分割的全新解决方案！| 已开源

文章作者: 量子位
发布时间: 2024-09-07 12:39
发布地: 北京
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247747590&idx=3&sn=b0db94019c5f26b37990d1b66602d359&chksm=e8dfbf74dfa83662e531c2e4957891d9a2917666cc6fc123b3445cc3f2e782cd720497147cb4#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtB1r2Fu9869Wlia1NpX1QfYM8wKV2UibPR15JAlzAjicEiayw94lTbgVCgKrYdR5oSJFN6AkcHFfiaHGeQ/300

##### 北京航空航天大学 李红羽 投稿 凹非寺  
量子位 | 公众号 QbitAI

**免训练** 多模态分割领域有了**新突破** ！

中科院信工所、北航、合工大、美团等单位联合提出了一种名为**AL-Ref-SAM 2** 的方法。

这种方法利用GPT-4和SAM-2来统一多模态分割，让系统在免训练的情况下，也能拥有不亚于全监督微调的性能！

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB1r2Fu9869Wlia1NpX1QfYMrPhdCKJMMM34t1OH8KHD19Hibmqt4uwFOEWWOqD4Jkib2ozfZvbtFClg/640?wx_fmt=png&from=appmsg)

## ≥ 全监督方法

多模态分割主要有两种方法：一种是依据**文字描述** 找到视频中特定对象的分割方法（RVOS），另一种是通过**声音识别**
视频中发声对象的方法（AVS）。

免训练的多模态视频指代分割虽然在数据和训练成本上有较大优势，却由于缺乏在特定任务数据上针对性的模型参数调整，导致性能与全监督方法有较大差距。

而研究团队要解决的就是这个问题。

实验中，他们对多个RVOS基准数据集进行了广泛验证，包括Ref-YouTube-VOS、Ref-
DAVIS17和MeViS，同时在AVSBench的多个子集上也进行了测试。

最后的实验结果显示，AL-Ref-SAL 2在这些数据集上的表现不仅**优于**
其他无需训练和弱监督的方法，并且甚至在一定情况下，系统的性能可以与**全监督方法相媲美** 。

特别是在Ref-YouTube-VOS和Ref-DAVIS17这两个数据集上，AL-Ref-SAM 2的表现甚至**超过了大多数全监督方法** 。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB1r2Fu9869Wlia1NpX1QfYMVTb3AnoltrxQ0MkibKic95eDp3ib5ibnVic7SFeKhDkGrbm1URsia1SOckKw/640?wx_fmt=png&from=appmsg)

###### **△** 免训练设置下三阶段分割基线方法(a)与本文方法(b)的比较

## 具体咋实现的？

研究团队把AL-Ref-SAM 2的算法分成了**三个阶段** ：

**第一阶段**
：获取形式统一的指代信息，对于RVOS任务，指代信息是输入的文本描述本身，而对于AVS任务，研究人员们利用LBRU模块将音频转化为对发声对象的语言形式描述。

**第二阶段** ：根据语言指代和视频内容，利用GPT-4进行两阶段时空推理，从视频中逐步选出关键帧和关键框。

**第三阶段** ：以关键帧为分割起点，关键框为初始提示，利用SAM 2获得目标对象在整段视频中的分割掩码序列。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB1r2Fu9869Wlia1NpX1QfYMFGpl4qEicKXD7MXKowRcf80EYNzOkH78fqg6DNCI7icRicTT4SDwtILTg/640?wx_fmt=png&from=appmsg)

###### **△** AL-Ref-SAM 2的整体流程

具体实验细节，请看下文展开~

**语言绑定的音频指代转换（LBRU）**

LBRU将音频信号转换为与语言描述统一的格式（例如“[CLS] that is making
sound”，其中[CLS]代表了具体的发声对象类别），以减少音频信息中的语义模糊性和冗余性。

为了获取发声对象的准确类别，LBRU利用了一个预训练音频分类器，如**BEATs** ，对音频进行分类，并保留置信度前k高的类别文本。

由于这些类别中可能包含了重复类别或背景声类别，LBRU进一步引入了视频作为视觉上下文，**利用GPT-4**
根据视频内容对音频类别进行过滤、合并，并将保留的音频类别转化为发出该声音的对象类别。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB1r2Fu9869Wlia1NpX1QfYMyfQthsufES08LxBAKwgSxv7bzASbiagb0kckjA87mTJRFdAtCiaWeXGA/640?wx_fmt=png&from=appmsg)

###### **△** 语言绑定的音频指代转换模块

**GPT辅助的关键帧/框选择（GPT-PS）**

GPT-PS利用**GPT-4** 分别进行时序推理选出关键帧，以及空间推理选出关键框。

在时序推理阶段，为了使GPT可以处理视频格式的内容，研究团队首先对视频帧进行采样，将采样后的若干帧拼接为一张图并在图上标出帧号。

为了显式引导GPT在理解视频内容的基础上选择关键帧，他们还**针对性地设计了关键帧思维链提示模板**
，要求GPT首先描述整段视频的场景，再根据语言指代选出关键帧。

之后，研究人员将语言指代信息和关键帧输入GroundingDINO模型中，获得多个**可能的候选框** 。

在空间推理阶段，首先将候选框画在关键帧上，并且依旧将其与其他采样帧顺序拼接作为视觉信号输入GPT。

类似地，他们也设计了**关键框思维链提示模板**
，要求GPT描述每个候选框中对象的特征和不同对象之间的关系，并对指代信息进行语法分析确定真正的指代主体，最后再根据语言指代选出包含目标对象的候选框作为关键框。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB1r2Fu9869Wlia1NpX1QfYMNsnLWocbIgWoy7FiawW0tnBAEQavpONOq3q2edWPq7ZeS5Aicib1b47YA/640?wx_fmt=png&from=appmsg)

以下是研究团队得出的相关数据：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB1r2Fu9869Wlia1NpX1QfYM7g3ZY90dSXicPgyaKoctUvvM0AeLnkEu1oFzTOYIiaR5onmeNhWKM4Ww/640?wx_fmt=png&from=appmsg)  
![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB1r2Fu9869Wlia1NpX1QfYMy9N4uBSRGVbUjJILvv2JXI8BZ1oiawxmib5bwFK2nKGIRzBQ24jy6gng/640?wx_fmt=png&from=appmsg)

论文链接：https://arxiv.org/pdf/2408.15876  
代码链接：https://github.com/appletea233/AL-Ref-SAM2

— **完** —  

  

投稿请发邮件到：

**ai@qbitai.com**

标题注明【投稿】，告诉我们：

你是谁，从哪来，投稿内容‍

附上论文/项目主页链接，以及联系方式哦

我们会（尽量）及时回复你

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtC5nGy7YMGhQ0ZJeyibWyL0KVCtiaLEPMyd4Bszuo0bFIOxZOvdmqdxnOosYXyu5aI7MXpyUrUWfz6g/640?wx_fmt=gif&wxfrom=5&wx_lazy=1&tp=wxpic)

  

  

**点这里 👇关注我，记得标星哦～**

  

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg&wxfrom=5&wx_lazy=1&wx_co=1&tp=webp)

