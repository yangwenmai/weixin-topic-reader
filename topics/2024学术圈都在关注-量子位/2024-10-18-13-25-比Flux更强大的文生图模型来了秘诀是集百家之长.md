# 比Flux更强大的文生图模型来了！秘诀是“集百家之长”

文章作者: 量子位
发布时间: 2024-10-18 13:25
发布地: 未知地区
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247753323&idx=5&sn=dcfd3876ece6cc6d8ed57c6cdc421366&chksm=e8dfa919dfa8200fc0d0343fd380d3e28c7f65b9de656b7a3b17790c5395e7fec8ac98e7252a#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtArVwYP4jsJAgia7G7bXcasIGxDu2xzgkibzya9ec3oj8eotxnnOLjCQ8tUV3FaIAFdWeu3H72jGKTw/300

##### IterComp团队 投稿  
量子位 | 公众号 QbitAI

打造更强大文生图模型**新思路** 有——

面对Flux、stable diffusion、Omost等爆火模型，有人开始主打**“集各家所长”。**

具体来说，清北、牛津、普林斯顿等机构的研究者提出了⼀个**全新文生图框架IterComp** 。

它能提取不同模型在各自领域的优势，同时不引入额外的复杂指令或增加计算开销。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtArVwYP4jsJAgia7G7bXcasIhVZDdib7tiafGU9Nghicktw9mo8YZY0o4m34NUVtTTfBSDte5gxfRTArQ/640?wx_fmt=png&from=appmsg)

论文一经发布，即在𝕏（前推特）获得AI论文领域大V转发，吸引大量关注。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtArVwYP4jsJAgia7G7bXcasIrTuTwbeFVQvwEJibxdMVTHjITVibd4AdF21EoN1tFol7RbOKPCSy9ZzQ/640?wx_fmt=png&from=appmsg)

那么，研究人员具体是如何实现的呢？

## 全新文生图框架：IterComp

⾃2022年以来，基于diffusion的文生图模型取得了快速发展，尤其在复杂组合生成（complex/compositional
generation）任务上取得了显著进展。

例如，今年8月发布的Flux展现出了十分震撼的复杂场景生成能力与美学质量；

RPG通过MLLM的辅助，将复杂的组合生成任务分解为简单子任务；

InstanceDiffusion通过布局控制（layoutbased），实现了与布局分布高度一致的精确图像生成。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtArVwYP4jsJAgia7G7bXcasI8YCbYJQeXtza4iakBjnzqS9UBXicS0eYBw8yPlWGryJg6qRjd9CdLOibA/640?wx_fmt=png&from=appmsg)

然而，这些模型的优势**仅限于某些特定的组合生成任务，且存在诸多局限** 。

**基于文本的生成方法**
（如SDXL、SD3、Flux等），由于其训练数据和框架的优势，在物体与场景的模拟上表现出色，特别在美学质量、属性绑定和非空间关系（non-spatial
relationships）方面具有显著优势。

然而，当涉及多个物体，且存在复杂的空间关系时，这类模型往往表现出明显不足。

**基于大语言模型（LLM-based）** 的生成方法，如RPG和Omost，通常需要对LLM进⾏额外训练或设计复杂的提示。

然而，对于LLM处理后的复杂指令，diffusion backbone并不具备精确生成的能力。

**基于布局（layourbased）**
的生成方法，如Instancediffusion和LMD+，虽然提供了精确的控制，但在图像的美学质量和真实性上存在明显下降，并且需要人为设计布局。

因此，一个值得深⼊探讨的问题是：

> **能否设计出一个强大的模型，在上述多个方面都表现出⾊，同时不引入额外的复杂指令或增加计算开销？**

基于此，研究人员提出一个全新的文生图框架：**IterComp** 。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtArVwYP4jsJAgia7G7bXcasIBz9PtGMznMd0lFarFLoqTLSXejAzR7kLevUULoybicEBjLY5cnwuxcQ/640?wx_fmt=png&from=appmsg)

要充分解决这⼀问题，研究面临**两大难点** ：

**1、****如何提取不同模型在各自领域的优势，并引导模型进行有效学习？**

针对该问题，研究人员首先构建了模型库（model
gallery)，其包含多种在不同方面表现出色的模型，每个模型都具备特定的组合生成偏好（composition-aware model
preference）。

研究人员尝试通过**扩散模型对齐方法** ，将base diffusion model与模型库中的组合生成偏好进行对齐。

团队聚焦于compositional generation的三个关键方面：

  * 属性绑定（attribute binding）

  * 空间关系（spatial relationship）

  * ⾮空间关系（non-spatial relationship）

为此，研究人员收集了不同模型在这些方面的偏好数据，并通过人工排序，构建了⼀个面向组合生成的**模型偏好数据集** （composition-aware
model preference dataset）。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtArVwYP4jsJAgia7G7bXcasIe1vKQrCw3N7TzNxUrKbpng0GKx4TUXO8wd6Ky23fsa5bUP1xGhj7Jw/640?wx_fmt=png&from=appmsg)

针对这三个方面，团队分别训练三个composition-aware reward models，并对base模型进行**多奖励反馈优化** 。

**2、组合生成任务很难优化，如何更充分地学习到不同模型的组合偏好？**

研究人员在diffusion领域创新地引入**迭代式学习框架** （iterative feedback learning），实现reward
models与base diffusion model之间“左脚踩右脚登天”。

具体来说，在上述第一轮优化后，团队将optimized base diffusion model以及额外选择的其他模型（例如Omost等）添加进model
gallery。

对新增模型进行偏好采样，与初始model gallery中的图像按照预训练的reward model构建图像对。

这些图像对被用于进⼀步优化奖励模型，随后继续用更新的奖励模型优化基础扩散模型。

具体的流程如下伪代码所示：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtArVwYP4jsJAgia7G7bXcasI7ibbYS4dyWvMRxD0VibCSTfx1Uq7TaU0YI1hiaOKuKRTVRZybX5Su4OJA/640?wx_fmt=png&from=appmsg)

## 实验

在**定性实验** 中，与其他三类compositional
generation方法相比，IterComp取得了显著的组合质量的提升，并且不会引入额外的计算量。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtArVwYP4jsJAgia7G7bXcasI3kXRvm0kYMdIFUyjPPOEicOp6q8ubvwL1sHML1UkUtic0UpcXK3t73hg/640?wx_fmt=png&from=appmsg)

从定量结果中可以看出，IterComp在T2I-CompBench上取得了各方面的领先。

另外，针对图像真实性以及美学质量，IterComp也表现出色。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtArVwYP4jsJAgia7G7bXcasI63dZSDRn2lr5A5JyOnxfm0rfFgibZPdCIfJGlRDG3hIC83brsCV1JTA/640?wx_fmt=png&from=appmsg)  
![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtArVwYP4jsJAgia7G7bXcasITp7GcVUnoGtFPfmQM1ATIvnxO9l3ZPLUdr4oDHeRicYnohWKOXdmiahg/640?wx_fmt=png&from=appmsg)

然而IterComp的应用潜力不限于此，其可以作为强大的backbone显著提升Omost, RPG等模型的生成能力。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtArVwYP4jsJAgia7G7bXcasIvMYDW4V5PkVxuugtyWpznmbLfpTHPJkiccZBm6cshfSup79mTu7Dxeg/640?wx_fmt=png&from=appmsg)

更多细节欢迎查阅原论文。

论文地址：  
https://arxiv.org/abs/2410.07171  
代码地址：  
https://github.com/YangLing0818/IterComp  
模型地址：  
https://huggingface.co/comin/IterComp  
Civitai:  
https://civitai.com/models/840857

— **完** —

  

投稿请发邮件到：

**ai@qbitai.com**

标题注明【投稿】，告诉我们：

你是谁，从哪来，投稿内容‍

附上论文/项目主页链接，以及联系方式哦

我们会（尽量）及时回复你

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtC5nGy7YMGhQ0ZJeyibWyL0KVCtiaLEPMyd4Bszuo0bFIOxZOvdmqdxnOosYXyu5aI7MXpyUrUWfz6g/640?wx_fmt=gif&tp=webp&wxfrom=5&wx_lazy=1)

  

**点这里 👇关注我，记得标星哦～**

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

