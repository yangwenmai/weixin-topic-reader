# 微软开源爆火1.58bit大模型推理框架！千亿参数模型量化后单CPU可跑，速度每秒5-7个token

文章作者: 量子位
发布时间: 2024-10-22 17:57
发布地: 北京
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247754111&idx=4&sn=7fb24a99074a24dd17067b0aff6a33c2&chksm=e8dc560ddfabdf1b1b61785e813c5c718d4031a1629a644d2a8ab09e63241f558c7c2fb569a1#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtBtv6DAbjeSibVzicODbsAq4MYZZCjAIqugkF2chNsvZwkqGpCJD80yRMIev3J5A2U2B6CGNQ1nJibfA/300

##### 西风 发自 凹非寺  
量子位 | 公众号 QbitAI

微软**开源** 1****bit大模型推理框架！

现在1000亿参数大模型量化后单CPU可跑，速度可达每秒5-7个token。

比如在**苹果M2** 新品上运行BitNet b1.58 3B模型，be like：

就是今年爆火论文**The Era of 1-bit LLMs** 的官方代码实现，开源不到一周GitHub已揽获**7.9k Star** 。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBtv6DAbjeSibVzicODbsAq4MvJ9kwRu59KWlkYuqTcW1GNuYxd5Nhkoa1df1LibbK0u9nTQpwJ3WY5A/640?wx_fmt=png&from=appmsg)

传统大模型参数以16位浮点数（如FP16或BF16）形式的存储，而BitNet b1.58将其统统变成了**三进制** ，也就是**{-1, 0, 1}**
。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBtv6DAbjeSibVzicODbsAq4MWuia9RPzafq1DFrmFmUAiaV4Vhhd8MQqcoEE9JhpJl5A4O6lrnUqeT9A/640?wx_fmt=png&from=appmsg)

这里的“1.58 bit”指每个参数可以用1.58位的信息来表示。

转换之后，**矩阵中的计算****就只会涉及到****加法**
，因此会让大模型在保持一定精度的同时，显著减少所需的存储空间和计算资源，也显著提升了在本地设备上运行LLM的可能性。

这个项目开源后，在X上也受到了一波高度关注。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBtv6DAbjeSibVzicODbsAq4MwaicQfwtKduoGKQvFGicibYhPvNu1lhqWJnjAaRBa0SZf4EL2saicK3M8Q/640?wx_fmt=png&from=appmsg)

## 千亿参数模型量化后单CPU可跑

**bitnet.cpp** 是1bit LLM（例如 BitNet b1.58）的官方推理框架。

该框架配备了一系列优化内核，支持在CPU上进行快速且无损的1.58bit模型推理，未来将扩展支持NPU和GPU。

bitnet.cpp的**首版主要支持CPU推理** 。

具体性能改进方面，在ARM CPU上，该框架可实现**1.37至5.07倍的加速** ，而且更大的模型将有更显著的性能提升。

同时，它能将**能耗降低55.4%至70.****0%** ，进一步增强效率。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBtv6DAbjeSibVzicODbsAq4MskmKqWCchFPHKiaKP4kqJXyLTwCXzQX4nS018wx4L6RSxvOzpqUsgNA/640?wx_fmt=png&from=appmsg)

在x86 CPU上，**加速效果介于2.37至6.17倍** 之间，**能耗减少71.9%至82.2%** 。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBtv6DAbjeSibVzicODbsAq4MrzyzRaetqqGhAYfZvnkovFSEh2QC9wicR1LWTr4qjRxrnYyMFsXcxHA/640?wx_fmt=png&from=appmsg)

网友们也发现了华点，在x86上的性能增益量比ARM更大。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBtv6DAbjeSibVzicODbsAq4MGWibsUC7L0ahomXqLicgvKwetSjpstIfD4RfIw3LQbEQ7DT6icG8vZ8AQ/640?wx_fmt=png&from=appmsg)

此外，bitnet.cpp能使千亿参数模型量化后单CPU可跑，速度可达每秒5-7个token，接近人类阅读速度。

微软还展示了********使用bitnet.cpp推理框架支持的不同1 bit LLM。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBtv6DAbjeSibVzicODbsAq4McRXtOviaibJgX9DiaDrTfLaoMQoGBK4ib5MS9Pne1uMtp9bicUcbtmkfQ4A/640?wx_fmt=png&from=appmsg)

## 6页论文，引入1 bit LLM

1 bit LLM的实现方法，微软在一年前就有相关研究，称为BitNet（一种Transformer），用**BitLinear**
替换了nn.Linear。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBtv6DAbjeSibVzicODbsAq4MdaLJ8YFZ1FD3C2d51NKy6bobrVvGcp77YJR67mfCQzs75ibvPU0wfJQ/640?wx_fmt=png&from=appmsg)  
![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBtv6DAbjeSibVzicODbsAq4MYMbz2Gv3ETHB6WaUZHGRWLR4gI7JXXjrCcb4kSmB0mGYWh2iapjrZhA/640?wx_fmt=png&from=appmsg)

今年二月，BitNet原班人马在上一篇论文的基础之上做了优化，提出BitNet b1.58，在原始BitNet的基础上增加了一个额外的**0值** 。

然后这篇内容只有6页的论文引发热议：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBtv6DAbjeSibVzicODbsAq4MticGvYW6FvaTmI42COR0sJ5841eWdNQEcia656ebwXkVga14XYIs7qmQ/640?wx_fmt=png&from=appmsg)

BitNet b1.58模型的权重被量化为三元值{-1, 0, 1}，**相当于在二进制系统中使用了1.58 bit来表示每个权重** 。

采用了absmean量化函数来约束权重，将权重矩阵通过其平均绝对值进行缩放，然后四舍五入到最接近的整数值（-1、0或1）。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBtv6DAbjeSibVzicODbsAq4MoNZsIG8J1OIHtiag1EWZv8pia4fgIicJDYK5qVzWRHuibQ0fVMbgwvIv9Q/640?wx_fmt=png&from=appmsg)

激活量化中，激活值被缩放到[−Qb, Qb]的范围，以此来消除零点量化。

在架构设计上，BitNet
b1.58借鉴了Llama，使用了RMSNorm、SwiGLU、旋转位置编码等组件，并移除了所有偏置项。这种设计使其能够轻松集成到主流的开源框架中。

实验中，与Llama相比，BitNet b1.58**在矩阵乘法方面节省了71.4倍的计算能耗。**

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBtv6DAbjeSibVzicODbsAq4MGk6G849UG99JsNMpfkoAnsxwMsYNQwJt6dLCNS0j2IFiaM6fLlnJ5DA/640?wx_fmt=png&from=appmsg)

这种方法发布后，也有不少人在这项研究的基础之上进行探索。

其中一个问题是，BitNet b1.58将每个参数仅用三元值表示，但是所有这些都需要从头开始训练模型，并不是谁都有预算来进行LLM预训练。

而Huggingface Transformers最近整合了BitNet b1.58，运用了一些技巧，使得**现有模型可以直接微调到1.58bit** 。

感兴趣的童鞋可以自行查阅。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBtv6DAbjeSibVzicODbsAq4Mcw0j6t27Uzsm82hv14jEMFYwJaa2E4ezDcRic0y9bNtv7Sdh4mfLH5A/640?wx_fmt=png&from=appmsg)

不过也有网友指出了这种方法的局限：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBtv6DAbjeSibVzicODbsAq4M1KkPkIic2fColPRxl9IyqDJ66GNFTyQ01E7mNEkRqy1EnicV7UOxqycg/640?wx_fmt=png&from=appmsg)

总之，1 bit LLM具有巨大的潜力。

但也正如网友所言，1 bit LLM关键还得是能在实践中证明自己。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBtv6DAbjeSibVzicODbsAq4MJs8g5cG3uDydyBmkOAUBRfT3pZMDxdnBojg6mt6A0iazKvlwDIlZozQ/640?wx_fmt=png&from=appmsg)

参考链接：  
[1]https://github.com/microsoft/BitNet  
[2]https://x.com/rohanpaul_ai/status/1847814379657462201  
[3]https://x.com/rohanpaul_ai/status/1848172443258728860?s=46&t=iTysI4vQLQqCNJjSmBODPw  
[4]https://huggingface.co/blog/zh/1_58_llm_extreme_quantization

— **完** —

**评选征集中**

**「2024人工智能年度评选」**

量子位2024人工智能年度评选已开启报名通道，评选从**企业** 、**人物** 、**产品** 三大维度设立了5类奖项。

**欢迎扫码报名评选！**
评选结果将于12月[MEET2025智能未来大会](http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247752188&idx=2&sn=c1bc1e4d987c3a10cfef338059b3dfb1&chksm=e8dfae8edfa82798657f4fcb6469d47175940482fd452f1aff146be45890942a2385a2533344&scene=21#wechat_redirect)公布，期待与数百万从业者共同见证荣誉时刻。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAOVibXbw5eUnvqbCic6T1OKtFJzFhIdiauXic5xgYVG2LogYPX94d9GO5yiaQKicPFPUwgM30w350XNfIQ/640?wx_fmt=png&from=appmsg)

**点这里 👇关注我，记得标星哦～**

**一键三连「点赞」、「分享」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg)

