# 微软清华改进Transformer：用降噪耳机原理升级注意力，一作在线答疑

文章作者: 量子位
发布时间: 2024-11-03 12:32
发布地: 未知地区
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247756224&idx=3&sn=56b8503c76a2ebe0908f7bbd49e9377c&chksm=e8dc5eb2dfabd7a467fbc1ae6ba6d19aa20ee1411aa2ec89310ece27dc7a2976cbad04296a45#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtBoKXMMaGolsCJzpMqfYz1aepMicV7Plw8Mq6vT9uhoZQXvVFL2eSnpSrSLRzxoOOqgVKw7QurfLNQ/300

##### 梦晨 发自 凹非寺  
量子位 | 公众号 QbitAI

Transformer自问世后就大放异彩，但有个小毛病一直没解决：

总爱把注意力放在不相关的内容上，也就是**信噪比低** 。

现在微软亚研院、清华团队出手，提出全新改进版Differential Transformer，专治这个老毛病，引起热议。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBoKXMMaGolsCJzpMqfYz1aicRo5zFVxibLeibyhYrhSHLibvfVDRII9CO1Mz3BCoQ9XbC9B4q1xu0pEg/640?wx_fmt=png&from=appmsg)

论文中介绍，整体思路类似差分放大电路或降噪耳机，用两个信号的差值来滤除共模噪声。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBoKXMMaGolsCJzpMqfYz1aAicYkOsPEWrz3YTv56lcLFtlc0jd7LibkLTjCVsvQoMM9CqnF3u1cVQg/640?wx_fmt=png&from=appmsg)

具体到在语言模型中，如果句子很长，只有少数token会真正影响当前token的含义。而注意力机制允许每两个词之间产生交互，其中就包含大量噪声了。

团队提出的方法是在注意力层中增加一个Softmax，然后两个Softmax做减法。

这一减，噪音信息就被大幅抵消，让注意力更集中在相关内容上。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBoKXMMaGolsCJzpMqfYz1aDnyZGom3dkHpmZo5lfvTGH8ZJJHia8u8z2kbCIjvWicn8ibY2Ky4QiahIQ/640?wx_fmt=png&from=appmsg)

语言建模任务上的一系列实验结果显示，仅需约65%的模型大小或训练tokens，DIFF
Transformer就能达到与传统Transformer相当的性能。

新架构在长上下文建模、关键信息检索、减少幻觉、提高上下文学习能力以及减少激活异常值等各项指标中，普遍优于Transformer架构。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBoKXMMaGolsCJzpMqfYz1aibr5VJVzd63chwAxoBKy3Pr0EZn9XJwXNtZuLcBicic0cW9zYPMye6HpQ/640?wx_fmt=png&from=appmsg)

论文上传到arXiv平台后，有不少学者到[弹幕版alphaXiv](http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247742020&idx=1&sn=74ab3fbf394f2e94c7f71a70f6c165f7&chksm=e8df8536dfa80c20e08a8ab4e775435463a3884698fb1332b43d89b736b455976043ecb861fa&scene=21#wechat_redirect)划线提问。一作Tianzhu
Ye正绝赞在线答疑中。‍‍‍‍‍‍‍‍‍‍‍

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCfEeQVAHCict3MEmZqWCvzdv7Njq0ju9EBs2ruicVvFaWDlSVZKnUzEjZSQNevbdHxnXF3fknribVaQ/640?wx_fmt=png&from=appmsg)

## 差分Transformer

与传统Tranformer相比，DIFF Transformer保持宏观架构不变，主要区别在于用差分注意力替换传统softmax注意力。

此外还采用了LLaMA系列中的一些改进，如pre-RMSNorm归一化和SwiGLU激活函数。

在差分注意力模块中，需要先给Q和K分成两个组，然后分别计算softmax。

第二组乘了一个标量λ，是可学习的参数，在同一层的注意力头之间共享。

λ的引入是为了在差分操作中平衡两组注意力的贡献，使得差分注意力机制能够更好地适应不同的任务需求和数据分布。  
![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBoKXMMaGolsCJzpMqfYz1ahictCYQ60JvwwtdQPyafFbGbbWf0ia7RaVuuxtqiaibqeMKGYFsb9Gv9FA/640?wx_fmt=png&from=appmsg)

接下来是一系列实验结果。

### 语言建模评估

在1T tokens上训练3B大小的DIFF Transformer，遵循
StableLM-3B-4E1T的配方，在各种下游任务中与以前训练良好的Transformer模型相比表现出优势。  
![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBoKXMMaGolsCJzpMqfYz1aUAojiboQbSqlWibWIncCib0iapMrpDkbAKIWtNDUNhMgicY3licWypdubh3Q/640?wx_fmt=png&from=appmsg)

### 可扩展性评估

只需约65%的模型参数或训练tokens来匹配Transformer的性能。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBoKXMMaGolsCJzpMqfYz1a0xJ019LoFURxJh6VxRadticpH4o3hUZOWWnNVBdiatsibtViaazbM7heLg/640?wx_fmt=png&from=appmsg)

### 长上下文能力评估

在额外1.5B tokens上训练3B大小的DIFF Transformer，扩展上下文长度至64k。

随着上下文长度增加，累计平均负对数似然（NLL）持续降低，并且比传统Transformer的NLL值更低。

表明DIFF Transformer可以有效利用不断增加的上下文。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBoKXMMaGolsCJzpMqfYz1axQW8WjIpbJ6qhzpIFIy1udWELQJZUEDJzrhvp2pt7PXATbPhfr6x9Q/640?wx_fmt=png&from=appmsg)

### 关键信息检索能力评估

也就是多个“针”的大海捞针试验，设置不同的上下文长度（4K和64K）来模拟不同复杂程度的信息检索场景。

在4K上下文长度下，随着插入 “针” 数量和查询数量的增加，DIFF Transformer的准确率保持稳定，而Transformer 的准确率显著下降。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBoKXMMaGolsCJzpMqfYz1afdPibyca303qZTBBvic5O66SiaTfFtKyacEJLIPyWibaKQdLvYgE1epAXg/640?wx_fmt=png&from=appmsg)

在64K上下文长度下，DIFF
Transformer在不同答案针深度（即关键信息在长上下文中的位置）和上下文长度下都能保持稳定性能，且在关键信息位于上下文前半部分时优势明显。

特别是当关键信息位于25%深度时，DIFF Transformer比Transformer的准确率提高了 76%。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBoKXMMaGolsCJzpMqfYz1aIwmWn9Y9FOHoK2HGSibTEquEU8ds7A3MrASQ8DU30EAkdQ5YzuzhAQA/640?wx_fmt=png&from=appmsg)

### 上下文学习能力评估

分为两个角度来评估，分别是多样本分类和上下文学习的稳健性。

多样本分类任务，同样使用64K上下文长度的3B参数模型，DIFF
Transformer的准确率始终高于Transformer，提升幅度从5.2%到21.6%不等

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBoKXMMaGolsCJzpMqfYz1aCb8aqcyQNNfBu5oWHALF78pMny4s5JyNpTKgtMMCiasAMM2znDBNeRA/640?wx_fmt=png&from=appmsg)

上下文学习稳健性采用排列顺序任务，DIFF Transformer的结果方差远小于传统Transformer。  
![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBoKXMMaGolsCJzpMqfYz1aXmG2463RKssOAZVxAkIxWQtvhcibZA21g66Ih4x4ia7uSNOGJ6epg0XA/640?wx_fmt=png&from=appmsg)

### 上下文幻觉评估

主要关注输入中包含正确事实，但模型仍然无法产生准确输出的情况。

将模型输出与ground-truth一起发给GPT-4o，让GPT-4o来判断是否存在幻觉，此前试验表明GPT-4o与人类评判结果一致率较高，相对可靠。

在不同数据集上DIFF Transformer的准确率更高，幻觉更少。  
![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBoKXMMaGolsCJzpMqfYz1a6cnXGGXEI7XhDugODIRyy6YdiaZibZeo1p9iacusUAzpgq9qgVo1yM6Pw/640?wx_fmt=png&from=appmsg)

### 激活异常值分析

Transformer中的激活异常值，导致模型在训练和推理过程中难以量化。

试验比较了注意力logits和隐藏状态两种激活类型下的最大激活值，DIFF Transformer都表现出更低的顶部激活值，即产生更少的激活异常值。

在对注意力logits进行量化实验时，DIFF Transformer在降低比特宽度量化时仍能保持较高性能，而Transformer在6-bi
量化时准确性显著下降。

4-bit的DIFF Transformer能达到与6-bit的Transformer相当的准确性，且比4-bit的Transformer准确率提高约
25%。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBoKXMMaGolsCJzpMqfYz1a9QRb9OmicXNdsNrNJt2Q2AOVkc7Jtv7OSqrluO2krv91Kf2tAFDMuIw/640?wx_fmt=png&from=appmsg)

## 代码已开源， 降噪耳机类比引热议

对于目前读者的疑问，作者已做出几点答复‍‍‍‍‍‍‍‍‍‍‍‍‍‍

**问题1：** Diff Transformer与每个注意力头温度可学习的方法有什么不同？与门控注意力对比如何？

作者回应在实验中，可学习温度效果不大。而本文方法是门控注意力的改进。  
‍‍‍‍‍‍

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCfEeQVAHCict3MEmZqWCvzdPaN7hdtIjjNiaHZmE3tor8wFiaFPX4Vw36L9dMrEEy16WGqibFWibX92uA/640?wx_fmt=png&from=appmsg)

**问题2：差分注意力是否意味着将标准注意力矩阵参数翻倍？**

作者澄清，单个注意力头维度翻倍，但是注意力头数量减半，总体在参数和FLOPS上都是对齐的。**‍ ‍‍**

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCfEeQVAHCict3MEmZqWCvzdImzE9JYiaiaia5XHGiasrKhbIedYeqTzNP8qn7k9qRtfzaCicRmF55wicS4A/640?wx_fmt=png&from=appmsg)

**问题3：第二组Softmax乘可学习参数lambda的研究思路。**

作者也做出详细回应。‍‍‍

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCfEeQVAHCict3MEmZqWCvzdPksP4UkqIsgD2lJMl8RrykuBvciaQPTXxXf2ehSqWut0ibclcOSibSz1w/640?wx_fmt=png&from=appmsg)

DIFF Transformer在纯学术圈之外也引起非常多的讨论，有不少人困惑论文中将方法与降噪耳机的类比。

降噪耳机采集环境噪声并生成相反的信号，在这种情况下哪些信号属于噪声是已知的，但差分注意力中并不能事先确定哪些是噪声。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBoKXMMaGolsCJzpMqfYz1aicFhuyf0qfVp15icqpksH9TINfFAgoOuJWx4NZ4fmndwWTfdD7KNImUA/640?wx_fmt=png&from=appmsg)

一种解释是，低注意力分数的噪声也有很低的梯度，因此模型其实已知哪些是噪声，只是单个Softmax无法输出0，所以噪声很难去除。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBoKXMMaGolsCJzpMqfYz1aiaYSqYaX44tag5vtD3QzWsHWoVPuZgYJA3bYld30IcNbCLLbkgAHu6g/640?wx_fmt=png&from=appmsg)

也有人提出，比起降噪耳机，其实专业音频中“平衡线”，或者USB、网卡等传输方式更适合一些。

使用两条信号线传输正负信号，接收器只对比他们之间的差异，由于在空间中离得很近，受到的干扰是相同的。

有用的正负信号相减会被放大，相同的噪声相减却被抵消，大大增强抗干扰能力。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBoKXMMaGolsCJzpMqfYz1az72ujLL0eBvurgjoJYGzWb6tko0lkm57aGA1gho7XdLPydz7leGib8w/640?wx_fmt=png&from=appmsg)

总之，DIFF Transformer代码已开源在微软unilm项目下，其中还包含魔改版支持差分注意力的FlashAttention-2的代码。  

感兴趣的可以试起来了。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBoKXMMaGolsCJzpMqfYz1ax1iacIqAib4gGTzsSg1ib5mZcYHAHQTohNpe5aZWoK4TTurC2bwP8wc3Q/640?wx_fmt=png&from=appmsg)

论文：  
https://arxiv.org/abs/2410.05258

代码：  
https://aka.ms/Diff-Transformer

参考链接：  
[1]https://news.ycombinator.com/item?id=41776324

— **完** —

**评选征集中**

**「2024人工智能年度评选」**

量子位2024人工智能年度评选已开启报名通道，评选从**企业** 、**人物** 、**产品** 三大维度设立了5类奖项。

**欢迎扫码报名评选！**
评选结果将于12月[MEET2025智能未来大会](http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247752188&idx=2&sn=c1bc1e4d987c3a10cfef338059b3dfb1&chksm=e8dfae8edfa82798657f4fcb6469d47175940482fd452f1aff146be45890942a2385a2533344&scene=21#wechat_redirect)公布，期待与数百万从业者共同见证荣誉时刻。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAOVibXbw5eUnvqbCic6T1OKtFJzFhIdiauXic5xgYVG2LogYPX94d9GO5yiaQKicPFPUwgM30w350XNfIQ/640?wx_fmt=png&from=appmsg)

**点这里 👇关注我，记得标星哦～**

**一键三连「点赞」、「分享」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg)

