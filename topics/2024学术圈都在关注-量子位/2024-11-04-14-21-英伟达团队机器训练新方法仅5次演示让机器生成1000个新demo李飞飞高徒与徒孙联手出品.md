# 英伟达团队机器训练新方法！仅5次演示让机器生成1000个新demo，李飞飞高徒与徒孙联手出品

文章作者: 量子位
发布时间: 2024-11-04 14:21
发布地: 北京
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247756503&idx=4&sn=348c5329d40dd855f2ae60ef7aaeffa0&chksm=e8dc5da5dfabd4b3d3c2063ab363c7d6cf01af07023bf8e55d44c21cb5b5bdcfd12b18d795bc#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtBmEkW0iasCtmZ09TbMQc9e9y88HMwrkmMVVhFIVzXBS1JBgqgJ2Lm16z6ShyV588RfFbZPE9Wr4YQ/300

##### 克雷西 发自 凹非寺  
量子位 | 公众号 QbitAI

人类只需要演示五次，就能让机器人学会一项复杂技能。

英伟达实验室，提出了机器人训练数据缺乏问题的新解决方案——**DexMimicGen** 。

五次演示之后，DexMimicGen就可以直接模仿出1000个新的demo。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBmEkW0iasCtmZ09TbMQc9e9kyCoPoiaGyKxn1FqxSCMibomY3xElYbT6qxMc5hwOG7C9NUwmJkvpvHw/640?wx_fmt=png&from=appmsg)

而且可用性强，用这些新demo训练出的机器人，在仿真环境中的任务成功率可以高达97%，比用真人数据效果还要好。

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtBmEkW0iasCtmZ09TbMQc9e9PcldUFXoSzI0xdYIhA3cNn7F8zxgvfCgqQqADveUP3wnt4DxobrNrQ/640?wx_fmt=gif&from=appmsg)

参与此项目的英伟达科学家范麟熙（Jim Fan）认为，这种**用机器训练机器** 的方式，解决了机器人领域最大的痛点（指数据收集）。

同时，Jim Fan还预言：

> 机器人数据的未来是生成式的，整个机器人学习流程的未来也将是生成式的。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBmEkW0iasCtmZ09TbMQc9e96BbiboxsyX6CyeJYEibOHAVZMjkAdiaovaZyoMNyI822UQFkEDMC2nM0g/640?wx_fmt=png&from=appmsg)

值得一提的是，DexMimicGen三名共同一作都是李飞飞的“徒孙”，具体说是德克萨斯大学奥斯汀分校（UT奥斯汀）助理教授朱玉可（Yuke Zhu）的学生。

而且三人均为华人，目前都在英伟达研究院实习。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBmEkW0iasCtmZ09TbMQc9e9Ny4ByLjlW6tj0KtRSyKDuYQE3G3niby1fzHs2mY5Eqe6e97ZM1aNEyw/640?wx_fmt=png&from=appmsg)

## 5次演示，生成1000条数据

如前所述，DexMimicGen可以仅根据人类的5次演示，生成1000个新DEMO。

在整个实验中，作者设置了9个场景，涵盖了3种机器人形态，共进行了60次演示，获得了21000多个生成DEMO。

在仿真环境当中，用DexMimicGen生成数据训练出的策略执行整理抽屉这一任务，成功率可达76%，而单纯使用人工数据只有0.7%。

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtBmEkW0iasCtmZ09TbMQc9e9qxwEEkT5IyIKsZYXz9VjsmmFs2XDM6GP4X4WVTPwb3le3NYr0yDX1g/640?wx_fmt=gif&from=appmsg)

对于积木组装任务，成功率也从3.3%提升到了80.7%。

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtBmEkW0iasCtmZ09TbMQc9e9chicaYEgicic60s1RPnjhyKnAPbuW5eL8fWVqrK9nF89aw4Bdwyhbwv0g/640?wx_fmt=gif&from=appmsg)

成功率最高的任务是罐子分类，更是高达97.3%，只用人工数据的成功率同样只有0.7%。

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtBmEkW0iasCtmZ09TbMQc9e9YadWSn6gUvW0ohveQOUfTI59zoicsdnCfVZ8BXLDvGKY0ic1YBw0Tyvw/640?wx_fmt=gif&from=appmsg)

整体来看，在仿真环境中，生成数据让机器人在作者设计的九类任务上的成功率均明显增加。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBmEkW0iasCtmZ09TbMQc9e9YqYricmZzddgMZo31nqQxlcPHIYumNIAN7A0ichqvJkAfzvmyVX0tlrQ/640?wx_fmt=png&from=appmsg)

相比于baseline方法，用DexMimicGen生成的数据也更为有效。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBmEkW0iasCtmZ09TbMQc9e9oeoPjyycia2hnEpUfiaIRXYxV4ka6y3krb7xdx2Ps1x88toUTBLvW7Zw/640?wx_fmt=png&from=appmsg)

迁移到真实环境之后，作者测试了易拉罐分拣的任务，结果仅用了40个生成DEMO，成功率就达到了90%，而不使用生成数据时的成功率为零。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBmEkW0iasCtmZ09TbMQc9e93wrvDhrZG5wucEPEOBGjY8WuqUELKkicS8kUxLwrdGBrEz16fhyvwZg/640?wx_fmt=png&from=appmsg)

除此之外，DexMimicGen还展现了跨任务的泛化能力，使训练出的策略在各种不同任务上表现良好。

针对初始状态分布变化，DexMimicGen也体现出了较强的鲁棒性，在更广泛的初始状态分布D1和D2上测试时，仍然能够拥有一定的成功率。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBmEkW0iasCtmZ09TbMQc9e9iaAhIvo6US37kicQ7TXMkKqLMB8WPBBONLSzsXVNgZ93nAAqIvicrE5Vg/640?wx_fmt=png&from=appmsg)

## 将仿真方法迁移到现实

DexMimicGen是由**MimicGen** 改造而成，MimicGen也出自英伟达和UT奥斯汀的联合团队。

朱玉可和范麟熙都参与过MimicGen的工作，该成果发表于CoRL 2023。

MimicGen的核心思想，是将人类示范数据**分割成以目标物体为中心的片段** ，然后通过**变换物体相对位置和姿态**
，在新环境中复现人类示范轨迹，从而实现**自动化数据生成** 。

DexMimicGen则在MimicGen系统的基础上，针对双臂机器人灵巧操作任务做了改进和扩展，具体包括几个方面：

  * 引入**并行、协调、顺序** 三种子任务类型，以适应双臂灵巧操作任务的需求；

  * 对应三种子任务类型，设计了异步执行、同步执行和顺序约束等机制，以实现双臂的独立动作、精密协同和特定顺序操作；

  * 实现了“现实-模拟-现实”的框架，通过构建数字孪生，将DexMimicGen拓展到了实际机器人系统的应用。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBmEkW0iasCtmZ09TbMQc9e9BqYH1jITkx6sDSZ0MbQPh3LM46gFK1iagUjb6nianq1iaEtnG5I23TdMw/640?wx_fmt=png&from=appmsg)

工作流程上，DexMimicGen会首先对人类示范进行采集和分割。

研究人员通过佩戴XR头显，远程控制机器人完成目标任务，在这一过程中就会产生一小批示范数据，作者针对每个任务采集了5~10个人类示范样本。

这些人类示范样本会按照并行、协调、顺序三种子任务定义被切分成片段——

  * 并行子任务允许两臂独立执行；

  * 协调子任务要求两臂在关键时刻同步动作；

  * 顺序子任务则规定了某些子任务必须在另一些子任务完成后才能执行。

总之，在示范数据被切分后，机器人的每个手臂会得到自己对应的片段集合。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBmEkW0iasCtmZ09TbMQc9e9DXDe5IKCch6DxoCGriclyjFrz4fBrWCgDjcRJwBGH19k1zFCbKpXkvw/640?wx_fmt=png&from=appmsg)

在数据生成开始时，DexMimicGen随机化模拟环境中物体的位置、姿态等数据，并随机选择一个人类示范作为参考。

对于当前子任务，DexMimicGen会计算示范片段与当前环境中关键物体位置和姿态的变换。

之后用该变换对参考片段中的机器人动作轨迹进行处理，以使执行这一变换后的轨迹能够与新环境中物体位置匹配。

生成变换后，DexMimicGen会维护每个手臂的动作队列，手指关节的运动则直接重放示范数据中的动作。

在整个过程中，系统不断检查任务是否成功完成，如果一次执行成功完成了任务，则将执行过程记录下来作为有效的演示数据，失败则将数据丢弃。

之后就是将生成过程不断迭代，直到获得足够量的演示数据。

收集好数据后，作者用DexMimicGen生成的演示数据训练模仿学习策略，策略的输入为RGB相机图像，输出为机器人动作。

最后是模拟到现实的迁移，同样地，作者使用DexMimicGen在数字孪生环境中生成的大规模演示数据，训练模仿学习策略。

之后作者对在数字孪生环境中评估训练得到的策略进行调优，以提高其泛化性能和鲁棒性，并迁移到实际机器人系统中。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBmEkW0iasCtmZ09TbMQc9e9wEBUjAa3TDI1JBfu5pT5InqhJLPr4BtsyWkX05oxxNGCzNyVLSAp6Q/640?wx_fmt=png&from=appmsg)

## 作者简介

DexMimicGen的共同一作有三人，都是UT奥斯汀的华人学生。

并且三人均出自李飞飞的学生、浙大校友朱玉可（Yuke Zhu）助理教授门下，他们分别是：

  * 博士生Zhenyu Jiang，本科就读于清华，2020年进入UT奥斯汀，预计将于明年毕业；

  * 硕士生Yuqi Xie（谢雨齐），本科是上海交大和美国密歇根大学联培，预计毕业时间也是明年；

  * 博士生Kevin Lin，本科和硕士分别就读于UC伯克利和斯坦福，今年加入朱玉可课题组读博。

朱玉可的另一重身份是英伟达的研究科学家，团队的另外两名负责人也都在英伟达。

他们分别是Ajay Mandlekar和范麟熙（Jim Fan），也都是李飞飞的学生，Mandlekar是整个DexMimicGen项目组中唯一的非华人。

另外，Zhenjia Xu和Weikang Wan两名华人学者对此项目亦有贡献，整个团队的分工如下：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBmEkW0iasCtmZ09TbMQc9e9CZXwMMVadqCqEH4XQtjIUvicUH2ic3CsLtHqHjrV81FX1VcQLeswEapg/640?wx_fmt=png&from=appmsg)

###### **△** 中文为机翻，仅供参考

项目主页：  
https://dexmimicgen.github.io/  
论文地址：  
https://arxiv.org/abs/2410.24185  
参考链接：  
[1]https://x.com/SteveTod1998/status/1852365700372832707  
[2]https://x.com/DrJimFan/status/1852383627738239324

— **完** —

**评选征集中**

**「2024人工智能年度评选」**

量子位2024人工智能年度评选已开启报名通道，评选从**企业** 、**人物** 、**产品** 三大维度设立了5类奖项。

**欢迎扫码报名评选！**
评选结果将于12月[MEET2025智能未来大会](http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247752188&idx=2&sn=c1bc1e4d987c3a10cfef338059b3dfb1&chksm=e8dfae8edfa82798657f4fcb6469d47175940482fd452f1aff146be45890942a2385a2533344&scene=21#wechat_redirect)公布，期待与数百万从业者共同见证荣誉时刻。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAOVibXbw5eUnvqbCic6T1OKtFJzFhIdiauXic5xgYVG2LogYPX94d9GO5yiaQKicPFPUwgM30w350XNfIQ/640?wx_fmt=png&from=appmsg)

**点这里 👇关注我，记得标星哦～**

**一键三连「点赞」、「分享」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg)

  

