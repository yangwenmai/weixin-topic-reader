# 80G显存塞50个7B大模型！清华&OpenBMB开源增量压缩新算法，显存节省8倍

文章作者: 量子位
发布时间: 2024-11-29 13:11
发布地: 北京
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247762549&idx=4&sn=f7d3e8767d7cbc8b68afc3c83d97c0b2&chksm=e8dc7507dfabfc11ee66cd5acff9af3dd580b7946a987330f3f608ec0c24df65555c953dce5a#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtAslXt2ZHMXrSx8b4JAn1dh6ktHvaMDkvciaTFiasFeWFNvibZErbFsfko1P5ibibNIIEibeP2DakeTVMaA/300

##### Delta-CoMe团队 投稿  
量子位 | 公众号 QbitAI

最新模型增量压缩技术，一个**80G的A100 GPU** 能够轻松加载多达**50个7B模型** ，**节省显存约8倍**
，同时模型性能几乎与压缩前的微调模型相当。

清华大学NLP实验室携手OpenBMB开源社区、北京大学和上海财经大学的研究团队，提出**Delta-CoMe** 。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAslXt2ZHMXrSx8b4JAn1dhazbkJgZ02017jawnOnicXZSy8nE6AHcB7gGhrZDeq1jiaGhcxpPuYAIg/640?wx_fmt=png&from=appmsg)

这项技术的核心在于**利用主干模型与任务专用模型之间参数增量**
（即Delta）的特点进行压缩，从而实现存储开销和部署成本的大幅降低。不仅有助于解决资源瓶颈问题，更为多任务处理和模型部署开辟新的可能。

具体而言，Delta-
CoMe将低秩分解和低比特量化技术相结合，充分利用Delta参数的低秩特性，提出了一种全新的混合精度压缩方法。这种方法不仅能够实现接近无损的任务性能，还能显著提升推理效率。

## Delta-CoMe方法介绍

微调是增强预训练模型的重要手段，不同任务往往需要不同的微调方式。例如Luo et al.[1]提出RLEIF通过Evove-
instruction来增强模型数学推理能力；Wei et al.[2]利用Code
snnipet合成高质量的指令数据来增加模型的代码能力。然而，这些方法通常依赖高质量数据，并需要精心设计的策略才能实现显著的效果。

在一些场景中往往需要具有不同能力的LLM同时处理问题，例如多租户场景，多任务场景以及端侧场景等等。一种自然的解决方案是部署单个通用模型作为主干，配合多个具有专有能力的Delta。

以Bitdelta[3]为例，它通过将模型的Delta压缩到1-bit，有效保留了模型在问答等场景中的能力。尽管该压缩方法在存储和推理效率上表现出色，其在更复杂的任务（如数学推理和代码生成）上仍存在明显的能力瓶颈。

针对这一挑战，THUNLP实验室联合北京大学和上海财经大学提出Delta-
CoMe。这一方法结合低秩分解和低比特量化技术，不仅显著提升了模型在复杂任务上的表现，还兼顾了压缩效率和实际应用需求，为模型的高效部署提供了一种新思路。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAslXt2ZHMXrSx8b4JAn1dhX9ia4pgR9ZKOZ7UkyXwibbygaN8GMDlQW1fhianSEvoN92N38AxYF8gaA/640?wx_fmt=png&from=appmsg)

与前人的方法相比，Delta-CoMe方法的优点在于：

  * **结合低秩与低比特量化，** 利用了Delta低秩的特点，并发现低秩分解后的Delta是长尾分布的；之后采用混合精度量化进一步压缩

  * **性能几乎无损，** 相比于BitDelta等方法，在Math, Code, Multi-modal等复杂任务上，性能与压缩前的微调模型表现基本接近

  * **推理速度提升，** 为混合精度量化实现了Triton kernel算子，对比Pytorch的实现方式，带来近3倍的推理速度提升

  * **超过Delta-tuning，支持多精度Backbone，** Delta-CoMe在效果上显著优于LoRA微调，并可以用在多种精度的Backbone上

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAslXt2ZHMXrSx8b4JAn1dhic7ibD89tVLP375XxFzUghhABOiaCA5H108aGb8LsVLpadJNl5zj7j3oQ/640?wx_fmt=png&from=appmsg)

具体而言，Delta-CoMe首先采用SVD进行低秩分解，Delta
具有低秩性，经过低秩分解之后，其特征值呈现出长尾分布的规律，仅有少数较大奇异值对应的奇异向量对最终的结果贡献较大。

一个自然的想法，我们可以根据奇异值的大小进行混合精度量化，将较大的奇异值对应的奇异向量用较高精度表示，而较小的奇异值对应的奇异向量用较低精度表示。

## 实验结果

多个开源模型和 Benchmark 的实验验证了该方法的有效性。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAslXt2ZHMXrSx8b4JAn1dh0fw3RvCTtPb9DeFl8Nialz8wnIxu6sgFtCkSgxQt6mHDQdDV379sNPA/640?wx_fmt=png&from=appmsg)

使用Llama-2作为主干模型，在数学、代码、对话、多模态等多个任务中进行实验，Delta-
CoMe展现出平均几乎无损的性能。下面分别是7B模型和13B模型的实验效果。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAslXt2ZHMXrSx8b4JAn1dhFJPGhkWV366tfZwT7oIYsaDAxQ6ff7QANNnxhHztYhN8JHg3wg0tOA/640?wx_fmt=png&from=appmsg)

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAslXt2ZHMXrSx8b4JAn1dhVddRgHVlHWumxaol9zkW6Hic8ozBFWPpjBzOQBJkjrOM6ge5IgiaC7QQ/640?wx_fmt=png&from=appmsg)

此外，还在Mistral、Llama-3等其它主干模型上对不同的压缩方法进行了验证。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAslXt2ZHMXrSx8b4JAn1dhDJDGayJv9W7fzBlheGY0VntSfcXbIE7icvNWzrotK9rlMEGdic3D15sw/640?wx_fmt=png&from=appmsg)

为了提升混合精度量化的计算效率，实现一个Triton Kernel，相比于Pytorch的实现方式，推理速度提升了约3倍。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAslXt2ZHMXrSx8b4JAn1dhcNuiagib6TjBlTFq3j1p4akWNiaJFlVIWMCRJVE6ngeiaiccTkCZZtC3zbg/640?wx_fmt=png&from=appmsg)

实验结果表明，使用一块80G的A100 GPU可以加载50个7B模型。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAslXt2ZHMXrSx8b4JAn1dhdLXEJeu0DKX0UUF6nUwlwy6YkicJ8UUTrYy7X7JCnIYVtagSEF5IXgw/640?wx_fmt=png&from=appmsg)

最后，还比较了Delta-Tuning和Delta-Compression的效果差异（Delta-Tuning指的是通过训练部分参数进行微调，Delta-
Compression指的是先进行全参数微调，再将微调带来的模型参数增量进行压缩）。其中Delta-Tuning采用的是LoRA。Delta-
CoMe对比LoRA在相同的存储开销下，性能显著提升。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAslXt2ZHMXrSx8b4JAn1dhb1kkUwLxh8jZDFktNeUtKibfHUKsl9ibzVycl7vp2o3UbZtMCh7BOnow/640?wx_fmt=png&from=appmsg)

Delta-CoMe
通过结合低秩分解和低比特量化，不仅实现了大幅度的存储压缩，还在复杂任务如数学推理、代码生成和多模态任务上维持了与压缩前模型相当的性能表现。相比于传统的微调方法，Delta-
CoMe 展现出了更高的灵活性，尤其在多租户和多任务场景中具有显著的应用价值。此外，借助 Triton kernel
的优化，推理速度得到了显著提升，使得部署大规模模型成为可能。未来，这一方法的潜力不仅在于进一步优化模型存储和推理速度，也有望在更广泛的实际应用中推动大语言模型的普及和高效运作。

## 参考文献

[1]Yu, L., Jiang, W., Shi, H., Jincheng, Y., Liu, Z., Zhang, Y., Kwok, J., Li,
Z., Weller, A., and Liu, W.Metamath: Bootstrap your own mathematical questions
for large language models. In The Twelfth International Conference on Learning
Representations, 2023.  
[2] Luo, Z., Xu, C., Zhao, P., Sun, Q., Geng, X., Hu, W., Tao, C., Ma, J.,
Lin, Q., and Jiang, D. Wizardcoder: Empowering code large language models with
evol-instruct. arXiv preprint arXiv:2306.08568, 2023b  
[3] Liu, J., Xiao, G., Li, K., Lee, J. D., Han, S., Dao, T., and Cai, T.
Bitdelta: Your fine-tune may only be worth one bit. arXiv preprint
arXiv:2402.10193, 2024b.

Paper链接：https://arxiv.org/abs/2406.08903  
Github链接：https://github.com/thunlp/Delta-CoMe

— **完** —

  

投稿请发邮件到：

**ai@qbitai.com**

标题注明【投稿】，告诉我们：

你是谁，从哪来，投稿内容‍

附上论文/项目主页链接，以及联系方式哦

我们会（尽量）及时回复你

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtC5nGy7YMGhQ0ZJeyibWyL0KVCtiaLEPMyd4Bszuo0bFIOxZOvdmqdxnOosYXyu5aI7MXpyUrUWfz6g/640?wx_fmt=gif&tp=webp&wxfrom=5&wx_lazy=1)

  

**点这里 👇关注我，记得标星哦～**

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

