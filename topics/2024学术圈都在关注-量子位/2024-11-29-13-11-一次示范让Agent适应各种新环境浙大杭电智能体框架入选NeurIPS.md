# 一次示范让Agent适应各种新环境，浙大杭电智能体框架入选NeurIPS

文章作者: 量子位
发布时间: 2024-11-29 13:11
发布地: 北京
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247762549&idx=5&sn=14161c23100247ba88b27cdcc0dd5a78&chksm=e8dc7507dfabfc11e0407ee4c6e98cc3b0f865086d10f960ceae0980495c49f6770591f80497#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtAslXt2ZHMXrSx8b4JAn1dh2SqfB0OiaJsGzVkQxXfB5iapkgKrXE2XYyZKHtnKS3pEwc2ybLBwjkqQ/300

##### 陈铭浩 投稿  
量子位 | 公众号 QbitAI

只需一次人类示范，就能让智能体适应新环境？

来自杭州电子科技大学和浙江大学的研究者，提出了一套新的智能体框架**AutoManual** 。

**该研究****有效解决了智能体过度依赖人类专家提供的知识，难以自主适应新环境的问题** 。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAslXt2ZHMXrSx8b4JAn1dhPQXTicxbDua7orpPYHzQaz76Fic7vdFWoBedRyibInSpLIvx4LMlPVuQw/640?wx_fmt=png&from=appmsg)

通过模仿人类认识世界“记笔记”的过程，AutoManual可以让智能体执行任务成功率高达**97%** 。

不仅如此，智能体在过程中学习的经验还可以供人类阅读，甚至给其他智能体提供规划指导。

## 现有智能体对人类依赖较大

目前，基于大语言模型的智能体（LLM Agents）展现出强大的潜力，能够自主完成各个领域的任务，如机器人规划、游戏角色控制与网站导航。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAslXt2ZHMXrSx8b4JAn1dhCaTlmdgJdEhWuLRdnXOxWICrZ3M7KVyG1icbzXIYVjicvKyMOo1AWHsA/640?wx_fmt=png&from=appmsg)

###### **△** AgentBench: Evaluating LLMs as Agents.Xiao Liu (THU) et al.
arXiv.

然而，这些智能体往往是为特定环境和特定任务设计的。

如果我们分析一个 LLM Agent 的系统提示词（System Prompts），会发现它通常由这五个部分组成：

  * 角色描述；

  * 可供使用的动作函数；

  * 输出格式；

  * 额外指示或要求；

  * 人类专家的示例。

对于新的环境，其中的前三项可以根据新环境对已有模板做调整后很快速地定义好；

但对于后两项提示词，**会需要人工汇总环境知识，并不断调试这些提示，以及准备多个人类专家示例** ，才能使 LLM Agent在新环境中顺畅运行。

那么，是否能让智能体自己从环境交互中学习这些知识呢？

已有的一些工作使用自我反思self-reflection或技能库skill library，来让智能体在交互中自我提升，部分缓解了对人工的依赖。

然而，这些反思和技能并没有用于对环境形成深入的理解，即理解环境的知识或机制。

因此，直接使用经验中的技能来作为大模型的上下文示例，容易形成**路径依赖** 。

## 从人类认识世界过程获得灵感

AutoManual框架有效地解决这一难题，其研究者从**人类认识世界的过程** 中获取了灵感——

当面对陌生的环境时，人类会通过探索发现、记录与更新自身的理解来逐渐认识到新环境的规律；

而且，人类可以将自己的理解整理出来，以文本的方式传授给他人。

AutoManual就效仿了这种过程来记录和更新LLM Agent对环境的理解。

最终，AutoManual框架将生成的一本指导手册，**不仅可以提高智能体对新环境的适应性，还可以为较小的模型的规划提供指导，并且易于人类阅读** 。

仅需一个人类演示，AutoManual便在机器人规划环境ALFWorld将智能体的成功率提高到**97%**
，在网站导航环境MiniWoB++上的任务成功率则达到**98%** 。

具体来说，AutoManual 框架整体由三个阶段组成：

  * **Building阶段：** Planner Agent与Builder Agent合作从环境的交互中构建出一系列的规则。当规则超过最大限制时，Consolidator Agent将合并或删除冗余的规则；

  * **Formulating阶段：** Formulator Agent将规则制定成一个Markdown格式的指导手册；

  * **Testing阶段：** 将指导手册提供给测试时的Planner Agent，来评估效果。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAslXt2ZHMXrSx8b4JAn1dhwNia6qw4nt0vK35UM7ff3k7AatmrAo0ibCdjP7TiawKxj6P38XqduibC7g/640?wx_fmt=png&from=appmsg)

###### **△** AutoManual框架总览

首先在Building 阶段，研究者受在线强化学习的启发，**使用了两个交替的迭代过程来构建环境规则** ：

  * 基于当前规则，****Planner Agent与环境进行一轮交互；

  * Builder Agent根据该交互轨迹使用规则系统来更新规则。

与传统强化学习相比，基于文本的规则管理取代了样本效率低下的参数优化。

具体而言，**对于Planner Agent** ，研究者采用Python代码来表示的可执行的计划，这是因为已有工作表明使用代码作为输出能有效提升LLM
Agent效果。

在每一轮的开始，Planner的输入为目前已知的规则，技能库或反思库中相关的案例，当前的任务与初始观测。

而每次Planner 的输出分为四个部分：

  * 对当前观测的分析；

  * 相关规则的解读；

  * 总体计划；

  * 一个划分为多个步骤的Python代码块。

然后，代码将在环境中执行，并得到反馈与新的观察结果。

在这一整轮结束时，根据任务是否成功，结果可以分为三种情况：Direct Success、Indirect Success（发生错误但稍后解决）和
Failure。

对于不同情况，提示 Planner 相应地汇总技能代码或反思，而这些技能和反思会存入技能库或反思库来辅助后续的任务完成。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAslXt2ZHMXrSx8b4JAn1dhYO7GgmGyjd75icHB4TXia0kdjxNBh4HYVWuqb7NpGMeaI7cibgjwHsQicg/640?wx_fmt=png&from=appmsg)

###### **△** Planner Agent与环境进行交互形成Trajectory的过程

**对于Builder Agent** ，其将根据Planner这轮的轨迹，使用规则系统的工具函数来编写和更新规则。

为了促进规则管理，研究者引入了**一个结构化的规则系统** ，规则系统中的每个规则都具有以下四个属性：

  * 规则的类型（分为了6种规则）；

  * 规则的内容；

  * 规则的示例；

  * 验证日志。

然而，研究者发现Builder Agent在面对这种结构化的规则系统时，有时候会出现幻觉，例如从失败的轨迹中得出成功经验的规则。

为了降低错误创建规则的风险，研究者对Builder采用了**case-conditioned prompting策略** ：

Builder首先需要分析并确定主要错误的来源为“Imperfect Rules”或“Imperfect
Agents”，然后相应的针对性的提示会指导Builder进行规则管理。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAslXt2ZHMXrSx8b4JAn1dheN9fETS10o69qDtpibt95YRygmtvJnTBeZia6Nib0UmXmiaD3tH352pHow/640?wx_fmt=png&from=appmsg)

###### **△** Case-Conditioned Prompting策略示例

在Building阶段结束后，Formulating阶段的目标是增强规则的可读性和全局理解。

因此，作者选择引入Formulator Agent对规则自动进行分类，总结每类的关键点，并以Markdown的格式将它们制定成一本指导手册。

## 只需一个人类示例

为了测试AutoManual框架的效果，研究团队在三个知名的交互式环境中进行了实验：

  * ALFWorld，一个家用机器人的虚拟环境，提供了基于文本的交互方式；

  * MiniWoB++，一个模拟Web环境，智能体通过执行键盘和鼠标操作在网页上完成各种任务；

  * WebArena，一个逼真的Web环境，复制了现实的Reddit网站的功能和数据。

在Building和Formulating阶段，所有Agent都配备了GPT-4-turbo （gpt-4-1106-preview）。

在Testing阶段，Planner Agent将配备GPT-4-turbo 或 GPT-3.5-turbo，来评估生成的手册是否可以指导较小的模型。

从ALFWorld任务的结果中可以看出，AutoManual需要很少的环境相关的专家先验知识，**只提供一个人类示例即可获得十分出色的结果** 。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAslXt2ZHMXrSx8b4JAn1dhubV4EI6EZNQ1740Sw5JdS8jKmSHkkwQice3ic8WnPqTObX5RL5aTpp2w/640?wx_fmt=png&from=appmsg)

而对于另外两个Web环境的结果，也可以得出相同的结论。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAslXt2ZHMXrSx8b4JAn1dh5GgzUTdLVLbXuQMmcsagUhZj5byliaMckvhpUVNXicn4VRkpdriaOqZsQ/640?wx_fmt=png&from=appmsg)

此外，AutoManual生成的 Markdown 手册**对人类阅读也很友好** 。

通过分析AutoManual生成的手册，可以看到其**发现了许多有意思的环境规则** 。

比如在rule_2，类型为“Special Phenomena”的规则中说：

>
> 当使用微波炉时，即使里面有另一个物体，智能体拿着什么东西，并且没有明确提到微波门是打开的，智能体也可以与它互动（例如，加热一个物体）。然后其举了一个例子，是在epoch_1中的经历。

还有在rule_3中说：

> Agent一次只能持有一个物体，并且必须在拿走另一个物体之前放下任何持有的物体。

因此，**AutoManual 通过更深入地挖掘机制、更新和整合成功流程以及注释重要细节来解决只使用技能的路径依赖问题。**

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAslXt2ZHMXrSx8b4JAn1dhfJiatUeJfkczXbARNuGFqHFYxmmbUD2gHfAtHgxOmmAOjp95FVCicctw/640?wx_fmt=png&from=appmsg)

###### **△** ALFWorld环境中AutoManual生成的Markdown手册

## 作者简介

该论文由杭州电子科技大学和浙江大学等合作完成。

第一作者陈铭浩，现任杭州电子科技大学计算机学院特聘副教授，博士毕业于浙江大学CAD&CG国家重点实验室。

![](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtAslXt2ZHMXrSx8b4JAn1dhicyLb6icUd4uibkRLyPP4GVGcMSwJV8cRJuib6f6dnYBVRSWvFgC7ibp5hQ/640?wx_fmt=jpeg&from=appmsg)

论文链接：https://arxiv.org/abs/2405.16247  
GitHub地址：https://github.com/minghchen/automanual

— **完** —

  

投稿请发邮件到：

**ai@qbitai.com**

标题注明【投稿】，告诉我们：

你是谁，从哪来，投稿内容‍

附上论文/项目主页链接，以及联系方式哦

我们会（尽量）及时回复你

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtC5nGy7YMGhQ0ZJeyibWyL0KVCtiaLEPMyd4Bszuo0bFIOxZOvdmqdxnOosYXyu5aI7MXpyUrUWfz6g/640?wx_fmt=gif&tp=webp&wxfrom=5&wx_lazy=1)

  

**点这里 👇关注我，记得标星哦～**

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

