# 一周发明GAN！时间检验奖得主分享背后故事：每件发明都不是最后的发明

文章作者: 量子位
发布时间: 2024-12-11 12:25
发布地: 北京
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247766207&idx=4&sn=a414c6df48c52c693ddbe204fa8574b0&chksm=e8dc67cddfabeedb971d75da1ae2ab4ccd3988f482259cf325023d8db9840668c8c3ab982499#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtAh9MlP6vG4rng17ccgOhvz8IBibhJNWHKElhFUibqicvSrLZ7oj4q40zZKz2n0QjXBn1fmjpvJOwibkg/300

##### 西风 发自 凹非寺  
量子位 | 公众号 QbitAI

引用超85000次的经典论文**GAN获NeurIPS2024时****间检****验奖** 后，它的起源和背后故事也被抛了出来。

要从**Yoshua Bengio实验室** 的一次头脑风暴说起。

Bengio召集实验室成员，提出了一个富有挑战性的设想：

训练一个确定性的生成网络g，该网络仅在输入z中包含随机噪声。这个网络的输出x=g(z)应该是从某个分布p(x)中抽取的样本。输出可以是任何形式：图像、音频、文本。

正当众人皆无头绪之时，一个在当时看似滑稽且几乎无意义的想法揭开了GAN的序幕：

> **如果能有另一个神经网络充当判别器，会怎样？**

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAh9MlP6vG4rng17ccgOhvz6KUBicSKj8ObJFw8V35496xMF6DKottgrYecqhTuajmPdtqzYBno6Wg/640?wx_fmt=png&from=appmsg)

作者之一Sherjil Ozair，一边讲述着这段经历，一边还透露曾有DeepMind研究员向他开玩笑，说他可能已经完成了最伟大的工作，可以直接退休了。

但他认为事实并非如此。

CNN感觉像是最后的发明，但并不是。

GAN感觉像是最后的发明，但也不是。

LSTM、ResNet、DQN、AlphaGo、AlphaZero、MuZero都并非终结。

> Transformer和大语言模型，亦不是最后的发明。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAh9MlP6vG4rng17ccgOhvzEEdP114ewawVmHgib9stfhcNNxtbVtzSyYibR71Tfy0AiaBibKpKaaicZtg/640?wx_fmt=png&from=appmsg)

这项出自Yoshua Bengio、lan
Goodfellow等一众大佬，引用超过85000次，被NeurIPS2024官方评价为“生成建模的基础部分之一，在过去10年中激发了许多研究进展”的研究。

究竟是如何炼成的？

## Sherjil Ozair讲述背后故事

以下是Sherjil Ozair的完整自述：

非常高兴听到GAN（生成对抗网络）在2024年NeurIPS大会上获得时间检验奖。

NeurIPS时间检验奖是授予那些在十年时间里经受住考验的论文。

“我”花了一些时间回顾GAN是如何产生的以及过去十年中人工智能的发展。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAh9MlP6vG4rng17ccgOhvzibNVuAa6IGmIWdf0bS8JHPibykqvPGlONRvAtHpc4MibWWVf6e8sDJT0w/640?wx_fmt=png&from=appmsg)

2012年初，当“我”还是印度理工学院德里分校的本科生时，“我”偶然发现了Geoffrey Hinton在Coursera上的一门深度学习课程。

深度学习当时是机器学习中一个边缘化且小众的分支领域，它承诺能实现更多的“端到端”学习，并且更接近人类大脑的工作方式。

这门课非常精彩。它不仅很好地解释了深度学习的原理，还充满了Hinton特有的英式幽默和非传统思维。

比如，他建议“我们”这样可视化高维空间：

> 要处理14维空间中的超平面，想象一个3维空间，然后大声对自己说“14”，每个人都是这么做。
>
> 但请记住，从13维到14维的转变，其增加的复杂性与从2维到3维的转变一样大。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAh9MlP6vG4rng17ccgOhvzsICUqHt7myhd9ypib8Darxuib7P4l3icRC0vxib76uokN0RoQuEs66icwTA/640?wx_fmt=png&from=appmsg)

出于好奇兴奋地想学习更多知识，“我”开始仔细研究所有能找到的资料。

当时主要是一些杰出研究者发表的学术论文，比如**Yoshua Bengio** ，其中很多都保存在他实验室的网站上。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAh9MlP6vG4rng17ccgOhvzYBvGj9VVVsm8NwgK4uHLIhOVVPdWpSEcJeRGpIhiakhMz3VSk8SrbWg/640?wx_fmt=png&from=appmsg)

2012年，Quora非常火爆，Yoshua经常在Quora上回答有关深度学习的问题。

“我”真诚地感谢他帮助像“我”这样的本科生理解深度学习。“我”通过Quora联系他，表达谢意。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAh9MlP6vG4rng17ccgOhvzGjI86YLQCickicibRNy8yHvwNZxb0uSUbNeTl5Sq7zYNvz7oFMdYK4mQQ/640?wx_fmt=png&from=appmsg)

令“我”非常惊喜的是，“我”不仅收到了回复，还收到了一份他实验室的实习邀请。

这是一次命运的相遇，而当时的“我”对这次交流和即将展开的旅程的重要性和影响力还只有一点点模糊的认识。

“我”由衷地感激Yoshua Bengio为这个世界和为“我”所做的一切。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAh9MlP6vG4rng17ccgOhvza2pAcZhicn8btLCPI0giaY6m5XaT3FV5rdI7Y4icNPmLlYoJbnysqzodg/640?wx_fmt=png&from=appmsg)

“我”通过了面试获得了实习机会，2014年夏天，将在Yoshua的LISA实验室实习。

本想2013年就实习的，但印度理工学院的制度要求学生必须在第三学年的暑假在他们认可的公司实习。

2014年5月，“我”飞抵蒙特利尔，来到了实验室。

刚见到Yoshua，他就立马把“我”拉进了一个房间，里面坐着的还有Ian Goodfellow和Aaron Courville。

Yoshua继续解释着他最近一直在思考的一个新想法：

**设想构建一个确定性的生成网络g，只在输入z中包含随机噪声。这个网络的输出x=g(z)应该是来自某个分布p(x)的样本，可以是任何形式：图像、音频或文本。**

他强调这就是“我们”需要训练的目标。

但怎么训练呢？在这种“隐式”网络中，概率p(x)并没有明确表达。

他提出应该对生成器的输出（生成分布）和某个样本数据集（可以是图像、音频等）进行“双样本分布匹配”。

但如何进行这种分布匹配仍然不明确。

作为一个年轻天真的本科生，“我”提出了矩匹配，但“我们”都知道矩匹配可能无法应对高维数据。小组里也讨论了其他想法，也都感觉不够有说服力。

不过，Yoshua对训练一个确定性的、消耗噪声并产生样本的生成神经网络的愿景和热情令人印象深刻且富有启发性。

团队决定私下继续思考这个问题。

在Les Trois Brasseurs餐厅的一次实验室聚餐中，Ian Goodfellow突然想到了一个在当时看似滑稽且几乎毫无意义的主意：

**如果让另一个神经网络来充当判别器会怎样？**

这是一个开拓前沿的时刻。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAh9MlP6vG4rng17ccgOhvzJjWz7FymXAfvK5iauu4Y4yNJkvGgoS9en0gtmCjkt8BiaVYPt4NUysicw/640?wx_fmt=png&from=appmsg)

当时，神经网络的训练还相当“原始”。通常做法是：

建立一个主神经网络，输入数据，得到一个预测结果，对其应用一个数学损失函数，然后使用梯度下降来优化这个网络。

而Ian的想法则把损失函数本身设想成一个可学习的神经网络。不是优化一个固定的数学损失，而是用另一个“判别器”神经网络来提供损失值和梯度，用于训练“生成器”神经网络。

这个想法自然招致质疑。整个系统会不会崩溃到退化输出？判别器从何而来？处处都是先有鸡还是先有蛋的困境。

但Ian对此也早有腹案。他提出**让判别器和生成器在一个零和博弈中对抗** ：

生成器试图产生与真实数据“难以区分”的输出，而判别器则要设法分辨看到的是生成样本还是真实样本。

也许这能行？第二天，实验室所有成员都收到了一封邮件。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAh9MlP6vG4rng17ccgOhvzzM3iaFmHxIuYYAGoq1BeaddIovjQEaGXBGBvrJzDPPgoCIL1XBKu8ibw/640?wx_fmt=png&from=appmsg)

在一个充斥着编程和运行实验的长夜，Ian成功让第一个生成对抗网络运行起来。

这些是**在MNIST数据集上产生的第一批样本** 。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAh9MlP6vG4rng17ccgOhvzXkaWwBibTFT6KRBZKFtEibuUgTBeWTRudwXwMhXkSRQFLcMu8vVgibw9A/640?wx_fmt=png&from=appmsg)

当时“我”正在研究类似的东西，用非神经网络判别器进行训练，但效果远不及预期。

于是“我”决定转而帮助Ian研究GAN。距离NeurIPS 2014的提交截止日期只有一周了。“我们”决定全力以赴，应该能赶上提交一篇论文。

在接下来的几天里，“我们”设置了评估方法来与现有的生成模型进行比较，尝试了不同的架构、噪声函数和博弈公式。

Jean、Yoshua和“我”发现GAN博弈是收敛的，并且在平衡状态下最小化了Jensen-Shannon散度。

“我们”坚持了下来，**在最后一周完成了所有工****作** ，并提交了一篇论文到NeurIPS。

GAN被接收为海报展示论文（posted presentation）。

“我”记得虽然大家都很兴奋，但也都知道GAN的训练动态非常不稳定。大部分合作者开始研究其它模型架构，试图解决在GAN中发现的问题。

GAN在12月份进行了展示，却基本上没有引起注意。

几个月后，2015年8月，**Alec Radford** 开始发布他一直在研究的卷积GAN的样本。

没错，就是那个几乎参与了OpenAI所有重大突破的Alec Radford。2015年，他正在研究卷积神经网络、批量归一化和GAN。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAh9MlP6vG4rng17ccgOhvzFHZ0Eb3NneYrHC6aKvUp6exxiaUHYYydDkyyQDmn51F3ibsNnmkenib0g/640?wx_fmt=png&from=appmsg)

“我”无法完全展现DCGAN之后GAN引发的巨大关注。

但“我”想强调的是，GAN的演进过程被恰如其分地用来象征AI整体的进步。

这张展示图像生成惊人发展的图片已经过时了，因为现在的图像生成模型已经能生成百万像素级的图像，甚至可以生成视频。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAh9MlP6vG4rng17ccgOhvzDCwy9z6TjncdpibicDjh0It6k0rzdiaamWmgiaa3GQibXTSPn61ZtuU7tKA/640?wx_fmt=png&from=appmsg)

至于“我”个人的故事，GAN作为“我”的第一篇学术论文既是福也是祸。一位DeepMind的研究员曾开玩笑说，“我”可能已经可以退休了，因为“我”可能已经完成了自己最伟大的工作。

但是“认为历史已经终结”可能是AI领域最大的错误。“我们“总是倾向于认为“就是这个了，这是最后的发明”。但事实从来都不是这样。

CNN曾经感觉像是最后的发明，但并不是。

GAN曾经感觉像是最后的发明，但并不是。

LSTM曾经感觉像是最后的发明，但并不是。

ResNets、DQN、AlphaGo、AlphaZero、MuZero都不是最后的答案。

回过头来看，这些想法总是显得很滑稽。但是想想现在，Transformer和大语言模型被认为是最后的发明。

但它们也不是。

“我”最近离开了前沿AI实验室的圈子，开始创办一家公司来构建一些真正令人惊叹的东西。“我”很快会分享更多相关信息。敬请关注。

感谢NeurIPS Conference授予GAN时间检验奖，也感谢这些对抗者们：Ian Goodfellow、Jean Pouget-
Abadie、Mehdi Mirza、Bing Xu、David Warde-Farley、Aaron Courville、Yoshua Bengio

也为Seq2Seq论文作者们表示祝贺。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAh9MlP6vG4rng17ccgOhvz60lhia1Q67DNa1AsPCLenOfDz9SA2t18uPU8bG3icfgzrPPIb7h5jauw/640?wx_fmt=png&from=appmsg)

## Ian Goodfellow开麦

Mehdi Mirza将这段经历分享出来后吸引到不少网友围观，网友们看得津津有味：

> 没想到论文一周就写出来了。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAh9MlP6vG4rng17ccgOhvzJbQibrZVcia2RkzndqxpfYCrvjZ6KeThgUo1SRFc7PPQx5MQ1UCwYThQ/640?wx_fmt=png&from=appmsg)

> 好一段精彩的历史回顾！在”Attention is all you need”之前，GAN才是主流。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAh9MlP6vG4rng17ccgOhvzhCnysiazqNsZvQWM2aUhPefrZXB6S4ryicFWx9WZcmOFncvLhJ1Q0gVA/640?wx_fmt=png&from=appmsg)

GAN论文一作Ian Goodfellow也激情开麦：

> 如果你是那个时代的亲历者，值得一读以怀旧；如果你不是，也能通过这些文字一窥当年的情形。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAh9MlP6vG4rng17ccgOhvzY8HRcsIgRl9q8KaLEjXTwwKZE3uXokrNj7cJ2yAs9DfTbGfEq3lR7A/640?wx_fmt=png&from=appmsg)

关于GAN论文的更多细节，可以点击这里查看：[史无前例！Seq2Seq和GAN同获NeurIPS时间检验奖，Ilya连续2年获奖](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247761961&idx=1&sn=fa05e2531ebf03ca9dd7601a938edff4&scene=21#wechat_redirect)。

参考链接：https://x.com/sherjilozair/status/1864013580624113817

— **完** —

**点这里 👇关注我，记得标星哦～**

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg)

