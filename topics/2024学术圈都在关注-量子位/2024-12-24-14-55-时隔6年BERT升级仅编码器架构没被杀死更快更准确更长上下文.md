# 时隔6年BERT升级！仅编码器架构没被杀死，更快更准确更长上下文

文章作者: 量子位
发布时间: 2024-12-24 14:55
发布地: 北京
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247768840&idx=5&sn=48de7915e78b9377c5c6935859647476&chksm=e8dc6c7adfabe56c041a22a3f0375b332947cb9a3fb1000e6acea8aebd7609df1610f67f233d#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWLqibUNGLJH5G8zRuCLs9Ptpib6DujyqymOAzzq4s2RTk6SPTeicYAX5jFg/300

##### 西风 发自 凹非寺  
量子位 | 公众号 QbitAI

时隔6年，一度被认为濒死的“BERT”杀回来了——

更现代的**ModernBERT** 问世，更快、更准、上下文更长，发布即开源！

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWLDVPFF5UZiagBnP0NlVuV9z0WXrIJB2j8UywaibZ8PUOSccR54cddZn4g/640?wx_fmt=png&from=appmsg)

去年一张“大语言模型进化树”动图在学术圈疯转，decoder-only枝繁叶茂，而曾经盛极一时的encoder-only却似乎走向没落。

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWL6LBeQhBSxLjHIy5jmUibVlkfqbYCxofk2b228a9HdOFnRaMwkCV7MHg/640?wx_fmt=gif&from=appmsg)

ModernBERT作者Jeremy Howard却说：

> **encoder-****only被低估了。**

**![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWLEQF5wyPHdOpppqRqUy40cK8MU8Afb4F1XAadVib8VS01K4dxzIcSbcw/640?wx_fmt=png&from=appmsg)**

他们最新拿出了参数分别为139M （Base）、395M（Large）的**两个模型** ，**上下文长度为8192 token**
，相较于以BERT为首的大多数编码器，其长度是它们的**16倍** 。

ModernBERT特别适用于信息检索（RAG）、分类、实体抽取等任务。

在检索、自然语言理解和代码检索测试中性能拿下SOTA：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWLJD9aEOJibtEf8TsicQlWAKT9FofvFPXfzYISd92FCuLicQxQBXq4icKp4Q/640?wx_fmt=png&from=appmsg)

效率也很高。

ModernBERT速度是DeBERTa的两倍；在更常见的输入长度混合的情况下，速度可达4倍；长上下文推理比其它模型快约3倍。

关键它所占的内存还不到DeBERTa的五分之一。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWLVwQ0XbRxp2F4AibLoXkPiaQ9ea9vkKW7WK00hnPmKUmYatkUJVOaZpyw/640?wx_fmt=png&from=appmsg)

Jeremy Howard表示，目前关于生成式模型的热议掩盖了encoder-only模型的作用。

> 像GPT-4这样大模型，太大、太慢、私有化、成本高昂，对许多任务来说并不适合，还有Llama 3.1，参数都达到了405B。
>
> 这些模型运行缓慢，价格昂贵，而且不是你可以控制的。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWLphNfZzWhYgibDh4Wxe4UruBpQ17bXZiauC9Fk7o3cwqiaa5iaXVo1O3cRA/640?wx_fmt=png&from=appmsg)

GPT-4这样的生成模型还有一个限制：它们不能预先看到后面的token，只能基于之前已生成的或已知的信息来进行预测，即只能向后看。

而像BERT这样的仅编码器模型可以同时考虑前后文信息，向前向后看都行。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWLh4jXQMqf3nPaCXjWXUhnzlKAW1dI65Iia6ia713HRLCy0G7oVgBoxyFQ/640?wx_fmt=png&from=appmsg)

ModernBERT的发布吸引数十万网友在线围观点赞。

抱抱脸联合创始人兼CEO Clem Delangue都来捧场，直呼“爱了！！”。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWLicQbuF9NWStSIMhxX2iav9JoX4iamic92JdKDAfXW2aLDe87QJRDDm8okQ/640?wx_fmt=png&from=appmsg)

为什么ModernBERT冠以“现代”之名？相较于BERT做了哪些升级？

## 杀不死的encoder-only

ModernBERT的现代体现在三个方面：

  * 现代化的Transformer架构

  * 特别关注效率

  * 现代数据规模与来源

下面逐一来看。

首先，ModernBERT深受**Transformer++** （由Mamba命名）的启发，这种架构的首次应用是在Llama2系列模型上。

ModernBERT团队用其改进后的版本替换了旧的BERT-like构建块，主要包括以下改进：

  * **用****旋转位置嵌入** （RoPE）**替换旧的位置编码** ，提升模型理解词语之间相对位置关系的表现，也有利于扩展到更长的序列长度。

  * **用GeGLU层替换旧的MLP层** ，改进了原始BERT的GeLU激活函数。

  * 通过**移除不必要的偏置项** （bias terms）**简化架构** ，由此可以更有效地使用参数预算。

  * 在嵌入层之后添加一个额外的归一化层，有助于稳定训练。

接着，在提升速度/效率方面，ModernBERT利用了Flash Attention 2进行改进，依赖于三个关键组件：

一是使用**交替注意力** （Alternating Attention），提高处理效率。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWL2JL4wriacleia3XWeG4BvIhrmQG54btEs4doGicbODLZFVmdY7lFh1XqA/640?wx_fmt=png&from=appmsg)

二是使用**Unpadding和Sequence Packing** ，减少计算浪费。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWLK9Wu6gSsY46SUpUdBJCcZ3nX4icDHzAWWxsrQflEpKwHSz0V06ktpxQ/640?wx_fmt=png&from=appmsg)

三是通过**硬件感知模型设计** （Hardware-Aware Model Design），最大化硬件利用率。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWLyo7ZmZHGMxzBbPDPk0IibD7J06IpnZX0zTNPicpRUKoLqEjXd2Hdib3jA/640?wx_fmt=png&from=appmsg)

这里就不详细展开了，感兴趣的童鞋可以自行查阅原论文。

最后来看训练和数据方面的改进。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWLce73pRZIusYEepQCk0uwGSaMuo8JaJ5LDxjFW1AoibScic4CUJL4sJtQ/640?wx_fmt=png&from=appmsg)

团队认为，encoders在训练数据方面的落后，实际问题在于**训练数据的多样性**
，即许多旧模型训练的语料库有限，通常只包括维基百科和书籍，这些数据只有单一的文本模态。

所以，ModernBERT在训练时使用了多种数据，包括网络文档、编程代码和科学文章，覆盖了2万亿token，其中大部分是独一无二的，而不是之前encoders中常见的20-40次的重复数据。

训练过程，团队坚持使用原始BERT的训练配方，并做了一些小升级，比如移除了下一句预测目标，因为有研究表明这样的设置增加了开销但没有明显的收益，还将掩码率从15%提高到30%。

具体来说，139M、395M两个规格的模型都通过了三阶段训练。

首先第一阶段，在序列长度为1024的情况下训练1.7T tokens。然后是长上下文适应阶段，模型处理的序列长度增加到8192，训练数据量为250B
tokens，同时通过降低批量大小保持每批次处理的总tokens量大致相同。最后，模型在500亿个特别采样的tokens上进行退火处理，遵循ProLong强调的长上下文扩展理想混合。

一番操作下来，模型在长上下文任务上表现具有竞争力，且处理短上下文的能力不受损。

训练过程团队还对学习率进行了特别处理。在前两个阶段，模型使用恒定学习率，而在最后的500亿tokens的退火阶段，采用了梯形学习率策略（热身-稳定-衰减）。

团队还使用两个技巧，加速模型的训练过程，一个是常见的batch-size
warmup，另一个是受微软Phi系列模型启发，利用现有的性能良好的ModernBERT-
base模型权重，通过将基础模型的权重“平铺”扩展到更大的模型，提高权重初始化的效果。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWLno6PFiaq9t7AwOtwJoia4RiafvrwQknWwgF3yMEkRlCyhP7R6WwXhicrtQ/640?wx_fmt=png&from=appmsg)

作者透露将将公开checkpoints，以支持后续研究。

## 谁打造的？

前面提到的Jeremy Howard是这项工作的作者之一。

ModernBERT的三位核心作者是：

Benjamin Warner、Antoine Chaffin、Benjamin ClaviéOn。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWLCC6eGhDemDvKWKTEAKJPHJiaCzPDiakQAdnMeoSs97YAaYxJ4YbMnauQ/640?wx_fmt=png&from=appmsg)

Jeremy Howard透露，项目最初是由Benjamin Clavié在七个月前启动的，随后Benjamin Warner、Antoine
Chaffin加入共同成为项目负责人。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWLfDFialico8QlPNCCVGibt0icTszjPJ5OiaIxE5IJLtjibIU1LRDXr6YspabA/640?wx_fmt=png&from=appmsg)

Benjamin ClaviéOn、Benjamin Warner，同Jeremy
Howard一样，来自Answer.AI。Answer.AI打造了一款能AI解题、概念阐释、记忆和复盘测试的教育应用，在北美较为流行。

Antoine Chaffin则来自LightOn，也是一家做生成式AI的公司。

团队表示BERT虽然看起来大家谈论的少了，但其实至今仍在被广泛使用：

目前在HuggingFace平台上每月下载次数超6800万。正是因为它的encoder-
only架构非常适合解决日常出现检索（例如用于RAG）、分类（例如内容审核）和实体提取任务。

Jeremy Howard表示明年将训练这个模型的更大版本。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWLgINohaoPBZTBPHyBS4ibp0uYicrarL9yXFKC7R8m19l5o6LWFgshJeDg/640?wx_fmt=png&from=appmsg)

Blog：https://huggingface.co/blog/modernbert  
ModernBERT-Base：https://huggingface.co/answerdotai/ModernBERT-base  
ModernBERT-Large：https://huggingface.co/answerdotai/ModernBERT-large  
论文：https://arxiv.org/pdf/2412.13663  
参考链接：https://x.com/jeremyphoward/status/1869786023963832509

— **完** —

**点这里 👇关注我，记得标星哦～**

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg)

