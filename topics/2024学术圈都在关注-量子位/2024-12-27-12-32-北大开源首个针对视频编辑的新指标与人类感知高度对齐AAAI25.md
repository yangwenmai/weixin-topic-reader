# 北大开源首个针对视频编辑的新指标，与人类感知高度对齐｜AAAI25

文章作者: 量子位
发布时间: 2024-12-27 12:32
发布地: 北京
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247769525&idx=4&sn=cc9b0adfa11ba45975ce122dac93b8ca&chksm=e8dc6ac7dfabe3d175f32970a440cfdfb62561d66b12bf76bad607250a58fe92358ea64faae0#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWLThiblHic2FgOZ53icwicfP3v1S6zgd6tue4fZ5cic9TjNNOkibvCZQSvPD1A/300

##### 北京大学MMCAL团队 投稿  
量子位 | 公众号 QbitAI

视频生成模型卷得热火朝天，配套的**视频评价标准** 自然也不能落后。

现在，**北京大学MMCAL团队** 开发了首个用于**视频编辑质量评估** 的新指标——**VE-Bench** ，相关代码与预训练权重均已开源。

它重点关注了AI视频编辑中最常见的一个场景：视频编辑前后**结果与原始视频之间的联系** 。

例如，在“摘掉女孩的耳环”的任务中，需要保留人物ID，源视频与编辑结果应该有着较强语义相关性，而在“把女孩换为钢铁侠”这样的任务中，语义就明显发生了改变。

此外，它的数据还更加符合人类的主观感受，是一个有效的**主观对齐量化指标** 。

实验结果显示，与FastVQA、StableVQA、DOVER、VE-Bench QA等视频质量评价方法相比，VE-Bench
QA取得了**SOTA的人类感知对齐结果** ：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWLVShMlM0jYf8ODM22skMGD4abgIIO5WrorqfSK7geQ05to0LxUric7DQ/640?wx_fmt=png&from=appmsg)

这到底是怎么做到的呢？

简单来说，VE-Bench首先从原始视频收集、提示词收集、视频编辑方法、主观标注4个方面入手，构建了一个更加丰富的数据库**VE-Bench DB** 。

此外，团队还提出了创新的测试方法**VE-Bench QA** ，将视频的整体效果分成了**文字-目标一致性** 、**参考源与目标的关系**
、**技术畸变** 和**美学标准** 多个维度进行综合评价，比当前常用的CLIP分数等客观指标、PickScore等反映人类偏好的指标都更加全面。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWLgpUSurMVt6l45F062RicBAoy7rdyJETric5lA9pIzxz8FOddmerh716w/640?wx_fmt=png&from=appmsg)

相关论文已入选AAAI 2025（The Association for the Advancement of Artificial
Intelligence）会议。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWLBgBUnEWia0DVQFX0EHOnmGYK8PR1mryDfnG3u81GLv4dHdwvLz0me5Q/640?wx_fmt=png&from=appmsg)

## 更丰富全面的数据库VE-Bench DB

### 原始视频收集

为了确保数据多样性，VE-Bench DB除了收集来自**真实世界场景** 的视频，还包括**CG渲染的内容** 以及基于文本生成的**AIGC视频** 。

数据来源包括公开数据集DAVIS、Kinetics-700、Sintel、Spring的视频，来自Sora和可灵的AIGC视频，以及来自互联网的补充视频。

来自互联网的视频包括极光、熔岩等常规数据集缺乏的场景。

所有视频都被调整为长边768像素，同时保持其原始宽高比。

由于目前主流视频编辑方法支持的长度限制，每段视频都被裁剪为32帧。

源视频的具体内容构成如下图所示，所有样本在收集时均通过**人工筛选** 以保证内容的多样性并减少冗余：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWLWQ5siaYXCTy8KkMetNic1SKiacTm6pc8WqI1mqfDt0rhz5pNn7fZWGCgw/640?wx_fmt=png&from=appmsg)

###### **△** VE-Bench原始视频构成。(a)视频来源 (b)视频类型 (c) 视频运动种类 (d) 视频内容种类

### 提示词收集

参考过往工作，VE-Bench将用于编辑的提示词分为**3** 大类别：

  * **风格编辑（Style editing）** ：包括对颜色、纹理或整体氛围的编辑。

  * **语义编辑（Semantic editing）** ：包括背景编辑和局部编辑，例如对某一对象的添加、替换或移除。

  * **结构编辑（Structural editing）** ：包括对象大小、姿态、动作等的变化。

针对每个类别，团队人工编写了相应的提示词，对应的词云与类别构成如下：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWLsmfianHibJ9n98pQmicSicIGq9px3bSJaiaqmh8sUt2AbibtfjYogHCLxWng/640?wx_fmt=png&from=appmsg)

###### **△** VE-Bench提示词构成。(a)词云 (b)提示词类型占比统计

### 编辑结果生成

VE-Bench选取了8种视频编辑方法。

这些方法包括早期的经典方法与近期较新的方法，涵盖从SD1.4～SD2.1的不同版本，包括需要微调的方法、0-shot的方法、和基于ControlNet、PnP等不同策略编辑的方法。

### 人类主观评价

在进行主观实验时，VE-Bench确保了**每个视频样本均由24位受试者进行打分** ，符合ITU标准中15人以上的人数要求。

所参与受试者均在18岁以上，学历均在本科及以上，包括商学、工学、理学、法学等不同的背景，有**独立的判断能力** 。

在实验开始前，所有人会**线下集中进行培训** ，并且会展示数据集之外的不同好坏的编辑例子。

测试时，受试者被要求根据其主观感受，并对以下几个方面进行综合评价：**文本与视频的一致性、源视频与目标视频的相关度以及编辑后视频的质量** ，分数为十分制。

最后收集得到的不同模型平均得分的箱线图如下：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWLPgSYDo7uBzKIuHswgfp8vGxhKjyGiayn61SMIgUwmlibgolBl19Itic9g/640?wx_fmt=png&from=appmsg)

###### **△** VE-Bench模型得分箱线图

其中，横坐标表示不同模型ID，纵坐标表示Z-score正则化后的MOS (Mean Opinion Score)分数。橘红色线条表示得分的中位数。

可以看出，当前的大多数文本驱动的视频编辑模型**中位数得分普遍在5分左右浮动** ，少数模型的得分中位数可以达到近6分，部分模型的得分中位数不到4分。

模型得分最低分可以下探到不到2分，也有个别样本最高可以达到近9分。

具体**每个样本在Z-score前后的得分直方图** 如下图所示，可以看出**极高分和极低分仍在少数** ：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWLfKYfz6lbn9hPom593ZsuHoepiaibn0axrgr8hTvk1CYFoXegdCwvibBRQ/640?wx_fmt=png&from=appmsg)

###### **△** VE-Bench模型得分直方图

在此基础上，团队进一步绘制了**不同视频编辑模型在VE-Bench提示词上的表现** ：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWLw9SDicIZLibCwYvJYPQzFfycy2GCfic4ud1tGB2chXdvKNIaC470XLOrA/640?wx_fmt=png&from=appmsg)

###### **△** 不同视频编辑模型在VE-Bench中不同类别的提示词上的表现

可以看出，目前的模型都相对较为擅长**风格化指令** ，这可能是利用了SD在大量不同风格图片上训练的先验成果。

同时，**删除指令相比于添加得分更低** ，因为它需要额外考虑物体或背景重建等问题，对模型语义理解与细粒度特征提取能力有更高要求。

现有模型都**还不太擅长形状编辑** 。这方面FateZero模型表现较为优秀，这可能与它针对shape-aware提出的注意力混合方法有关。

## 从3个纬度进行评估的VE-Bench QA

在构建的VE-Bench DB的基础上，团队还提出了创新的VE-Bench QA训练方法，目标是得到**与人类感知更加接近的分数** 。

下面这张图展示了VE-Bench QA的主要框架：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWLhicMANEWWUk1mwuRNNt02icz9ibQKpMwG2wk1JIVJ7vBrMdCFtfKicJPQw/640?wx_fmt=png&from=appmsg)

VE-Bench QA从**3** 个维度对文本驱动的视频编辑进行评估：

  * **文本-视频一致性**

为了衡量所编辑视频是否与文本有关，VE-Bench QA基于BLIP进行了有效的视频-文本相关性建模，通过在BLIP视觉分支的基础上加入Temporal
Adapter将其扩展到三维，并与文本分支的结果通过交叉注意力得到输出。

  * **源视频-编辑后视频动态相关性**

为了更好建模随上下文动态变化的相关性关系，VE-Bench
QA在该分支上通过时空Transformer将二者投影到高维空间，并在此基础上拼接后利用注意力机制计算二者相关性，最后通过回归计算得到相应输出。

  * **传统维度的视觉质量方面**

VE-Bench QA参考了过往自然场景视频质量评价的优秀工作DOVER，通过在美学和失真方面预训练过后的骨干网络输出相应结果。

最终各个分支的输出通过**线性层回归** 得到最终分数。

实验结果显示，VE-Bench QA在多个数据集上所预测的结果，其**与真值的相关性得分** 都领先于其他方法：

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWLEjMElyNguibA1q6c0howib7akNiaBPZqTibEiah13wruueosAMnSQe2CDSg/640?wx_fmt=png&from=appmsg)

###### **△** VE-BenchQA在T2VQA-DB数据集上的结果

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtB4L2XuuY1kF3FpY17wzSWLzWIiamaEibJqZfW4ZYgf6BibpRMS0SnP3noa1ZbXQ0DmdO1JSkiarEWA6Q/640?wx_fmt=png&from=appmsg)

###### **△** VE-Bench QA在VE-Bench DB数据集上的结果

论文链接：https://arxiv.org/abs/2408.11481  
代码链接：https://github.com/littlespray/VE-Bench

— **完** —

  

投稿请发邮件到：

**ai@qbitai.com**

标题注明【投稿】，告诉我们：

你是谁，从哪来，投稿内容‍

附上论文/项目主页链接，以及联系方式哦

我们会（尽量）及时回复你

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtC5nGy7YMGhQ0ZJeyibWyL0KVCtiaLEPMyd4Bszuo0bFIOxZOvdmqdxnOosYXyu5aI7MXpyUrUWfz6g/640?wx_fmt=gif&tp=webp&wxfrom=5&wx_lazy=1)

  

**点这里 👇关注我，记得标星哦～**

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

