# 把注意力计算丢给CPU，大模型解码吞吐量提高1.76~4.99倍

文章作者: 量子位
发布时间: 2024-12-27 12:32
发布地: 北京
原文链接: http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247769525&idx=5&sn=b836c8a2fad7ba3427b8cfe927541101&chksm=e8dc6ac7dfabe3d1ac894e9b06f9eadf9a4c92935c0157fd5a464b703640cd825c5d9f89e3ec#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtAjNaZQicLHSQr2fjY0J9bOjwficCkbciby4O2Jia8D7uDwXrMM67QyE4NP0rJSnqYqw8rXh3bqibiaeUoQ/300

##### Zhuoming Chen 投稿  
量子位 | 公众号 QbitAI

CPU+GPU，模型KV缓存压力被缓解了。

来自CMU、华盛顿大学、Meta AI的研究人员提出**MagicPIG**
，通过在CPU上使用LSH（局部敏感哈希）采样技术，有效克服了GPU内存容量限制的问题。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAjNaZQicLHSQr2fjY0J9bOjDJV1fy5tUYEvMqUBhFPiaDicjDBEqh8lgjpcDjMsCibKDhAqjsqR58hug/640?wx_fmt=png&from=appmsg)

与仅使用GPU的注意力机制相比，MagicPIG在各种情况下提高了**1.76~4.99倍**
的解码吞吐量，并在检索和推理任务中实现了更高的下游准确率，优于Quest等现有技术。

概括而言，这项研究**主要贡献** 有两点：

1、相比于其他的稀疏注意力（Sparse Attention），MagicPIG基于采样/估计而非搜索，提升了推理质量。

2、研究把解码阶段注意力模块的计算和哈希表卸载到CPU上，探索了异构计算的可能性，并且提升了吞吐量，有望降低实际模型部署成本。

下面具体来看。

## KV缓存限制了GPU高效利用

在长上下文大模型（LLM）的推理过程中，**KV缓存** （Key-Value
Cache）成为关键瓶颈。KV缓存主要用于存储中间的注意力键和值，从而避免重复计算。

然而，其显存占用随着批量大小和序列长度的线性增长而迅速增加，这严重限制了GPU的批量处理能力，导致计算资源无法被充分利用。

以**NVIDIA A100-40GB GPU**
为例，在处理Llama-3.1-8B模型且上下文长度为128k时，仅支持单个请求，且近一半的解码时间都消耗在访问KV缓存上，GPU利用率明显不足。

此外，推理过程中采用的一些策略，如多样性生成（Best-of-N）和长链式推理（Long Chain-of-
Thoughts），会进一步增加生成的Token数量，加剧显存压力，导致推理效率进一步下降。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAjNaZQicLHSQr2fjY0J9bOjsZVDrtIzsa5u45PYKvJQvBhpCH7ocibib8Wk2QRNOL3ReS8X5c7Mr0CQ/640?wx_fmt=png&from=appmsg)

## TopK Attention的问题

众所周知，注意力机制**本质上具有稀疏性** ，因此动态稀疏注意力和基于TopK的近似方法得到了广泛研究。

然而，这些方法往往伴随着显著的质量下降问题。

目前已有的KV缓存压缩技术，如Quest、H2O和Loki，主要通过筛选出KV缓存中注意力得分最高的子集来提高效率。然而，尽管这些方法在实践中表现出一定的效果，基于TopK的注意力依然是一种存在偏差的近似方法，且缺乏理论上的严格保障。

这种不足限制了其在高精度场景中的广泛应用。

下图显示，即使是精确的TopK注意力机制也会导致显著的估计误差和下游任务性能下降。

这一问题在需要高上下文利用率的复杂任务中尤为突出，例如聚合任务、常用词提取（CWE）、高频词提取（FWE）以及逻辑推理任务。在这些场景中，基于TopK近似方法的性能下降尤其严重。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAjNaZQicLHSQr2fjY0J9bOjMhmMibACxBmblb9OVV23YpuOr41n8d5ibksagh8U43rUQrCcEmTE0HRA/640?wx_fmt=png&from=appmsg)

以下几点观察揭示了**为何TopK注意力机制无法始终有效工作** 。

这些观察不仅解释了注意力机制的行为，还可能对模型训练具有重要意义：

1、首个输入token（注意力汇聚点，sink）的隐藏状态（包括但不限于键和值状态）几乎不随输入变化而改变。（见左图，
在采样的输入中，其最小相似度均高于0.99）

2、键状态的中心方向在不同输入句子中保持稳定。（见中图， 相似度均高于0.9）

3、键状态的中心与汇聚点token的键状态几乎相反。（见右图， -0.9至-0.8之间）

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAjNaZQicLHSQr2fjY0J9bOjORc7hYkclkX2AUnTHmia3ic6oPteFJJdRX89atHBEplFPibNP7hqnhgcw/640?wx_fmt=png&from=appmsg)

这些现象为理解注意力机制提供了新的视角，同时也表明传统的TopK近似方法在某些场景下可能存在局限性。

为了解决这一问题，研究提出了一种基于**采样** 而非搜索TopK键值缓存的新方法。

## 算法：基于采样的注意力估计

与仅依赖注意力分数最高的键值对相比，融入基础分布信息可以显著提高估计的准确性。

研究将这一问题视为采样中的偏差校正问题。在生物学、社会学和机器学习等领域，无偏且高效的采样技术已被广泛研究，并具有坚实的理论保障。

如图所示，基于注意力分数按比例进行采样（即所谓的Oracle
Sampling，研究把注意力模块的输出看成value向量的期望值，对应的分布是注意力得分）相比于传统的TopK选择方法，其估计误差要小得多，最多可降低4倍。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAjNaZQicLHSQr2fjY0J9bOjfaTCK5VrYEFNlFrpaAIk9bW8dVPSGSu4zDYRU0u8iao9alqaXN1q0Cg/640?wx_fmt=png&from=appmsg)

这表明采样技术在注意力近似中的潜力。

从注意力得分𝑊中采样，在实际中不可行。重要性采样（Importance
Sampling）允许从一个已知分布𝑢中抽取样本𝑖1，𝑖2，…，𝑖B，来估计未知分布𝑊的期望。

最终的输出由下式给出：  
![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAjNaZQicLHSQr2fjY0J9bOjZ8sbMQ7Y0zRdPnxglshvgeAjeDmR6J1ptnZPDY59KT40GfR4EPibNvQ/640?wx_fmt=png&from=appmsg)  
重要性采样要求𝑢和𝑊的峰值对应以降低估计方差，为此，研究使用局部敏感哈希（LSH） 来生成采样概率𝑢。

需要指出的是，因为存在Softmax（注意力得分需要归一化）, 所以研究实际上试图近似的是自归一化重要性采样。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAjNaZQicLHSQr2fjY0J9bOjVRbcoXmSpLlfGubicGdlYWxXnLBfsGhjDMBFDsVichhR3TRBkaAfckHQ/640?wx_fmt=png&from=appmsg)

## 系统：将注意力计算和哈希表放在CPU上

除了精度下降的问题外，受限的GPU显存容量也限制了现有动态KV缓存压缩方法（如Quest和Loki）在许多场景中的适用性。

与此同时，像DeepSpeed-Zero-Inference和FastDecode这样的技术展示了将KV缓存和注意力计算卸载到CPU上的潜力。

CPU的内存带宽大约是GPU显存带宽的**10%-20%** ，这引出了一个自然的问题：

> **能否在不牺牲精度的前提下，将注意力计算中的内存访问量减少10倍？**

通过利用采样算法，例如MagicPIG中基于LSH（局部敏感哈希）的采样技术进行注意力估计，研究大幅降低了内存访问量。这种方法等效地提升了CPU的内存带宽，使得在维持精度的情况下实现高效的注意力计算。

论文的系统设计扩展了以往的工作，将大语言模型（LLM）的解码分为以下四个部分：

  * **参数计算** ：包括所有线性投均在GPU上运行。

  * **注意力计算** ：涉及公式![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAjNaZQicLHSQr2fjY0J9bOjeX1FicMu6DE8QtM1K5S6Y5Wic2UWLAh2ncUVL40OGe0LjTulia25C8qcw/640?wx_fmt=png&from=appmsg)，该部分在CPU上运行。

  * **随机投影** ：在生成过程中，对于每个𝑞执行K x L次随机投影以生成哈希码。由于所有注意力头可以共享相同的随机投影器，内存开销较小（在实际实现中约为400KB）。实验中K=9或10，而L为数百，因此该步骤主要受计算限制，放置在GPU上运行。

  * **检索：** 需要在L个哈希表中查找q的哈希码。这部分计算开销非常轻量，但预构建的哈希表占用的内存较大，因此更适合放置在CPU上运行。通过上述任务分区，可以支持更大规模的K和L哈希表，而无需担心哈希码计算和哈希表存储的开销。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAjNaZQicLHSQr2fjY0J9bOj0cWDlfUgUcaRLrOXDyGnpI5zKPQVV2f6loMutqzUxaJDukLrnStS8A/640?wx_fmt=png&from=appmsg)

## 实验

研究从**准确率** 和**推理速度** 两个方面来评估MagicPIG系统的能力。

图片中的百分比为实际采样的KV cache的数量，对于MagicPIG而言，K10L150≈2%, K10L170≈2.5%。

### 长文本RULER

以Llama-3.1-8B-Instruct为例，MagicPIG在检索和推理任务中比Quest（稀疏注意力的SOTA基线）实现了更高的下游准确率。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAjNaZQicLHSQr2fjY0J9bOjicmU3l0shC0xONgR6COnAYMia1VNLTVQNaZkia5swTkHOuibKU1Bnhtmdw/640?wx_fmt=png&from=appmsg)

### 推理速度和吞吐量

在L20 + Intel 8563C上测试吞吐量，MagicPIG与仅使用GPU的注意力机制相比，在各种情况下提高了1.76~4.99倍的解码吞吐量。

![](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtAjNaZQicLHSQr2fjY0J9bOjibd78ZmLJ25EnUEmKaoMx1yl9akg0FTBicg8x6Z14pFzmaBPToibkESYg/640?wx_fmt=png&from=appmsg)

整体而言，MagicPIG是将经典的哈希算法和高维向量估计用到LLM解码上的尝试。

接下来，研究将支持更加高效的局部敏感哈希算法，并希望进一步降低LLM部署成本，探索异构计算的可能性。

论文：  
https://arxiv.org/abs/2410.16179  
项目地址：  
www.lsh-ai.com

— **完** —

  

投稿请发邮件到：

**ai@qbitai.com**

标题注明【投稿】，告诉我们：

你是谁，从哪来，投稿内容‍

附上论文/项目主页链接，以及联系方式哦

我们会（尽量）及时回复你

![](https://mmbiz.qpic.cn/mmbiz_gif/YicUhk5aAGtC5nGy7YMGhQ0ZJeyibWyL0KVCtiaLEPMyd4Bszuo0bFIOxZOvdmqdxnOosYXyu5aI7MXpyUrUWfz6g/640?wx_fmt=gif&tp=webp&wxfrom=5&wx_lazy=1)

  

**点这里 👇关注我，记得标星哦～**

**一键三连「分享」、「点赞」和「在看」**

**科技前沿进展日日相见 ~**

![](https://mmbiz.qpic.cn/mmbiz_svg/g9RQicMD01M0tYoRQT2cMQRmPS5ZDyrrfzeksiay90KaDzlGBH61icqHxmgFKfvfXtVuwTHV740CDLAaXU1LIfZyoJEpYKcRIiaE/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

