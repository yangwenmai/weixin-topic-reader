# 谷歌泄密文件：我们没有护城河，OpenAI 也没有

文章作者: AI范儿
发布时间: 2023-05-05 08:42
发布地: 上海
原文链接: http://mp.weixin.qq.com/s?__biz=Mzk0NzQzOTczOA==&mid=2247488066&idx=1&sn=bbbe1e210f81828f2730cf6a52de76e0&chksm=c3778357f4000a41d7515bf8bfc6bee257d4c6dfbacf8f620488a23a82540101ff80b1b3e085#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/OdmYSS49h7FgngKjkIg2sfzYoZ4ibdtewcyMibIsiar3MdoKhzGDWoAiasM9d1n9unrPhnYQlRwjj7mKfbNUQnucrA/300

  

>
> 以下文字是最近泄露的文件，由一位匿名个体在公共Discord服务器上分享，并授权其再次发布。该文件来自Google内部的研究员。我们已经验证了它的真实性。唯一修改的是格式和删除了指向内部网页的链接。  
>  
>
> 这份文件只代表一个Google员工的观点，而不是整个公司。我们不同意下面所写的内容，也问过其他研究人员，但我们将为订阅者单独发表对此问题的看法文章。我们只是一个传递这份引人入胜文档信息的媒介。

![](https://mmbiz.qpic.cn/mmbiz_jpg/OdmYSS49h7FgngKjkIg2sfzYoZ4ibdteweJ7DaOkdfTLNI6fRyM8NXqibI51SMR4xYHP8K0kFf6QPBvJLqJWOmJQ/640?wx_fmt=jpeg)

#

我们一直在关注OpenAI的动态。谁会跨越下一个里程碑？下一步是什么？

但令人遗憾的事实是，我们和OpenAI都没有赢得这场竞争的优势。当我们还在争吵时，第三方已经悄悄地抢了我们的饭碗。

我当然是在谈论开源。简单地说，他们正在超越我们。我们认为的“主要开放问题”今天已经被解决并落入人们手中。仅举几例：

  * 手机上的LLMs：人们在Pixel 6上以每秒5个标记运行基础模型。

  * 可扩展的个人AI：您可以在一晚上内在笔记本电脑上微调个性化AI。

  * 负责任的发布：与其说是“解决”，不如说是“避免”。整个网站充满了没有任何限制的艺术模型，而文本也不远了。

  * 多模态：当前多模态ScienceQA SOTA在一个小时内进行了训练。

虽然我们的模型在质量方面仍然稍占优势，但差距正在迅速缩小。开源模型更快、更可定制、更私密，而且性价比更高。他们用100美元和13B参数做着我们花费1000万美元和540B参数才能完成的事情。而且他们是在几周内完成的，而不是几个月。这对我们有深远的影响：

  * 我们没有秘密配方。我们最好的希望是从谷歌以外其他人所做的事情中学习并合作。我们应该优先考虑启用第三方集成。

  * 当免费、无限制的替代品在质量上相当时，人们不会为受限制的模型付费。我们应该考虑到自己真正增值之处。

  * 庞大复杂的模型正在拖慢我们前进步伐。从长远来看，最好的模型是可以快速进行迭代改进的那些模型 。现在既然知道了<20B参数范围内可能实现什么样子 ，就不应将小变体视为次要问题。

![](https://mmbiz.qpic.cn/mmbiz_png/OdmYSS49h7FgngKjkIg2sfzYoZ4ibdtew3aGwPmehDwCwFKJ1E9DDGQdMYElsAtkJe768jNibq7RtN3TRe6RkRPA/640?wx_fmt=png)

![](https://mmbiz.qpic.cn/mmbiz_png/0TdyxIUOvBMkULxkEbewWkwTAGaH8aKRVWTYUNt1yVELfqgoicibViafKOiasCVaqF3TtuHkARX7KM8sc5UCXJScGQ/640?wx_fmt=png)

发生了什么

![](https://mmbiz.qpic.cn/mmbiz_png/cibANDic3fo1nRqUaUictzUd7OQQlcG55Cc9XGs0t5qITvzjU38gwpQia2h2RL9nGovAHhtkvokSLKzsdMuZB19GAA/640?wx_fmt=png)

三月初，开源社区得到了他们的第一个真正有能力的基础模型，Meta 的 LLaMA 泄露给公众。它没有指令或对话调整，也没有
RLHF。尽管如此，社区立即意识到他们所拥有的东西的重要性。

紧随其后是巨大的创新热潮，在一系列重要的发展之间只用了几天时间（请参阅时间表以获取完整细节）。现在我们已经过去不到一个月，就出现了具备指令调整、量化、质量改进、人类评估、多模态和
RLHF 等等变体。其中许多构建在彼此之上。

最重要的是，他们解决了扩展问题，在一定程度上任何人都可以进行实验和调试。许多新想法来自普通人。培训和实验的门槛从一家大型研究机构总产出降至个人、一个晚上和一台强大笔记本电脑。

![](https://mmbiz.qpic.cn/mmbiz_png/0TdyxIUOvBMkULxkEbewWkwTAGaH8aKRVWTYUNt1yVELfqgoicibViafKOiasCVaqF3TtuHkARX7KM8sc5UCXJScGQ/640?wx_fmt=png)为什么我们本可以预见到这一点![](https://mmbiz.qpic.cn/mmbiz_png/cibANDic3fo1nRqUaUictzUd7OQQlcG55Cc9XGs0t5qITvzjU38gwpQia2h2RL9nGovAHhtkvokSLKzsdMuZB19GAA/640?wx_fmt=png)

从许多方面来看，这对任何人都不应该是一个惊喜。开源LLM的当前复兴紧随图像生成的复兴之后而来。社区没有忽视它们之间的相似性，许多人称其为LLM的“稳定扩散时刻”。

在两种情况下，低成本公共参与得以实现，原因是有了名为LoRA的大幅度降价机制，并结合规模上取得重大突破（图像合成中使用潜在扩散技术；LLMs中使用Chinchilla）。在两种情况下，获得足够高质量模型的渠道引发了全球个人和机构涌现出各种想法和迭代。在两种情况下，这很快就超过了大公司。

这些贡献对于图像生成领域至关重要，在Stable Diffusion和Dall-
E之间开辟了不同路径。拥有一个开放模型导致产品集成、市场、用户界面和创新等事物发生，并且Dall-E并未出现类似变化。

效果明显：文化影响力方面迅速占据主导地位 vs
OpenAI解决方案，后者变得越来越无关紧要。LLMs是否会发生同样的事情还有待观察，但广泛的结构元素是相同的。

![](https://mmbiz.qpic.cn/mmbiz_png/0TdyxIUOvBMkULxkEbewWkwTAGaH8aKRVWTYUNt1yVELfqgoicibViafKOiasCVaqF3TtuHkARX7KM8sc5UCXJScGQ/640?wx_fmt=png)

我们错过了什么

![](https://mmbiz.qpic.cn/mmbiz_png/cibANDic3fo1nRqUaUictzUd7OQQlcG55Cc9XGs0t5qITvzjU38gwpQia2h2RL9nGovAHhtkvokSLKzsdMuZB19GAA/640?wx_fmt=png)

开源技术最近的成功是由创新推动的，直接解决了我们仍在苦苦挣扎的问题。更加关注他们的工作可能有助于避免重复造轮子。

![](https://mmbiz.qpic.cn/mmbiz_png/0TdyxIUOvBMkULxkEbewWkwTAGaH8aKRVWTYUNt1yVELfqgoicibViafKOiasCVaqF3TtuHkARX7KM8sc5UCXJScGQ/640?wx_fmt=png)LoRA
是一种非常强大的技术，我们应该更加关注![](https://mmbiz.qpic.cn/mmbiz_png/cibANDic3fo1nRqUaUictzUd7OQQlcG55Cc9XGs0t5qITvzjU38gwpQia2h2RL9nGovAHhtkvokSLKzsdMuZB19GAA/640?wx_fmt=png)

LoRA
的工作原理是将模型更新表示为低秩分解，这可以将更新矩阵的大小减少数千倍。这使得模型微调成本和时间降至极低。能够在几小时内在消费级硬件上个性化语言模型对于涉及实时整合新知识和多样化知识方面具有重要意义。尽管它直接影响到我们最雄心勃勃的项目之一，但事实上谷歌公司内部对这项技术利用不足。

![](https://mmbiz.qpic.cn/mmbiz_png/0TdyxIUOvBMkULxkEbewWkwTAGaH8aKRVWTYUNt1yVELfqgoicibViafKOiasCVaqF3TtuHkARX7KM8sc5UCXJScGQ/640?wx_fmt=png)

从头开始重新训练模型是一条艰难的道路。

![](https://mmbiz.qpic.cn/mmbiz_png/cibANDic3fo1nRqUaUictzUd7OQQlcG55Cc9XGs0t5qITvzjU38gwpQia2h2RL9nGovAHhtkvokSLKzsdMuZB19GAA/640?wx_fmt=png)

LoRA之所以如此有效，部分原因在于它像其他形式的微调一样可堆叠。例如，指令微调等改进可以应用并作为其他贡献者添加对话、推理或工具使用时发挥作用。虽然单个微调的秩很低，但它们的总和不需要这么低，允许全秩更新累积到模型中。

这意味着当新的更好的数据集和任务变得可用时，可以廉价地使模型保持最新状态，而无需支付完整运行成本。

相比之下，从头开始训练巨大的模型不仅会丢弃预训练内容，还会丢弃已经进行过迭代改进的内容。在开源世界中，在不久之后就会有这些改进占据主导地位，并使得完全重新训练极其昂贵。

我们应该深思熟虑每个新应用或想法是否真正需要一个全新的模型。如果我们确实有重大架构改进阻止直接重复使用模型权重，则应投资于更激进形式的蒸馏技术，并尽可能保留上一代能力。

  

  
![](https://mmbiz.qpic.cn/mmbiz_png/0TdyxIUOvBMkULxkEbewWkwTAGaH8aKRVWTYUNt1yVELfqgoicibViafKOiasCVaqF3TtuHkARX7KM8sc5UCXJScGQ/640?wx_fmt=png)

长期来看，大型模型并不更具优势。

![](https://mmbiz.qpic.cn/mmbiz_png/cibANDic3fo1nRqUaUictzUd7OQQlcG55Cc9XGs0t5qITvzjU38gwpQia2h2RL9nGovAHhtkvokSLKzsdMuZB19GAA/640?wx_fmt=png)

  

对于最受欢迎的模型尺寸而言，LoRA更新非常便宜（约100美元）。这意味着几乎任何有想法的人都可以生成并分发一个。一天内的训练时间是正常的。在这种速度下，所有这些微调效果累积起来很快就能克服从较小规模开始带来的劣势。事实上，在工程师小时方面，这些模型改进的速度远远超过了我们使用最大变体所能做到的，并且最好已经基本与ChatGPT无异了。专注于维护全球一些最大规格的模型实际上会使我们处于劣势。

![](https://mmbiz.qpic.cn/mmbiz_png/0TdyxIUOvBMkULxkEbewWkwTAGaH8aKRVWTYUNt1yVELfqgoicibViafKOiasCVaqF3TtuHkARX7KM8sc5UCXJScGQ/640?wx_fmt=png)

数据质量比数据大小更重要

![](https://mmbiz.qpic.cn/mmbiz_png/cibANDic3fo1nRqUaUictzUd7OQQlcG55Cc9XGs0t5qITvzjU38gwpQia2h2RL9nGovAHhtkvokSLKzsdMuZB19GAA/640?wx_fmt=png)

许多项目通过在小而高度筛选过得数据集上进行训练节省时间。这表明数据缩放定律存在某种灵活性。此类数据集遵循《Data Doesn't Do What You
Think》中思路建立，并且正在Google之外成为标准培训方式。使用合成方法（例如从现有模型中筛选出最佳响应）和其他项目中获取信息构建了这些数据集，但这两种方法在Google中都不占主导地位。幸运的是，这些高质量数据集是开源的，因此可以免费使用。

![](https://mmbiz.qpic.cn/mmbiz_png/0TdyxIUOvBMkULxkEbewWkwTAGaH8aKRVWTYUNt1yVELfqgoicibViafKOiasCVaqF3TtuHkARX7KM8sc5UCXJScGQ/640?wx_fmt=png)直接与开源竞争是一个失败的命题![](https://mmbiz.qpic.cn/mmbiz_png/cibANDic3fo1nRqUaUictzUd7OQQlcG55Cc9XGs0t5qITvzjU38gwpQia2h2RL9nGovAHhtkvokSLKzsdMuZB19GAA/640?wx_fmt=png)

最近取得的进展对我们的业务策略有着直接、即时的影响。如果存在没有使用限制且具有高质量替代品，谁会为带有使用限制的Google产品付费呢？

而且我们不应该期望能够追赶上来。现代互联网之所以运行在开源软件上，原因很明显。开源软件具有一些重要优势，我们无法复制。

![](https://mmbiz.qpic.cn/mmbiz_png/0TdyxIUOvBMkULxkEbewWkwTAGaH8aKRVWTYUNt1yVELfqgoicibViafKOiasCVaqF3TtuHkARX7KM8sc5UCXJScGQ/640?wx_fmt=png)

我们比他们更需要他们

![](https://mmbiz.qpic.cn/mmbiz_png/cibANDic3fo1nRqUaUictzUd7OQQlcG55Cc9XGs0t5qITvzjU38gwpQia2h2RL9nGovAHhtkvokSLKzsdMuZB19GAA/640?wx_fmt=png)

保守我们的技术秘密一直是一个不稳定的提议。谷歌研究人员经常离开公司，因此我们可以假设他们知道我们所知道的一切，并且只要这条管道打开，他们将继续了解。

但是，在LLMs领域拥有竞争优势变得更加困难，因为现在尖端研究已经变得非常实惠。全球各地的研究机构正在建立彼此之间的工作关系，在广度优先方式下探索解决方案空间，远远超过了我们自己的能力范围。我们可以试图紧紧抓住自己的秘密，而外部创新则会削弱其价值；或者我们可以试着相互学习。

![](https://mmbiz.qpic.cn/mmbiz_png/0TdyxIUOvBMkULxkEbewWkwTAGaH8aKRVWTYUNt1yVELfqgoicibViafKOiasCVaqF3TtuHkARX7KM8sc5UCXJScGQ/640?wx_fmt=png)个人受到许可证限制程度没有企业那么大![](https://mmbiz.qpic.cn/mmbiz_png/cibANDic3fo1nRqUaUictzUd7OQQlcG55Cc9XGs0t5qITvzjU38gwpQia2h2RL9nGovAHhtkvokSLKzsdMuZB19GAA/640?wx_fmt=png)

很多创新都是基于Meta泄露模型权重进行的。虽然真正公开模型越来越好会改变这种情况，但重点在于它们不必等待。由于“个人使用”的法律保护和起诉个人不现实性质使得个人能够在技术处于前沿时获得这些技术。

![](https://mmbiz.qpic.cn/mmbiz_png/0TdyxIUOvBMkULxkEbewWkwTAGaH8aKRVWTYUNt1yVELfqgoicibViafKOiasCVaqF3TtuHkARX7KM8sc5UCXJScGQ/640?wx_fmt=png)成为自己的客户意味着你理解使用情况![](https://mmbiz.qpic.cn/mmbiz_png/cibANDic3fo1nRqUaUictzUd7OQQlcG55Cc9XGs0t5qITvzjU38gwpQia2h2RL9nGovAHhtkvokSLKzsdMuZB19GAA/640?wx_fmt=png)

在图像生成领域浏览人们正在创建的模型时，有大量创造力涌现，从动漫生成器到HDR风景。这些模型是由深入研究其特定子流派的人们使用和创建的，他们具有我们无法匹敌的知识深度和同理心。

![](https://mmbiz.qpic.cn/mmbiz_png/0TdyxIUOvBMkULxkEbewWkwTAGaH8aKRVWTYUNt1yVELfqgoicibViafKOiasCVaqF3TtuHkARX7KM8sc5UCXJScGQ/640?wx_fmt=png)拥有生态系统：让开源为我们服务![](https://mmbiz.qpic.cn/mmbiz_png/cibANDic3fo1nRqUaUictzUd7OQQlcG55Cc9XGs0t5qITvzjU38gwpQia2h2RL9nGovAHhtkvokSLKzsdMuZB19GAA/640?wx_fmt=png)

矛盾的是，在所有这些事件中，唯一明确的赢家是Meta。因为泄露的模型是他们自己的，他们有效地获得了整个星球价值连城的免费劳动力。由于大多数开源创新都发生在他们架构之上，没有什么能阻止他们直接将其纳入产品中。

拥有生态系统的价值不言而喻。Google本身已经成功地在其开源产品（如Chrome和Android）中使用了这种范例。通过拥有创新发生的平台，Google巩固了自己作为思想领袖和方向制定者的地位，并赢得了塑造超越自身思想叙事能力。

我们控制模型越紧密，就会使开放式替代方案更具吸引力。Google和OpenAI都倾向于采取发布模式来保持对其模型使用方式严格控制以进行防御性调整。但这种控制只是虚幻的。任何寻求将LLMs用于未经批准目的人士可以随意选择可供免费使用的模型。

谷歌应该成为开源社区领导者，并通过与广泛对话合作来带头。这可能意味着采取一些不舒适的步骤，例如发布小型ULM变体的模型权重。这必然意味着放弃对我们模型的某些控制。但这种妥协是不可避免的。我们不能希望既推动创新又控制它。

![](https://mmbiz.qpic.cn/mmbiz_png/0TdyxIUOvBMkULxkEbewWkwTAGaH8aKRVWTYUNt1yVELfqgoicibViafKOiasCVaqF3TtuHkARX7KM8sc5UCXJScGQ/640?wx_fmt=png)

结语：OpenAI怎么样？

![](https://mmbiz.qpic.cn/mmbiz_png/cibANDic3fo1nRqUaUictzUd7OQQlcG55Cc9XGs0t5qITvzjU38gwpQia2h2RL9nGovAHhtkvokSLKzsdMuZB19GAA/640?wx_fmt=png)

所有这些关于开源的讨论可能会让人感到不公平，因为OpenAI目前采取了封闭政策。如果他们不分享，我们为什么要分享呢？但事实是，在高级研究员稳定流失的形式下，我们已经在与他们分享一切。在我们阻止这种趋势之前，保密就没有意义。

最终，OpenAI并不重要。他们在相对于开源的姿态上犯了和我们一样的错误，并且他们保持优势的能力必然受到质疑。除非改变立场，否则开源替代品最终将超越它们。至少在这方面，我们可以率先行动。

![](https://mmbiz.qpic.cn/mmbiz_png/0TdyxIUOvBMkULxkEbewWkwTAGaH8aKRVWTYUNt1yVELfqgoicibViafKOiasCVaqF3TtuHkARX7KM8sc5UCXJScGQ/640?wx_fmt=png)

时间线

![](https://mmbiz.qpic.cn/mmbiz_png/cibANDic3fo1nRqUaUictzUd7OQQlcG55Cc9XGs0t5qITvzjU38gwpQia2h2RL9nGovAHhtkvokSLKzsdMuZB19GAA/640?wx_fmt=png)

### 2023年2月24日 - LLaMA发布

Meta推出LLaMA，开源代码但不包括权重。此时的LLaMA还没有进行指令或对话调整。像许多当前的模型一样，它是一个相对较小的模型（可用于7B、13B、33B和65B参数），经过了相对长时间的训练，因此相对于其大小而言非常有能力。

### 2023年3月3日 - 不可避免的事情发生了

在一周内，LLaMA泄露到公众手中。这对社区产生的影响无法过分强调。现有许可证阻止其用于商业目的，但突然间任何人都可以进行实验。从这个时候开始，创新变得更加困难和快速。

### 2023年3月12日 - 语言模型在烤面包机上运行

一周多一点后，Artem Andreenko使该模型在Raspberry
Pi上运行起来。此时该模型运行速度太慢以至于不实用，因为必须将权重从内存中换入和换出。尽管如此，这为大量精简工作铺平了道路。

### 2023年3月13日 - 在笔记本电脑上进行微调

第二天斯坦福大学发布Alpaca，为LLaMA添加了指令调整。然而，比实际权重更重要的是Eric Wang的alpaca-lora
repo，它使用低秩微调在单个RTX 4090上进行训练“只需几小时”。

突然之间，任何人都可以对模型进行微调以完成任何任务，并开始了一场低预算微调项目的竞赛。论文自豪地描述他们总共花费了几百美元。更重要的是，这些低秩更新可以轻松分发并与原始权重分开使用，使它们独立于Meta的原始许可证。任何人都可以分享和应用它们。

### 2023年3月18日 - 现在速度更快了

Georgi Gerganov使用4位量化在MacBook CPU上运行LLaMA。这是第一个“无GPU”解决方案，足够快以实用。

### 2023年3月19日 - 13B模型与Bard达到“平衡点”

次日，跨大学合作发布Vicuna，并使用GPT-4驱动的评估提供模型输出的定性比较。虽然评估方法值得怀疑，但该模型比早期变体显著改进。培训成本：300美元。

值得注意的是，他们能够绕过ChatGPT API上的限制而使用来自ShareGPT等网站发布的“令人印象深刻”的ChatGPT对话示例数据。

### 2023年3月25日 - 自己选择模型

Nomic创建了GPT4All，它既是一个模型，更重要的是一个生态系统。首次看到（包括Vicuna在内）多个模型聚集在一起。培训成本：100美元。

### 2023年3月28日 - 开源GPT-3

Cerebras（不要与我们自己的Cerebra混淆）使用Chinchilla暗示的最佳计算时间表和μ参数化暗示的最佳缩放来训练GPT-3架构。这超过了现有的GPT-3克隆产品，是μ参数化“在野外”中首次确认使用。这些模型是从头开始训练的，这意味着社区不再依赖LLaMA。

### 2023年3月28日 - 一小时内的多模态训练

LLaMA-
Adapter使用新颖的参数高效微调（PEFT）技术，引入指令微调和多模态训练。令人印象深刻的是，他们只使用了1.2M个可学习参数就实现了这一目标。该模型在多模态科学问答方面取得了新的SOTA。

### 2023年4月3日 - 真正的人类无法区分13B开放式模型和ChatGPT之间的差异

伯克利推出Koala，这是一个完全使用免费数据进行训练的对话模型。

他们采取关键步骤，在其模型与ChatGPT之间测量真实用户偏好。虽然ChatGPT仍略有优势，但超过50%
的用户要么更喜欢Koala，要么没有偏好。培训成本：100美元。

### 2023年4月15日 - ChatGPT级别下开源RLHF

Open Assistant推出一个模型，并且更重要地推出Alignment via
RLHF数据集。他们的模型在人类偏好方面接近于ChatGPT（48.3％ vs.
51.7％）。除了LLaMA外，他们还展示了该数据集可以应用于Pythia-12B中，使人们有选择地使用完全开放的堆栈来运行模型。此外，由于该数据集是公开可用的，因此对于小型实验者而言，RLHF从不可实现变得便宜且易于操作。

  

  

  

**点这里****👇 关注我，记得标星哦～‍‍‍**

  

