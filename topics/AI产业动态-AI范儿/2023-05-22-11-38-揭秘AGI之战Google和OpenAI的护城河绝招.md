# 揭秘AGI之战：Google和OpenAI的护城河绝招！

文章作者: AI范儿
发布时间: 2023-05-22 11:38
发布地: 上海
原文链接: http://mp.weixin.qq.com/s?__biz=Mzk0NzQzOTczOA==&mid=2247490908&idx=1&sn=1078808e86da8a727ad8e5a110a3341c&chksm=c3778c49f400055f0389f795861cbe3d9f64713995a8b880ef22a1c315b7081b27ee286c0f9b#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/OdmYSS49h7FuR0XhgJ5iaLA8KYXGFZgjwmJQ8HSSDkG5TBPBKf6crP6B4xeXBvCuv8x3eeP0LicW18hDTUx91k6A/300

  

图｜BingImage

文｜汤源BingImage 2023-05-20: The Google G on top of the walls of a mystical
castle with a dramatic moat

▽

![](https://mmbiz.qpic.cn/mmbiz_png/OdmYSS49h7FuR0XhgJ5iaLA8KYXGFZgjwpDTr7CqlW3SKF8FKqqUaVD5UJ4GvYDQSchbXv6wjkW6kseZNTv4qTA/640?wx_fmt=png)

  

## 题记

AI范儿公众号上一篇「[谷歌泄密文件：我们没有护城河，OpenAI
也没有](https://mp.weixin.qq.com/s?__biz=Mzk0NzQzOTczOA==&mid=2247488066&idx=1&sn=bbbe1e210f81828f2730cf6a52de76e0&scene=21#wechat_redirect)」提到泄密内容传开之后，AI社区一片哗然，文件中匿名泄密者对AI大厂的悲观弥漫开来，大家仿佛从仰望的AI大厂城墙上看到了一丝希望的缝隙，而LLM业界大众也仿佛在LLaMa和LoRa开源之后又被打了一剂强心针！

岂不知google也好、OpenAI也好，根本不在意开源LLMs能否撼动他们固守的城堡，而都是仍旧向着LLM突破后的AGI曙光狂奔......

泄密文件中主要是提到了LLM开源力量 vs Google&OpenAI ，以及google与OpenAI之间人才竞争；

**笔者认为匿名泄密者显然过多关注LLM模型本身，高估了开源技术的力量，而这一次是本质上以LLM为突破的AGI竞争，是人类智能体对等的更高维度颠覆，可以比肩工业革命的范式开端。**

> 互联网传统竞争范式
>
>
> 在老式技术公司中，一个重要的决定性因素是他们如何创建护城河，保护投资免受（潜在）竞争对手的影响。护城河通常涉及技术优势、用户、数据，最重要的是反馈循环。反馈循环使用户继续使用该技术，并为公司提供资源，使其更具优势。一些现代的例子包括Office对微软的不可取代，Facebook或谷歌的网络效应，亚马逊的规模经济，苹果的品牌和专有技术等。

模型技术本身并不是护城河，在现代大型语言模型（LLM）为突破的AGI竞争空间中，仅仅靠开源几个不大不小的LLM模型以及微调模型，确实很难复制和发展LLM真正的AGI智能属性，正如阿Q仅靠长衫，是不可能成为真正的“ZHAO家人”的。

具体到LLM本身来说，这种GPT服务的智能差异产生于2点：一方面大模型性能scaling
law对参数量级带来的增长空间远未见顶，但连巨头们当前也受到算力成本的严重限制；另一方面是获得高质量和多样化的训练提示语料以进行微调（fine-
tuning），而这需要大量的数据和用户习惯养成。

这已经完全不是在LLM社区里闲庭信步，就可以轻松攻破google和OpenAI的正不断加固的AGI城堡。

让我们来看看google和OpenAI的AGI城堡护城河（moats）到底有多宽、有多深？

## 护城河之用户维度：宽不可及

![](https://mmbiz.qpic.cn/mmbiz_png/OdmYSS49h7FuR0XhgJ5iaLA8KYXGFZgjwzFRV5FrPpXkrPdYXHibBD51xjlwEKSummopmpl8qO8cTr3G9RaJOohw/640?wx_fmt=png)

△

达成1亿用户在线服务应用历时比较

ChatGPT成为历史上达到1亿用户历时最短的在线服务，这还是只有web和api版本、且开放地区有限的情况下，达到了这一历史最快记录，某种意义上，类似开启工业时代的电灯照明普及；

随着刚刚发布的ChatGPT
App的ios版本，一众之前靠ChatGPT引流的“壳”APP基本会完成其最短的历史使命，ChatGPT用户无疑还会继续快速增长。

而背后OpenAI LP的Intelligence as a
Service（IaaS）本质上的卖token生成服务（也是按token数量计费），其每生成token的智能含量，比目前仍然免费的google
Bard服务要领先一大段身位。

同时简单折算可以看出，OpenAI通过GPT系列服务中的智能含量，为其生成token的价值带来绝对的竞争优势，而且其token直接生成成本相比于其收入，可以为OpenAI公司创造更为巨大的利润空间：

DGX A100 满负荷6kw，生成1Billion tokens需要35小时，电力成本$21； OpenAI ChatGPT 按1k
tokens收费$0.02, 1B tokens可以产生收入$60k； 从token直接生成成本看，有数千倍的毛利空间。

而相对于成年人正常阅读速度每分钟300字，合450个tokens，其token生成效率也有数十倍的空间。

而这些商业化后的利润会继续投入到GPT产品的智能提升，以及新应用场景的产品开发。有了商业化产品飞轮的良好运转，OpenAI必然会继续巩固其AGI护城河。

再来看google，支撑其主要营收的搜索广告，其在全球关键词搜索广告市场地位仍然占绝对主导地位。而短期内有GPT加持的微软bing，BingChat只是一个引流的作用，而且GPT-4服务的后台运行成本远高于关键词搜索，所以说微软在搜索用户方面应该难以改变局面以及变现困难。

至于ChatGPT未来是否替代关键词搜索，那是另外一个维度的竞争话题，不在本文关注范围。

![](https://mmbiz.qpic.cn/mmbiz_png/OdmYSS49h7FuR0XhgJ5iaLA8KYXGFZgjw4GicvJPKwBibwMLNYMNXqkfK1MjCzIiayE3VckWIiaziaBcENBlIGV8dD5w/640?wx_fmt=png)

用户数及市场占有率只是护城河的表象，背后真正形成难以跨越壁垒的是用户使用习惯和认知，也就是们常说的用户偏见与关联护城河。

在线应用一般的经验法则是，通常用户需要大约 10
倍的体验才能摆脱他们现有的习惯。即使Bing在GPT加持后的体验是谷歌搜索的两倍，也很难让大多数人改变习惯去用Bing。推动C端技术服务经济周期性转变的，并不是痴迷于科技周期新闻中的超级科技狂，而是普通用户所能体验到的最终产品价值。

也就是说如果未来有知识含量的文本生成服务可信度越来越高，越来越普及，那我们为什么还需要用关键词搜索一堆信息然后自己筛选、判断、总结出最后想要的呢？

所谓“干”死google搜索一定不是Bing搜索，可能会是用户自发形成的ChatGPT使用习惯。“与AI交谈”就用ChatGPT，OpenAI正朝着这个用户习惯培养目标一步步推进，就像“google一下”，可能在谷歌倒闭后的几十年里仍然会张口就能说出来一样。令人惊讶的是，也许今天，多少人对此还不以为然。

## 护城河之数据维度：深不可测

虽然AI开源社区一直诟病从OpenAI Inc.的开源到OpenAI
LP的闭源及商业化，但这某种意义上也说明了今天大家感受的GPT系列的成就与突破，实属不是仅靠开源力量就能达成。这其中的难度，除了需要耗费巨大的训练算力成本，更难的是训练所需的数据大小、质量，都要耗费大量的人力和时间成本。

粗略看，训练一个像ChatGPT这样的GPT服务，有以下3个主要技术里程碑：

  * 预训练一个基本的多语言多任务语言模型（甚至多模态）
  * 通过InstructGPT微调模型。开源社区正在为这一步拼凑解决方案（NLP 中的经典之作）
  * 使用 RLHF 进一步微调模型以匹配用户的价值观和兴趣。与上述微调步骤类似，对 NLP 社区来说非常新。

一般来说，开源模型有最新进展，主要是指由更好的基础模型（第 1
步）发布驱动的（如Meta的LlaMa开源），并在初始微调中加入一些创新以使模型回答问题（而不仅仅是预测next
token）；然后第2个（尤其是第3个）组件需要全新的数据集、工程基础设施和专业知识。所有这3个里程碑的达成所需成本都很大，而且开发进度上都远远落后于商业化公司。

![](https://mmbiz.qpic.cn/mmbiz_png/OdmYSS49h7FuR0XhgJ5iaLA8KYXGFZgjw64c9NxoLLBjaDTxC5fzRdvw6yEicJ5bAoibhEpMwwSjtja9cP3lmjyYQ/640?wx_fmt=png)

△

OpenAI技术报告中的显示 InstructGPT 的数据集分布（长度和类别）的表格。预计GPT-4条目的数量和长度要大得多

GPT3之后，OpenAI就不再公布具体训练的细节，以及训练数据集的情况。根据OpenAI的技术报告，GPT-4支持多模态图文，其训练数据集的组成，业界也根据多方信息做了一个推测：

![](https://mmbiz.qpic.cn/mmbiz_png/OdmYSS49h7FuR0XhgJ5iaLA8KYXGFZgjwsU1zkYvgJAnHqKD74jpOz0wbqtYBzgSFIwr4HBVc9OJ2JNKBpfMjDQ/640?wx_fmt=png)

‍△

GPT-4文本训练数据集及多模态训练数据集-图源来自陈巍谈芯

对于提升GPT任务智能评分最为关键的是第2、3步的InstructGPT，其所需数据集，从近期公布的信息看，OpenAI已经通过GPT-4来利用AI进行进一步的过滤优化、提高数据集质量；同时在ChatGPT开放后，收集了大量用户行为数据，尤其一些来自“非常高质量的用户”的prompt数据，可能会经过过滤或调整加入现有数据集，用于提升InstructGPT的性能。

以上这些数据无疑不会被开源，而开源社区所能拿到的相关数据集：Open Assistant
数据集，虽然这个数据集非常令人印象深刻——它是目前发现的指令调优性能最高的数据集。但其数据分布肯定与OpenAI的RLHF数据集在规模上不好比，在数据质量上估计也达不到闭源数据的75%。

在前几天AI范儿放出的公众号[文章](https://mp.weixin.qq.com/s?__biz=Mzk0NzQzOTczOA==&mid=2247490454&idx=1&sn=11cbcfe1625e6c43c468020e237342e0&scene=21#wechat_redirect)中，提到OpenAI的开源计划，作为谷歌的竞争对手，OpenAI
正计划向公众发布他们的第一个开源语言模型，这可能会给谷歌带来压力，要求他们也这样做。作为商业公司的OpenAI 270
亿美元的估值，取决于他们专有的人工智能模型 GPT 是否被用于商业目的，不是开源什么软件。

笔者一直关注并研究GPT现象、以及看到OpenAI最近为商业化而更好解决GPT的对齐问题的努力，包括前几天Sam
Altman在美参议院质询时的呼吁政府监管态度，**笔者判断OpenAI在开源计划中的应该是InstructGPT，而不是GPT本身。而这同样也会给google在LLMs方向的开源政策带来压力。让我们拭目以待。**

在下一代GPT-5的进展方面，虽然OpenAI CEO Sam Altman说短期内不会有更新，而首席科学家Ilya则说LLM的Scaling
Law仍然存在，数据/参数的线性增长还在，但受算力约束较大，目前easy scaling阶段红利已经没有了，接下来的性能提升进入爬坡阶段。

另据称GPT-5很有可能在2024年问世，而不是之前传说的今年年底，不过可能会有一个支持图文多模态的GPT-4.5发布。

> **GPT-4:**
>
> **大语言模型预训练体中的参数，是应该类比于人类神经元还是突触？抑或是突触连接？**
>
>
> 在大型预训练语言模型（如GPT-4）中的参数，可以类比为人类神经系统中的突触强度。这是因为模型参数在训练过程中会调整，以便在输入和输出之间建立更有效的信息传递。这类似于生物神经系统中的突触可塑性，突触的强度根据经验和学习而变化。
>
>
> 神经元是神经系统中的基本单元，负责接收和发送信号。在神经网络模型中，这些功能由节点或单元执行。突触则是神经元之间的连接，它们负责在神经元之间传递信号。在人工神经网络中，这些连接由权重表示，即我们所说的模型参数。
>
>
> 所以，当我们在类比大型预训练语言模型中的参数时，最接近的类比是突触强度。然而，这个类比并不是完美的，因为神经网络模型和生物神经系统之间仍然存在很多差异。虽然神经网络模型受到生物神经系统的启发，但它们在许多方面仍然是简化的抽象。这意味着将模型参数与具体的生物结构进行直接对应可能会导致误导性的解释。

Google方面，在结束不久的google I/O 2023上，推出了最新的人工智能语言模型PaLM 2，它是OpenAI的GPT-4的竞争对手。

PaLM 2在逻辑和推理方面的训练更强，同时可以理解100多种语言的多语言文本，从而在推理、编码和翻译等文本任务方面表现出更好的能力。

PaLM 2是一个家族产品，有四个版本可供选择，并且适用于不同的领域，包括医疗保健和网络安全。

PaLM 2已被Google用于25个产品和功能，包括谷歌的实验性聊天机器人Bard。PaLM
2的轻量版Gecko小到可以在手机上运行，这种缩小型语言模型的意义重大，因为它可以提高隐私保护。

与OpenAI是创业公司不同，Google的PaLM2发布后，可以立即运用于自己现有产品系列的服务增强，而OpenAI则需要根据当初的投资协议赋能给微软。

据称PaLM2新模型比以前的版本更小，2022年PaLM发布时在540B的参数上进行了训练，而根据内部文件PaLM
2接受了340B个参数训练，训练token也由之前的7800亿个精简到3400亿个，而训练时间据说更长，这表明google
AI在完成更复杂任务的同时，变得更加注重训练效果。

google在一篇关于 PaLM 2 的博客文章中表示，该模型使用了一种称为“计算优化缩放（compute-optimal
scaling）”的“新技术”。这使得 LLM“效率更高，整体性能更好，包括更快的推理、更少的服务参数和更低的服务成本。”

根据公开披露，PaLM 2 比任何现有模型都更强大。Facebook 的 LLM 称为 LlaMa，在今年 2 月份宣布，声称接受了 1.4
万亿Token的训练。OpenAI 上一次分享 ChatGPT 的训练规模是在 GPT-3 上，当时该公司表示它接受了 3000
亿个tokens的训练。OpenAI 在 3 月份发布了
GPT-4，并表示它在许多专业测试中表现出“人类水平的表现”，但具体训练token没有公布。而LaMDA 是谷歌两年前推出并于 2 月与 Bard
一起推出的对话 LLM，据称接受了 1.5 万亿tokens的训练。

无疑google在面对OpenAI领先一步的LLM竞争方面，正全方位追赶；不过笔者试过号称用了PaLM
2的Bard服务，在具体问题的表现上仍与GPT-4有明显差距。

## AGI巨头们之间的竞争

###

同为AGI巨头，虽然可以不屑于LLM开源社区的努力，但巨头之间，各自加固城墙，已经在发生。Google日前要求其研究人员，在研究成果应用于产品之前，不允许公开论文或开源。

至于泄密文件里提到google与OpenAI竞争态势中处于不利的位置：“但事实是，我们已经以源源不断的挖角高级研究人员的形式与 [OpenAI]
共享一切”，这一点笔者倒也认为戳中了google的软肋，士气确实是一个问题。

随着PaLM 2的发布，Bard服务的质量提升，以及整体上追赶态势取得进展，google的AI
Lab与DeepMind的整合顺利，相信人员流失问题，会有所改善。

如果这个时候，政府监管、以及因为OpenAI自身的Scaling
Law爬坡产品迭代加速势头变缓，也许google一家，独自面对“赵家”新秀OpenAI+望族微软的联手竞争，现在说AGI“赵家”领头人是谁，可能还为时尚早。

最后，巨头们的竞争不能不提meta，LLM开源社区的热闹起始于其开源的LLaMA（Large Language Model Meta
AI），转一段笔者在另一篇文章的判断：

同时在文末也评价一下马克·扎克伯格的Meta最近的一系列动作（LLaMA大语言模型开源，Segment Anything
Model发布，以及与OpenAI对齐报告同一天放出的ImageBind全模态大模型）：

meta和OpenAI的战略定位维度不一样：OpenAI就是冲着AGI去的，所关注并坚守的是AGI里的通用智能，基于AGI原生的研发路径；meta的愿景还是元宇宙，也许是元宇宙里的facebook数字孪生？所以小扎关注的还是元宇宙里的数字内容机器生成；

**不过以我个人今天的认知判断，也许未来OpenAI的通用人工智能体才会是那个 world of bits（元宇宙）的主人。**

* * *

##  参考

-NATHAN LAMBERT：Unfortunately, OpenAI and Google have moats

https://www.interconnects.ai/p/openai-google-llm-moats  
![](https://mmbiz.qpic.cn/sz_mmbiz_png/nfAe2vQ6KWmX1ADJxicuKfNIcAiaZBxnqojYDx7iaHeyAT1wbNHSeQY78FkUegKMcu0wichciaUzaqPbhia9AdeWlnvw/640?wx_fmt=png)  

END

  

  

![](https://mmbiz.qpic.cn/mmbiz_png/OdmYSS49h7FuR0XhgJ5iaLA8KYXGFZgjwvvN2n3ak5QMqc8RdaqgQdkNOdZuOSMGtqKT5Kh5UBfS0rAUbMQByTg/640?wx_fmt=png)

扫码加群，

立变AI🍚。

  

AI范儿读者群  

  

**点这里****👇 关注我，记得标星哦～**  

那些prompt了我的，

是否也prompt了你...

  

