# Karpathy后悔了：2015年就看到了语言模型的潜力，却搞了多年强化学习

文章作者: 机器之心
发布时间: 2024-11-18 12:41
发布地: 北京
原文链接: http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650943381&idx=1&sn=455a96759a87b82d21fceef811af6108&chksm=84e7ebebb39062fdd79c53aa8f23512d43d8fb3558f1e6c25980c4ebadd8ccb0aaf66acae5b7#rd

封面图链接: https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9KKQxLGwaHrOumNzVXXQtxTTtXZ048arrUaNL09uh51ArbIk4G2rItQuDGVV58me5pymbhEG1Tmw/300

机器之心报道**机器之心编辑部**

> 耽误业界好多年？

  
「这是有史以来最大、最令人困惑的研究生涯错误，」Andrej Karpathy 感叹道。  
上个周末，OpenAI 创始成员、研究科学家、原特斯拉前 AI 高级总监、AI 领域的大神 Andrej Karpathy 一直在后悔。后悔自己没有早点带领
OpenAI 开创大模型时代。  
![](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9KKQxLGwaHrOumNzVXXQtxzGYBHuGe5A3AI0FjaRbwdyYEIgUQg90t99xHVBoVyRHibM3P23OJ8Mw/640?wx_fmt=png&from=appmsg)  
是怎么一回事？看起来 Karpathy 认为当年早已认识到自回归语言模型的强大潜力，但却在很长一段时间里「误入歧途」，随大溜一起搞强化学习。  
![](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9KKQxLGwaHrOumNzVXXQtxZ0KV3we2IP18PRrJ5vtJqUHFkecDPSm4s5esH1TR2HZQeic8rgwILGQ/640?wx_fmt=png&from=appmsg)  
2013 年的 Atari RL 论文被认为是深度强化学习的开山之作：一个通用学习算法就发现了 Breakout
和许多其他游戏的最佳策略，看起来，在很多任务上我们只需要对其进行足够的改进和扩展，就可以构建出强大的 AI 模型了。  
![](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9KKQxLGwaHrOumNzVXXQtxAYhRmQlENm7E3ibFm415LqKwfgJCtbpa35ib4LgtFVRT3h3XzwpTXerw/640?wx_fmt=png&from=appmsg)  
我们也还记得，在 Karpathy 跳槽去到特斯拉一年后，2018 年 OpenAI 推出了 OpenAI Five，利用强化学习的方法在 Dota 2
游戏上开始与职业选手过招。  
在 2019 年，OpenAI
的研究者还训练神经网络，利用一只类人机械手来玩魔方，表明强化学习工具不仅仅可以处理虚拟任务，而且还能够解决需要高度灵活性的真实世界问题。  
![](https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW9KKQxLGwaHrOumNzVXXQtxQMpJIWGTd0TuaAtJZeGngcBYuXiay8X3qYBP4FiaP20iaAtznsknYxQ7g/640?wx_fmt=gif&from=appmsg)  
这个时候 OpenAI 在另一边已经推出「迄今为止最大模型」GPT-2 了，强化学习的盛世，似乎很快就被后来兴起的大语言模型（LLM）所覆盖。  
Karpathy 还提到：「Yann LeCun
当时就不太看好强化学习，他一遍又一遍地谈论『蛋糕』，而强化学习（RL）只是蛋糕顶部最后一颗樱桃，表征学习是蛋糕主体，监督学习是锦上添花。至少在今天看来，他在概念上是完全正确的（预训练
= 蛋糕主体，监督微调（SFT）= 糖衣，RLHF = 樱桃，即基本的 ChatGPT 训练 pipeline）。这很有趣，因为今天他仍然不太看好
LLM。」  
![](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9KKQxLGwaHrOumNzVXXQtxYdUPUlGPeKChfLNj8WXTib4XKpQUTPkW9zulZ4jrZZ0qHkiasiawK0ebQ/640?wx_fmt=png&from=appmsg)  
说了这么多，如今已是「事后诸葛亮」了，当初明明看好却没把握住，看起来比当初根本没想过这回事还让人懊恼。  
让我们看看 Karpathy 那篇预言了如今大模型时代的文章，说了些什么。  
**Andrej Karpathy 当初是怎么看好的**  
其实关于 RNN，Karpathy 早在 15 年就已经注意到了。为此他还专门写了一篇名为《RNN 的不合理有效性》 文章。  
![](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9KKQxLGwaHrOumNzVXXQtxPo4UItSYsPgSkXHyDyMNdaRQxC9qnWMFLJQ51AqX02PsGzEPduJLhA/640?wx_fmt=png&from=appmsg)  
文章深入探讨了循环神经网络（RNN）的潜力与实际应用。文中提到了几个实验示例，包括使用 RNN
来生成类似莎士比亚作品的文本，以及模拟编程代码和数学公式的生成。  
Karpathy 用简单易懂的语言介绍了 RNN。RNN
是一种能够处理序列数据的神经网络，它通过其循环连接能够记住之前的信息，这对于时间序列数据或任何序列数据的处理尤为关键。  
Karpathy 描述了使用 RNN 进行图像描述任务的初次尝试，并分享了这一过程中的神奇体验。他回忆称，在使用 RNN
进行训练后不久，即使是随意选择的超参数配置下，他的模型开始生成看起来非常不错的图像描述，这些描述接近于有意义。这种简单模型与所获得的结果质量之间的比例，有时会远远超出预期，这让人感到惊讶。  
**当时的普遍看法认为 RNN 难以训练，但 Karpathy 后来的经验却让他得出了相反的结论。** 随着时间的推移，Karpathy 频繁地训练
RNN，并多次见证了它们的强大和稳健，尽管如此，这些网络产生的有趣输出仍然让他感到新奇和有趣。  
关于如何利用 RNN 逐字符生成文本的介绍，引发了对「这怎么可能？」这一问题的思考。  
事实上，众所周知，RNN 是图灵完备的，因为它们可以模拟任意程序（具有适当的权重）。但与神经网络的通用近似定理类似，你不应该对此进行过多的解读。  
如果训练普通神经网络是对函数的优化，那么训练循环网络就是对程序的优化。  
接下来，Karpathy 在博客中讲解了 RNN 的基本工作原理，并通过一个具体的字符级语言模型应用来说明其实际操作过程。  
具体而言，Karpathy 为 RNN
提供一大段文本，并要求它根据前面的字符序列对序列中下一个字符的概率分布进行建模。这样，就可以一次一个字符地生成新文本。  
假设词汇表为 hello， 这段训练数据可以被拆分为 4 个独立的训练样本：  

  * 基于 h 预测 e 的概率应该较高。
  * 基于 he 预测 l 的概率应该较高。
  * 基于 hel 预测 l 的概率应该较高。
  * 基于 hell 预测 o 的概率应该较高。

  
每个字符会被编码为一个向量，采用 1-of-k 编码，即向量中只有一个位置为 1，其余位置为 0，然后使用 step 函数将它们逐个输入到
RNN。接着会观察到一个 4 维输出向量序列（每个字符一维），并将其解释为 RNN 当前分配给序列中下一个字符的置信度。  
![](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9KKQxLGwaHrOumNzVXXQtx9aHWv1o6kBcV2co54QVpU88G01OicJbDWFLU7CiaMpydm0AFyq5oXqiaQ/640?wx_fmt=png&from=appmsg)  
接下来可以看到 RNN 的训练过程及其背后的逻辑：  
在第一个 step 中，RNN 看到字符 h 后，预测下一个字符的概率分布如下：  

  * h 的置信度为 1.0
  * e 的置信度为 2.2
  * l 的置信度为 - 3.0
  * o 的置信度为 4.1

  
但根据训练数据 hello，正确的下一个字符应该是 e。因此，需要提高 e 的置信度（绿色表示），同时降低其他字符的置信度（红色表示）。  
在这过程中，每个 step 都有一个期望的目标字符。目标是让网络对正确字符的置信度更高，而对错误字符的置信度更低。因此需要反向传播算法计算每个权重的梯度。  
根据梯度调整 RNN 的权重（参数），让正确字符的置信度提高（例如 e 的置信度从 2.2 提高到 2.3）。错误字符的置信度则会相应降低。  
这一过程会重复多次，直到模型收敛。收敛后，RNN 的预测会与训练数据更加一致，即每一步都能够正确预测下一个字符。  
为了进一步说明，出于教学目的，Karpathy 还用 Python/numpy 编写了一个最小的字符级 RNN 语言模型。代码大约只有 100
行。感兴趣的读者可以参考：  

  * 项目链接：https://gist.github.com/karpathy/d4dee566867f8291f086

  
更进一步的，Karpathy 在这篇博客中还列举了 5 个其他示例展示。所有示例字符模型都是在 Github 上发布的代码进行训练的。  

  * 项目链接：https://github.com/karpathy/char-rnn

  
我们以「莎士比亚」这个示例为例。  
Karpathy 希望探索 RNN 是否能够学习并生成具有更多结构和风格的文本内容。为此，他下载了莎士比亚的所有作品，并将它们合并成一个 4.4MB
的文件，用作训练数据。  
![](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9KKQxLGwaHrOumNzVXXQtxz0bOfE4uYAMcfpM2FYGlX3rMLMrfvc4g4gXxljTGeL13vsiamRjG7zw/640?wx_fmt=png&from=appmsg)  
接着，Karpathy 使用了一个包含 3 层 RNN 的模型，每层有 512
个隐藏节点，训练这个模型耗费了数小时。最后，模型生成了一些文本样本，包括角色名字和内容对话，有时还能生成较长的独白片段。  
![](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9KKQxLGwaHrOumNzVXXQtxzB7AtVibWyAlCtXkMfQthYhsJozffztfuCIJnicdsYUSOibB12pKjqxWg/640?wx_fmt=png&from=appmsg)  
不过，从结果来看，尽管生成的文本看起来像莎士比亚的作品，但仍有一定的差异。Karpathy 认为这些生成结果表现出了模型的能力和局限性，同时也展现了 RNN
在字符级语言建模上的潜力。  
Karpathy 还列举了如何生成婴儿名字这种有趣的示例，感兴趣的读者可以参考原博客了解更多内容。  
随后的故事我们都知道了，2017 年谷歌发布了 Transformer 论文，提出了自注意力机制。在这个基础上，人们逐步探索出大模型的 Scaling
Laws，将 AI 技术向通用化快速延伸，直到今天。  
既然连 Andrej Karpathy 这样的 AI 大佬也在研究方向上「走过弯路」，我们是不是也该回看一下过去？  
 _参考链接：https://karpathy.github.io/2015/05/21/rnn-effectiveness/_  
![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW9KKQxLGwaHrOumNzVXXQtxZbfU07IVE1ciceQxSWwI6SW7EtvdBFUfiajibqOkscYjrNAKNHZaiaSmpQ/640?wx_fmt=jpeg&from=appmsg)  
© THE END 转载请联系本公众号获得授权投稿或寻求报道：liyazhou@jiqizhixin.com  
  

