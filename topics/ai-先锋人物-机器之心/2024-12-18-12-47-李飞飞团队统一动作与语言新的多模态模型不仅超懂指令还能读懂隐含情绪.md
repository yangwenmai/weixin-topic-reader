# 李飞飞团队统一动作与语言，新的多模态模型不仅超懂指令，还能读懂隐含情绪

文章作者: 机器之心
发布时间: 2024-12-18 12:47
发布地: 北京
原文链接: http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650947754&idx=2&sn=f00a447e3b92c03fee00b9da6ab0f5c5&chksm=84e784d4b3900dc2da625f7a32eb1b49adf96af8c382b85d2743a1914ad96e9897cbd65173e7#rd

封面图链接: https://mmbiz.qpic.cn/sz_mmbiz_jpg/KmXPKA19gW8WW2PYYotZypEicls6uLOEpBGMAPuib7kknbfY1GO4jtWhF99PnelQbMdgt0jZRB3GnUFjC5xLo2Dg/300

机器之心报道

**机器之心编辑部**  
人类的沟通交流充满了多模态的信息。为了与他人进行有效沟通，我们既使用言语语言，也使用身体语言，比如手势、面部表情、身体姿势和情绪表达。因此，为了理解和生成人类动作，理解这些多模态的行为至关重要，而且这一研究方向最近受到的关注也越来越多。  
而多模态语言模型看起来颇具潜力，可将多种模态的不同任务统一在一个框架下。  
近日，斯坦福大学李飞飞、Gordon Wetzstein 和 Ehsan Adeli 领导的一个团队也在这方面做出了贡献，探索了语音 - 文本 -
动作生成任务。并且他们还提出了一个全新的多模态语言模型，可以实现富有表现力的动作生成和理解。  
这个模型可以同时接受音频和文本输入来生成动作。比如你指定这个人下半身的动作是绕圈走，并根据语音生成上半身动作，它就会配合你生成对应的动作。  
更重要的是，它支持动作编辑，可以将原本的绕圈走动更换为其他动作序列（如后退、跳跃、前跑、后跑等）。更换了动作指令，模型生成的动作依然自然流畅，并与语音内容保持良好的协调性。  
  
很显然，这项研究对于李飞飞的长远「空间智能」目标大有裨益。这项研究有三位共同一作：Changan Chen（陈昌安）、Juze Zhang 和
Shrinidhi K. Lakshmikanth。  
![](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8WW2PYYotZypEicls6uLOEpMU2eOeP4DHqUxNgolgQUb99XaHZbHB9yrfd3to4k0utPjuZyDqWCiaQ/640?wx_fmt=png&from=appmsg)  

  * 论文标题：The Language of Motion: Unifying Verbal and Non-verbal Language of 3D Human Motion

  * 论文地址：https://arxiv.org/abs/2412.10523v1

  * 项目页面：https://languageofmotion.github.io/

  
**论文概览**  
首先，该团队指出，为了统一人类动作的言语和非言语语言，语言模型是至关重要的。他们给出了三点原因：  

  * 语言模型能自然地与其它模态连接起来；

  * 语音富含语义，而「建模因笑话而发出的笑声」这样的任务需要强大的语义推理能力；

  * 经过大量预训练之后，语言模型能够具备强大的语义理解能力。

  
基于这样的理解，该团队打造出了一种全新的多模态语言模型，如图 1 所示。  
![](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8WW2PYYotZypEicls6uLOEpricaslJy0xqDsTTnOqm3NlOkTWYFsoNxiaSmoZaM2XHxvicqPISFUeVBA/640?wx_fmt=png&from=appmsg)  
为了使用语言模型来建模动作，首先自然要想办法将动作变成 token。该团队的做法是针对不同的身体部位（脸、手、上身、下身）来实现动作的 token
化。事实上，之前已有研究表明，这种划分策略在建模人脸表情方面确实很有效。  
之后，再搭配上现成可用的文本和语音 token 化策略，就可以将任何模态的输入都表示成 token 了。  
为了训练这个使用多种模态的 token 的语言模型，该团队设计了一个两阶段式训练流程：  

  * 首先，进行预训练，目标是通过身体组合动作对齐与音频 - 文本对齐来对齐各种不同的模态。

  * 预训练完成后，将下游任务编译成指令，并根据这些指令训练模型，使模型能够遵循各种任务指令。

  
该团队自然也进行了实验验证，结果发现新方法得到的多模态语言模型确实比其它 SOTA
模型更优。不仅如此，他们还发现，在严重缺乏数据的情况下，这种预训练策略的优势更为明显。  
 _与其他伴语手势生成模型的效果对比_  
 _与其他文生动作模型的效果对比_  
尽管该模型在预训练期间从未见过语音 - 动作数据，但在用于数据相对较少的全新说话人时，它依然达到了颇具竞争力的性能，表现出了显著的泛化能力。  
该团队表示：「就我们所知，这是首个构建多模态语言模型来统一 3D 人体动作的言语和非语言语言的工作。」  
**用于动作生成和理解的多模态语言模型**  
模型的整体结构如下图 2 所示。  
![](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8WW2PYYotZypEicls6uLOEpRsia3lnkGPTT18yCXgBAlHEfmoz1RAA5AdlBe4vGK3uaVZnljB6vM8A/640?wx_fmt=png&from=appmsg)  
作者使用针对特定模态的 tokenizer 来处理各种输入模态。具体来说，他们训练了一个组合式的身体动作 VQ-VAE，将面部、手部、上半身和下半身的动作
token 化为离散的 token，并将这些针对特定模态的词汇表（音频和文本）合并成一个统一的多模态词汇表。  
在训练过程中，他们使用来自不同模态的混合 token 作为输入，并通过编码器 - 解码器语言模型生成输出。混合 token 被送入 transformer
编码器，而解码器则在每一步以自回归的方式预测下一个 token 的概率分布。  
**模态对齐预训练**  
现有的动作生成模型在针对下游任务训练时严重依赖成对数据。然而，收集高质量的成对动作数据既昂贵又耗时。与此同时，还有大量未配对的每种模态的数据可供探索。受此启发，作者引入了一个生成式预训练策略，如图
3 所示。具体来说，他们在预训练阶段实施了两种类型的模态对齐：组合动作对齐和音频 - 文本对齐。  
![](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8WW2PYYotZypEicls6uLOEp2R3s3Fd7MB7la9818EdaAXxHJu1wol6HDibDIrZ2S9brjibKVQ744spg/640?wx_fmt=png&from=appmsg)  
1、组合动作对齐  
我们的身体动作本质上是组合性的，即不同的身体部位是相互协调动作的。例如，当我们高兴时，我们的面部会展现出微笑，我们的手势也倾向于变得更加积极。不同身体部位动作之间的相关性是普遍的，超越了文化界限。这种共享的先验知识构成了论文所提方法的基础。为了探索这种对应关系，作者考虑了两种类型的动作对齐任务：空间和时间。  

  * 空间

  
为了建模这些不同身体部位之间的相关性，作者训练模型接收随机选择的身体部位组合（例如，上半身或上半身 +
面部）并预测另一个随机选择的其他身体部位组合（例如，下半身或下半身 +
手部）。这有助于模型学习身体部位之间的空间关系。下面是一个定义任务提示、条件和答案的示例模板。模型接收提示和条件作为输入，并按预期输出答案。  
![](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8WW2PYYotZypEicls6uLOEpIloOnCoOSEsXI5SG36ZArn1B698jIALRaOI0NwLrRpKjibS9JRnBh0Q/640?wx_fmt=png&from=appmsg)  

  * 时间

预测动作如何随时间变化也是一个重要的自监督任务，它使模型能够捕捉动作的时间演变。作者通过随机遮盖（mask）某些动作帧来建模这一点，以帮助模型学习动作的时间先验。  
![](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8WW2PYYotZypEicls6uLOEp2h0sgic33YB7XPiawnby5Cz0PZrSu5jaCC4yBIUChIbzfd0GQbdFpYSA/640?wx_fmt=png&from=appmsg)
2、音频 - 文本对齐  
除了动作模态，作者还设计了音频和文本模态之间的翻译任务，以利用大量可用的数据。这些任务遵循「从模态 X 预测模态
Y」的格式。例如，「从音频预测文本」应该通过将音频嵌入映射到预训练良好的文本嵌入空间，来帮助模型提升「从音频预测动作」方面的性能。  
**指令遵循后训练**  
预训练之后，模型获得了对动作模态词汇中潜在的语法和句法的理解，以及音频和文本模态之间的良好对齐。然后他们使用成对数据在下游任务上对模型进行微调，例如伴语手势（co-
speech
gesture）生成或文本到动作生成。为了使模型在遵循自然人类指令的同时执行所需的下游任务，作者构建了一个多任务指令跟随模板，将几个关键任务（如音频到动作、文本到动作和情感到动作）格式化为指令。  
具体来说，对于每个任务，他们编写了数十种不同的指令模板，结果产生了超过一千个不同的任务，每个任务都有其独特的指令提示。下面展示了一个指令模板示例。  
![](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8WW2PYYotZypEicls6uLOEp9XA2tgkmUAdX2bLLYrNFZibrfaibE5D7hyibOyOZkjibrpJic6AklcRb25w/640?wx_fmt=png&from=appmsg)![](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8WW2PYYotZypEicls6uLOEpqH4Cr0nA4zmgyIEnkUpu1IjgEXgIqaqlPDtm9KK9mWwHtuoudEUiaUw/640?wx_fmt=png&from=appmsg)  
**实验结果**  
**伴语手势生成**  
该团队在 BEATv2 数据集上评估模型的音频到动作生成能力。他们使用了 BEATv2 和 Librispeech 两个数据集（总共包含 1000 小时音频
- 文本数据和 60
小时动作数据）来训练模型（在预训练中，他们确保模型不会看到任何音频到动作的数据），并在特定说话者的数据上进行测试。他们通过三个指标来评估模型效果：手势的真实性（FGD）、与语音的同步性（BC）以及动作的多样性（Diversity），以全面衡量模型的表现。  
实验结果显示，该模型在所有指标上均优于现有方法。得益于预训练语言模型的语义理解能力，无需额外特征辅助即可达到良好效果。实验证明，语言预训练和多模态预训练对模型性能至关重要，移除任一环节都会导致性能显著下降。图
4 展示的定性结果表明，模型能生成与语音同步的自然手势动作。  
![](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8WW2PYYotZypEicls6uLOEp9AoWBUnMpNJA0UMovFQBUBVhOZJ3Tm8fttGa0hZGWp6jvmyDK2dHfg/640?wx_fmt=png&from=appmsg)  
**生成式预训练的效果**  
由于为说话者收集动作数据既耗时又依赖专业设备，研究团队首先验证了各个预训练任务的重要性，然后探究生成式预训练能否提升模型在新说话者上的泛化能力，从而减少所需的训练数据量。  
为此，他们分别移除了音频 - 文本对齐任务 (w/o A2T)、空间身体动作对齐任务 (w/o spatial)、时序身体动作对齐任务 (w/o
temporal) 以及整体身体对齐任务 (w/o motion)。  
表 1 展示了实验结果。(w/o A2T)
降低了模型性能，说明音频与文本嵌入空间的对齐有助于语义理解和手势生成任务。移除空间动作预测、时序动作预测或同时移除两者都会损害模型的性能。这表明在预训练阶段，学习时空动作先验对下游任务很重要。  
![](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8WW2PYYotZypEicls6uLOEpEe8TMl0Ha75avyxmZic2tJNJVp9KfpKYcM4lTRgpEjbmrymvKicL7xibw/640?wx_fmt=png&from=appmsg)  
基于这些发现，该团队假设预训练策略可以捕获强大的多模态关联和动作先验，从而减少下游任务对配对数据的依赖。  
为验证这一假设，研究团队遵循上一节中的设置，在预训练阶段限制模型可用的训练数据量。值得注意的是，在模型的预训练阶段，研究团队没有使用任何音频和对应动作的配对数据（即音频
- 动作对）来训练模型。研究团队将数据量设为 1/2^n (n∈[1...5])，并在每种设置下训练完整模型、无预训练模型和 EMAGE
基线直至收敛，并在相同测试集上评估。  
实验结果如图 5 所示。仅使用 1/32 的配对训练数据，该团队的完整模型相比无预训练模型，FGD
分数更低。随着配对微调数据量增加，性能差距虽有所减小，但完整模型始终优于无预训练模型和 EMAGE
基线。这证明了预训练的价值以及模型在极度缺乏数据时的出色泛化能力。  
![](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8WW2PYYotZypEicls6uLOEpys61SicZl6UkyzGbxJcxGQLryPM2AVCm9cycBXfA6a148lWNA20kurg/640?wx_fmt=png&from=appmsg)  
**统一音频和文本的可编辑动作生成**  
这个模型可以同时接受音频和文本输入来生成动作。首先，在 BEATv2 和 AMASS
两个动作数据集上训练动作分词器。预训练和后训练阶段分别采用统一的任务设置，后训练阶段整合了音频到动作和文本到动作的指令，文本部分使用 HumanML3D
的标注数据。  
这种双重输入的设计让模型具备了可编辑动作生成的能力。模型可以根据语音内容和文本指令生成全身动作，比如生成一个边走边说话的人物动作。研究团队还实现了分别控制不同身体部位的动作，并能将它们自然地组合在一起。这项技术对游戏和
VR 等应用有重要价值。图 6 展示了几个实际 demo，说明模型能够准确响应音频和文本的双重指令。  
![](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8WW2PYYotZypEicls6uLOEpeYJpgdMeI2UGCUYibibLmmkp7Dc4telQbNV4VYuaH32UDKN3XAkpGKyw/640?wx_fmt=png&from=appmsg)**  
****根据动作预测情绪**  
凭借灵活的输入 / 输出模态，这种新的多模态语言模型还在一些新任务上展现出了出色的能力，比如不同身体部位或模态之间的转译。  
该团队提出了一个新任务来考验这个新模型：根据动作预测情绪，也就是阅读人的肢体语言。  
为此，他们提取了 BEATv2 的情绪标签（中性、愤怒、快乐、恐惧、厌恶、悲伤、轻蔑和惊讶），并将其转换为了训练指令。结果见表 3。  
![](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW8WW2PYYotZypEicls6uLOEpjwOkmnNSw2Qp6icUqdcpZllia04xMlgXlK6zS17jDLP8Vz67EYIrGibqQ/640?wx_fmt=png&from=appmsg)  
在这项任务上，MotionGPT
完全失败，其表现与随机乱猜差不多，因为它的训练目标就只是描述一般动作，而不是细微的手势动作和肢体语言。新模型的表现远远优于随机和
MotionGPT，这表明其能够根据动作预测情绪。以下动图展示了一个示例。  
![](https://mmbiz.qpic.cn/sz_mmbiz_gif/KmXPKA19gW8WW2PYYotZypEicls6uLOEpv5jT7FGWib908FeDOxXl78OMbvUGglL7IUSF6P91zLta4ehRBVfQYtw/640?wx_fmt=gif&from=appmsg)  
© THE END 转载请联系本公众号获得授权投稿或寻求报道：liyazhou@jiqizhixin.com  

