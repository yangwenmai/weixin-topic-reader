# Yann LeCun最新万字演讲：致力于下一代AI系统，我们基本上不做LLM了

文章作者: AI寒武纪
发布时间: 2024-10-16 12:49
发布地: 江苏
原文链接: http://mp.weixin.qq.com/s?__biz=Mzg3MTkxMjYzOA==&mid=2247498104&idx=1&sn=e62a0d37925d7adffb40f2300563c52b&chksm=cef5fca9f98275bf10e03160f730210a21fcb7bbba6c5a4e358168ff0c3c02bd3e7397f39f86#rd

  

Yann LeCun最新哈德逊论坛演讲：了解我们在通往人类水平AI的旅程中所处的位置，Yann基本上对现在LLM发展方向持否定态度

Yann LeCun说，扎克伯格（Mark Zuckerberg）一直在问他需要多长时间才能达到人类水平的人工智能，而他告诉马克-
扎克伯格，即使不是十年，也要好几年的时间

人类智能有四个基本特征是目前的人工智能系统所不具备的：推理、规划、持久记忆和理解物理世界。一旦我们拥有了具备这些能力的系统，还需要一段时间才能将它们提升到人类的水平

> Yann LeCun是Facebook
> AI研究院（Fair）的首席AI科学家，也是纽约大学的教授。他获得了许多奖项，包括2018年的ACM图灵奖。他是美国人工智能协会（AAAI）的成员，也是美国国家工程院院士

我们看看Yann LeCun最新的演讲又说了什么？

Yann LeCun哈德逊论坛上的主题演讲全文

## Yann LeCun，“人类水平的AI”

我将要谈论人类水平的AI，以及我们如何到达那里，以及我们不会如何到达那里

### 为何需要人类水平的AI？

首先，我们需要人类水平的AI，因为在未来，我们大多数人都会佩戴智能眼镜或其他类型的设备，我们会与它们交谈，而这些系统将托管助手，也许不只一个，也许是一整套助手。这将导致我们每个人基本上都有一组聪明的虚拟人为我们工作。就像每个人都是老板，只是不是真正的员工。我们需要构建这个来增强人类的智力，你知道，让人们更有创造力、更高效等等。但为此，我们需要能够理解世界的机器，它们可以记住事情，拥有直觉，拥有常识，可以像人类一样推理和计划。尽管你可能从一些最热情的人那里听到过，但当前的AI系统无法做到这些

所以这就是我们需要的，能够学习建立世界模型的系统，拥有关于世界如何运作的心理模型。每个动物都有这样的模型，你的猫肯定有一个比任何AI系统都更复杂的模型。拥有持久记忆的系统，这是目前的LLM所不具备的；能够规划复杂动作序列的系统，这在今天的LLM中是不可能的；以及可控和安全的系统

我将要提出一个架构，我称之为“目标驱动AI”。我写了一篇关于此的愿景论文，大约两年前发表了，Fair的很多人都致力于实现这个计划。Fair过去常常结合长期的和更应用的项目，但是一年半以前，Meta创建了一个名为GenAI的产品部门，专注于AI产品，他们进行应用研发。所以现在Fair已经被重新定向到更长期的、下一代AI系统。我们基本上不做LLM

### 当前AI系统的局限性：自监督学习的瓶颈

所以，AI的成功，包括LLM，包括过去五六年的许多其他系统，都依赖于一套我称之为自监督学习的技术。使用自监督学习的一种方法是，自监督学习包括训练一个系统，不是为了任何特定任务，而是基本上试图以一种良好的方式表示输入。一种方法是通过从损坏中重建：假设一段文本，你通过删除单词或更改其他单词来破坏它，你知道，它可以是文本，也可以是DNA序列或蛋白质或其他任何东西，甚至在某种程度上也可以是图像，然后你训练一个巨大的神经网络来重建完整的、未损坏的版本。这是一个生成模型，因为它试图重建原始信号

这应该是可以的，但它没有投射。所以红色框就像一个成本函数，它计算输入y和重建y之间的距离，这就是学习相对于系统中的参数最小化的目标。现在，在这个过程中，系统学习了输入的内部表示，可以用于各种后续任务。它当然可以用来预测文本中的单词，这就是自回归预测所发生的事情。LLM是这种情况的特例，其中的架构被设计成，为了预测一个项目、一个标记或一个单词，它只能查看它左边的其他标记，它不能查看未来

所以如果你训练一个系统来做这个，你给它看一段文本，你让它预测文本中的下一个单词或下一个标记，然后你可以使用这个系统来预测下一个单词，然后你将下一个单词移到输入中，然后预测第二个单词，然后将它移到输入中，预测第三个单词，这就是自回归预测。这就是LLM所做的，这不是一个新概念，它自CL
Shannon以来就一直存在，可以追溯到50年代，那是很久以前了。但改变的是，现在我们有了巨大的神经网络架构，我们可以在海量的数据上进行训练，而且看起来某些属性由此而生。但是自回归预测有一些主要的局限性，这里没有通常意义上的真正的推理。还有一个限制，就是这只适用于以离散对象、符号、标记、单词、你可以离散化的东西的形式出现的数据

我们仍然缺少一些重要的东西来达到人类水平的智力。我在这里不一定指的是人类水平的智力，但即使是你的猫或你的狗也能做出令人惊叹的壮举，而这些壮举仍然完全超出了当前AI系统的能力范围。一个10岁的孩子怎么能在一次尝试中就学会清理餐桌并装满洗碗机？一个17岁的孩子可以在大约20小时的练习中学会开车。我们仍然没有5级自动驾驶汽车，我们当然也没有可以清理餐桌并装满洗碗机的家用机器人。所以我们真的缺少了一些重要的东西，否则我们就能用AI系统做到这些事情

### 莫拉维克悖论与学习的挑战

我们不断地碰到这个叫做莫拉维克悖论的东西，那就是对我们来说看起来微不足道、我们甚至不认为是智能的事情，用机器来做似乎真的非常困难。但是像高级的、复杂的、抽象的思维，比如操纵语言，对机器来说似乎很容易，或者像下国际象棋和围棋之类的事情

所以，这其中的一个原因可能是以下几点。一个LLM通常训练在20万亿个标记上，一个标记基本上是……平均来说，对于一个典型的语言，大约是四分之三个单词。所以那是1.5
x 10^13个单词。每个标记通常大约是3个字节，所以那是6 x
10^13个字节。我们任何人读完这些都需要几十万年的时间，这基本上是互联网上所有公开文本的总量

但是，考虑一下一个4岁的人类孩子，一个4岁的人类孩子总共清醒的时间是16000个小时，顺便说一下，这相当于30分钟的“油管”上传量。我们有200万条视神经纤维进入我们的大脑，每条纤维每秒钟大约携带1字节，也许是0.5字节每秒。一些估计说它是3比特每秒，这无关紧要，这是一个数量级。所以数据量大约是10^14字节，与LLM大致相同数量级。所以在4年内，一个孩子看到的视觉数据或数据与在整个互联网上公开文本上训练的最大的LLM一样多

所以这告诉你一些事情。这告诉你，首先，我们永远不可能通过仅仅训练文本就达到接近人类水平的智力，这根本不会发生。然后反驳的观点是，好吧，但是视觉信息非常冗余。所以首先，这个每条视神经纤维每秒1字节的数据已经比你视网膜中的光感受器压缩了100倍。我们的视网膜中有大约6000万到1亿个光感受器，然后使用你视网膜前面的神经元压缩到100万条神经纤维。所以已经有100:1的压缩比。然后它到达大脑，然后它被扩展了50倍左右。所以我测量的是压缩信息，但它仍然非常冗余。而冗余实际上是自监督学习所需要的。自监督学习只有从冗余数据中学习到一些有用的东西，如果数据是高度压缩的，这意味着它是完全随机的，你什么也学不到。你需要冗余才能学习任何东西，你需要学习数据的底层结构

### 迈向更强大的AI：超越像素级预测

所以我们将不得不训练系统通过观看视频或生活在现实世界中来学习常识和物理直觉。我将要稍微打乱一下顺序，然后告诉你一些关于这个目标驱动AI架构的真实情况。它与LLM或前馈神经网络有很大的不同，因为推理过程不仅仅是运行神经网络的几层，而是实际上运行一个优化算法。从概念上讲，它看起来像这样

前馈过程是一个过程，在这个过程中，你看到一个观察结果，运行通过系统感知系统，例如，神经网络的几层，并产生一个输出。对于任何单个输入，你只能有一个输出。但是有很多情况下，对于一个感知，有多个可能的输出解释，你希望有一个过程，它不仅仅计算一个函数，而是计算一个映射，对于单个输入可能有多个输出。你唯一能做到这一点的方法是通过隐式函数，基本上是一个像这样的目标，右边的红色框，它基本上测量输入和建议输出之间的兼容性，然后通过找到与输入最兼容的一个输出值来计算输出。你可以通过想象这个目标是某种能量函数，并且你正在相对于输出最小化这个能量来做到这一点。你可能有多个解决方案，你可能有一些方法来遍历这些多个解决方案。人类的感知系统就是这样做的，如果你对一个特定的感知有多个解释，你的大脑会自发地循环遍历这些解释。有一些证据表明这种类型的事情可以发生。

但是让我回到架构。使用这种通过优化进行推理的原则，人们思考方式的假设是这样的：你在世界上进行观察，一个感知系统让你了解世界的状态，世界的当前状态。但是当然，它只让你了解你目前可以感知到的世界状态，你可能从记忆中对世界其他状态有一些了解。所以这可能会与记忆的内容相结合，并将其馈送到一个世界模型。什么是世界模型？世界模型是你关于世界如何运作的心理模型。

所以你可以想象你可能会采取的一系列动作，你的世界模型将允许你预测这一系列动作对世界的影响。所以绿色的框，世界模型，你给它一个假设的动作序列，它预测世界的最终状态是什么，或者预测世界中将要发生的事情的整个轨迹。你将它馈送到一堆目标函数，一个目标函数测量目标实现的程度，任务完成的程度，也许还有一组其他目标是护栏，这些目标基本上测量所遵循的轨迹或已采取的行动或对机器人或机器周围的人不危险的事情的程度等等。

所以现在的推理过程，我还没有谈到学习，它只是推理，包括找到使这些目标最小化的动作序列，找到使这些目标最小化的动作序列。这就是推理过程，所以它不仅仅是前馈。你可以通过搜索离散选项来做到这一点，但这效率低下。一个更好的方法是确保所有这些框都是可微的，你通过它们反向传播梯度，并使用梯度下降来更新动作序列。

现在，这个想法一点也不新，它已经有60多年了，如果不是更久的话。它是基于……好的，所以首先，让我谈谈使用世界模型进行这种推理的好处。好处是你可以基本上完成新的任务而不需要任何学习。我们一直在这样做。我们面临一个新的情况，我们思考它，我们想象我们行动的后果，我们采取将实现我们目标的行动序列，无论它是什么。我们不需要学习来完成这项任务，我们可以计划。所以这基本上就是计划。你也可以将大多数形式的推理简化为优化。

所以这种通过优化进行推理的过程本质上比仅仅运行神经网络中的几层更强大。现在，这种通过优化进行推理的想法，正如我所说的，在最优控制理论领域已经存在了60多年，它被称为模型预测控制。你有一个你试图控制的系统的模型，比如说火箭或其他什么东西，或者一架飞机，或者一个机器人，你可以想象，你可以使用你的世界模型计算一系列控制命令的影响，然后你优化这个序列，使运动按照你想要的方式进行。所有经典的机器人运动规划都是这样做的。这不是一件新事物。这里的新事物是我们正在学习世界模型，我们正在学习将提取世界情况的适当抽象表示的感知系统。

现在，在我进入如何运行它的示例之前，你可以构建一个包含所有这些组件的整体AI系统：世界模型，可以根据手前任务配置的成本函数，执行器，它是真正优化的模块，根据世界模型找到最佳动作序列，短期记忆，感知系统等等。

那么它是如何工作的呢？所以你……如果你的动作不是单个动作，而是一个动作序列，你的世界模型实际上是一个系统，它告诉你，给定时间t的世界状态和我可以采取的动作，预测时间t+1的世界状态。你想预测在这种情况下两个动作的序列将产生什么结果，你可以多次运行你的世界模型。所以这里它表示为时间展开。获取初始世界状态表示，输入动作0的假设，使用世界模型预测世界的下一个状态，然后是动作1，下一个世界的下一个状态，计算成本，然后通过反向传播和基于梯度的优化方法，找出将最小化成本的两个动作。这就是模型预测控制。

由于世界通常不是完全确定的，你可能需要使用潜在变量来馈送到你的世界模型。所以潜在变量基本上是可以在集合上滑动或从分布中抽取的变量，它们代表……它们基本上导致世界模型遍历与观察结果兼容的多个预测。世界并非完全可预测，因此在进行预测时，你可能需要处理这种类型的不确定性。

更有趣的是做人类似乎能够做的事情，当然也包括一些动物，那就是分层规划。如果你正在计划一次从纽约到巴黎的旅行，你可以使用你的世界模型，你身体的模型，也许还有你从这里到巴黎的整个世界配置的想法，根据你的低级肌肉控制来规划你的整个旅行。但当然，没有人会这样做。你做不到，你甚至没有信息来做，而且这有点疯狂。在你到达巴黎之前，你必须做的每10毫秒的肌肉控制的步数简直是太疯狂了。所以你所做的是分层规划。你在一个非常高的层次上进行规划，你说，好吧，要去巴黎，我首先需要去机场并乘坐飞机。我如何去机场？假设我在纽约市，我必须走到街上并叫一辆出租车。我如何走到街上？好吧，我必须从椅子上站起来，走到门口，打开门，走到电梯，按下按钮，等等。我如何从椅子上站起来？在某些时候，你可以用低级肌肉控制动作来表达事情，但我们不是用低级来规划整个事情，我们正在进行分层规划。如何用AI系统做到这一点是完全未解决的，我们不知道如何做到这一点。这似乎是智能行为的一个相当大的要求。

那么，我们将如何学习具有层次结构、在几个不同抽象层次上工作的世界模型？没有人展示过任何接近这一点的东西。这是一个巨大的挑战。是的，这只是我刚才所说的例子的图形表示。

那么，我们将如何训练这个世界模型呢？因为这真的是一个巨大的挑战。你看看婴儿，这对动物也是如此，心理学家和认知科学家试图弄清楚婴儿在什么年龄学习关于世界的基本概念，比如他们如何学习直觉物理学，物理直觉，所有这些东西。这发生在他们开始学习语言和互动之类的很久以前。

所以像面部追踪这样的事情发生得非常早，生物运动，有生命和无生命物体之间存在差异的事实，这也发生得非常早。物体永久性发生得非常早，当一个物体被另一个物体隐藏时，它仍然存在的事实。然后，你知道，婴儿学习自然种类，你不需要给他们东西的名字，他们会知道椅子、桌子和猫是不同的。稳定性和支撑，但是像重力、惯性、动量守恒这样的东西，实际上只出现在9个月左右，这需要很长时间。所以如果你给一个6个月大的婴儿看左边的场景，一辆小车在一个平台上，你把它推下平台，它似乎漂浮在空中。6个月大的婴儿几乎不会注意。一个10个月大的婴儿会像那个小女孩一样，她明白这不应该发生，物体应该掉下来。当发生一些令人惊讶的事情时，这意味着你的世界模型是错误的，所以你要注意，因为它可能会杀死你。

所以这里需要发生的学习类型与我们之前讨论的学习类型非常相似。取一个输入，以某种方式破坏它，并训练一个大型神经网络来预测缺失的部分。如果你训练一个系统来预测视频中将要发生的事情，就像我们训练神经网络来预测文本中将要发生的事情一样，也许这些系统将能够学习常识。

坏消息是，我们已经尝试了10年，这是一个彻底的失败。我们从来没有能够接近任何真正学习任何关于世界的一般知识的系统，仅仅通过试图预测视频中的像素。你可以训练一个系统来预测看起来不错的视频，现在有很多视频生成系统的例子，但在内部，它们并不是物理世界的良好模型，它们不能用于此。这个想法，我们将使用生成模型来预测视频中将要发生的事情，系统将神奇地理解世界的结构，彻底失败，我们在10年里尝试了很多东西。

失败的原因是因为有很多可能的未来，在像文本这样的离散空间中，你无法预测哪个单词将跟随一个单词序列，但你可以生成字典中所有可能单词的概率分布。但如果是视频，视频帧，我们没有很好的方法来表示视频帧上的概率分布。事实上，我的意思是，这项任务是完全不可能的。就像，如果我拍下这个房间的视频，我拿一个相机，我拍下那部分，然后我停止视频，我让系统预测视频中的下一个是什么，它可能会预测在某个时候会有房间的其余部分，会有墙，会有人坐着，密度可能与左边相似，但它不可能在像素级别上准确地预测你们所有人的样子，世界的纹理是什么样子，房间的精确大小，以及所有类似的事情。你不可能准确地预测所有这些细节

### 联合嵌入预测架构 (JEPA)：一种新的希望

所以解决这个问题的方法就是我所说的联合嵌入预测架构，其想法是放弃预测像素。与其预测像素，不如学习一个表示，一个关于世界中发生的事情的抽象表示，然后在该表示空间中进行预测。所以这就是架构，联合嵌入预测架构。这两个嵌入采用x，损坏的版本，运行到一个编码器；采用y，运行到一个编码器，然后训练系统从x的表示预测y的表示。

现在的问题是你如何做到这一点，因为如果你只是使用梯度下降、反向传播来训练这样的系统，以最小化预测误差，它将会崩溃。它会说，它将学习一个常数的表示，现在预测变得超级容易，但它没有信息量。所以，但这就是我希望你们记住的区别，生成架构试图重建预测器、自动编码器、生成架构、自动编码器等等之间的区别，然后是联合嵌入架构，你在表示空间中进行预测。我认为未来在于这些联合嵌入架构。我们有大量的经验证据表明，要学习图像的良好表示，最好的方法是使用这些联合嵌入架构。所有尝试使用重建来学习图像表示的尝试都很糟糕，它们的效果并不好。在这个方面有巨大的项目，并声称它们有效，但它们实际上并没有。最好的性能是通过右边的架构获得的。

现在，如果你仔细想想，这实际上就是我们用智力所做的，找到一个事物或现象的良好表示，以便你可以进行预测。这确实是科学的本质，真的。就像，想想这样一个事实，如果你想预测行星的轨迹，行星是一个非常非常复杂的物体，它非常巨大，它有天气和温度，你知道，密度和所有你可以测量到的关于行星的事情，也许是一个极其复杂的物体，但要预测行星的轨迹，你只需要知道6个数字，3个位置和3个速度，仅此而已，你不需要知道任何其他事情。所以这是一个非常重要的例子，它真正证明了这样一个事实：预测能力的本质实际上是为我们观察到的事物找到良好的表示

那么我们如何训练这些东西呢？所以这是一个……我们如何训练这些东西？所以你想防止这个系统崩溃。一种方法是有一些成本函数来测量来自编码器的表示的信息内容，并尝试最大化信息内容或最小化负信息，这就是这里写的内容。所以你训练系统同时从输入中提取尽可能多的信息，但同时最小化该表示空间中的预测误差。因此，系统将在提取尽可能多的信息与不提取不可预测的信息之间找到某种平衡。你将得到一个良好的表示空间，在这个空间中你可以进行预测，你可以进行预测

现在，你如何测量信息？这才是事情变得有点奇怪的地方。好的，我将跳过这一点。嗯，有一种方法可以根据训练基于能量的模型和能量函数从数学上理解这一点，但我没有时间深入讨论这个问题。但基本上，我在这里告诉你一些不同的事情：

>
> 放弃生成模型，转而使用这些JEPA架构；放弃概率模型，转而使用这些基于能量的模型；放弃对比方法，我没有谈论这个，因为我马上就会谈到这个；还有强化学习，但我已经说了10年了。而这些都是当今机器学习最流行的四大支柱。所以我现在不是很受欢迎

好的，所以……一种方法是对来自编码器的信息内容进行一些估计，目前有六种方法可以做到这一点。实际上这里少了一种叫做VICReg的方法，来自我在纽约大学和Flatiron的同事。所以这里的一个想法是防止系统崩溃并产生常数。取编码器输出的变量，并确保这些变量具有非零标准偏差。你可以把它放在一批样本的成本函数中，确保权重是这样的，变量不会崩溃并变成常数，这很容易做到。现在的问题是，系统可以作弊，使所有变量相等或高度依赖或相关。所以你必须做的是添加另一个术语，它说我想最小化这些变量的协方差矩阵的非对角线项，以确保它们不相关。当然，这还不够，因为变量仍然可以是依赖的，你知道，依赖但不相关。所以我们使用了另一个技巧，就是将sx的维度扩展到更高维的空间vx，然后在这个空间中应用这种方差-
协方差正则化，这似乎就足够了

但是有一个技巧，就像我愚弄了你或你们中的一些人，因为我在这里最大化的是信息内容的上限，我祈祷实际的信息内容会跟随我对上限的最大化。我需要的是一个下限，这样会推高下限，信息就会上升。不幸的是，我们没有信息的下限，或者至少我们不知道如何计算它，如果我们有的话。

还有第二套方法，它被称为蒸馏式方法，这种方法以神秘的方式工作。如果你真的想要一个清晰的解释它为什么有效，你应该问Sylvain
Ghouli，他就坐在那里，他有一篇关于这个的论文。就我个人而言，我不明白，但它确实有效。它包括只更新这个架构的一半，而不是在另一半上反向传播梯度，然后以一种有趣的方式共享权重。有很多关于这个的论文，如果你想训练一个完全自监督的系统来学习图像的良好表示，这和任何其他方法一样好。图像的损坏是通过掩码来实现的。

我们有一些更近期的工作，我们在视频上做这个，所以我们可以训练一个系统来基本上提取视频的良好表示，我们可以将其用于下游任务，比如动作识别、视频等等。它包括拍摄一段视频，掩盖其中的一大块，运行它，并在表示空间中进行预测，并使用这种蒸馏技巧来防止崩溃。这非常有效

所以在未来，我们所有的互动，如果我们在这个项目中取得成功，并最终得到能够推理、能够计划、能够理解物理世界的系统，这将需要数年时间，直到我们让这里的一切都运作起来，如果不是十年的话。马克·扎克伯格一直问我需要多长时间。所以如果我们成功地做到了这一点，我们将拥有真正能够调节我们与数字世界所有互动的系统，它们可以……它们将回答我们所有的问题，它们将一直与我们同在，它们将基本上构成所有人类知识的宝库。这感觉像是一种基础设施，就像互联网一样，它不像一个产品，更像一个基础设施

### 开源 AI：构建开放的未来

这个AI平台必须是开源的。我不需要说服这里任何来自IBM的人，因为IBM和Meta都是一个叫做AI联盟的组织的一部分，该组织推广开源AI平台。我真的很感谢Dario领导这件事，以及IBM的所有人。所以我们需要这些平台是开源的，因为我们需要这些AI助手是多样化的，我们需要它们理解世界上所有的语言、所有的文化、所有的价值体系。你不会从美国西海岸或东海岸的一家公司生产的单一助手中获得这些。你知道，这将需要来自全世界的贡献

当然，训练基础模型非常昂贵，所以只有少数公司可以做到这一点。所以如果像Meta这样的公司可以开源提供这些基础模型，那么全世界都可以根据自己的目的对其进行微调。所以这就是Meta和IBM所采用的理念。所以开源AI不仅仅是一个好主意，它对于文化多样性，甚至可能是维护民主来说是必要的

所以……训练和微调将是众包的，或者是由创业公司和其他公司的生态系统完成的。这真的是启动了AI创业公司生态系统的原因，是这些开源AI模型的可用性。达到人类水平的AI需要多长时间？我不知道，可能是几年到几十年。存在巨大的差异，有很多问题需要解决，而且几乎可以肯定比我们想象的要难。它不会在一天内发生，它会像……渐进的进化。所以这不像，有一天我们会发现AI的秘密，然后我们会打开一台机器，然后我们立即拥有了超级智能，然后我们所有人都会被超级智能系统杀死。不，不会这样发生。机器将超越人类的智力，但它们将处于控制之下，因为它们将是目标驱动的，我们给它们目标，它们实现这些目标。就像我们这里的许多人都是行业或学术界或其他领域的领导者，我们与比我们聪明的人一起工作，我当然也是。有很多与我一起工作的人比我聪明，但这并不意味着他们想要支配或接管

所以这就是故事。存在风险，但我将把它留到问答环节。非常感谢

  

  

**⭐星标AI寒武纪，好内容不错过**⭐****

**用你的****赞****和****在看****告诉我～**

  

  

  

👇👇

