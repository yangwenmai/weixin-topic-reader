# Ilya Sutskever 离职后首次采访：个人经历，Scaling Laws，AI幻觉

文章作者: AI寒武纪
发布时间: 2024-07-20 12:18
发布地: 江苏
原文链接: http://mp.weixin.qq.com/s?__biz=Mzg3MTkxMjYzOA==&mid=2247494996&idx=1&sn=182f336ae4335a51332a5cd6f6ba2c29&chksm=cef5e885f9826193d3e5fd986fc8bfcbc8d388611d656942c014907f929fa53a63508451b73c#rd

封面图链接: https://mmbiz.qpic.cn/sz_mmbiz_jpg/ICgnAptln0XObLakyuJWQDqHIPXvZVqBKbF81hfOR99G3vtXLxd0KZ29fzIibTSchBI1RDlCic3C05UiaWosLApAg/300

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ICgnAptln0XObLakyuJWQDqHIPXvZVqBFal5lcjSjUXk72fzyEUIibamI9WaSP9486mWoeInY3dic57MIhNibYzYQ/640?wx_fmt=jpeg&from=appmsg)  

  

Ilya Sutskever 自从 OpenAI 赶走 Sam Altman 后首次接受播客采访，他提到了 AI 和深度学习的过去、现在和未来。他回顾了他从
17 岁起与 Jeff Hinton 的合作，如何通过卷积神经网络取得突破，以及 GPT 项目如何从自我监督学习和 Transformer
模型中获益。他还探讨了大规模语言模型的局限性和可能的解决方案，尤其是关于模型“幻觉”的问题

  

![](https://mmbiz.qpic.cn/sz_mmbiz_png/ICgnAptln0XObLakyuJWQDqHIPXvZVqB3aVAdWrptbOQLq4lB33nPJmakGfvMUlk35jQjliceHozdVq17dpM24A/640?wx_fmt=png&from=appmsg)

  

  

以下是中文全文整理：

**成长经历与早期兴趣**

**Ilya**
：是的，我可以谈谈这个问题。确实，我出生在俄罗斯，在以色列长大，然后在青少年时期我的家人移民到加拿大。我的父母说我从很小的时候就对AI（人工智能）感兴趣，我也对意识（Consciousness）非常感兴趣，这让我感到困扰，我很好奇能够帮助我更好理解它的东西，AI
似乎是一个很好的角度。所以我认为这是一些让我开始的方式。我在17岁时就开始与Jeff
Hinton合作，因为我们搬到了加拿大，我立即能够加入多伦多大学。我非常想做机器学习，因为那似乎是人工智能最重要的方面，但当时完全无法接触。为了给大家一些背景知识，时间是2003年。今天我们理所当然地认为计算机可以学习，但在2003年我们理所当然地认为计算机无法学习。当时AI的最大成就是Deep
Blue下棋引擎，但那只是一个游戏，你有一个搜索树，有一种简单的方法来确定一个位置是否比另一个好，这真的不像可能适用于现实世界，因为没有学习，学习是一个很大的谜。所以我真的对学习非常感兴趣，非常幸运的是，Jeff
Hinton是我所在大学的教授，所以我能够找到他，我们几乎立即开始合作。

**对大脑与机器学习的探索**

**主持人** ：你的冲动像Jeff一样是想了解大脑的工作原理吗？还是你只是对机器学习的想法感兴趣？

**Ilya** ：AI非常大，动机也同样多样。有趣的是，现在我们有了相当的了解，比如它是一个大的神经网络（neural
network），我们知道它在某种程度上是如何工作的，但那时所有的神经网络都存在，没有人知道神经网络有什么用。所以，智能究竟是如何工作的？我们怎样才能让计算机变得稍微智能一点？我有一个非常明确的意图，即为AI做出一个很小但真实的贡献，因为有很多对AI的贡献并不是真实的，我可以出于各种原因看出它们不是真的，不会有任何结果。我只觉得AI是一个无望的领域。所以动机是我能否理解智能是如何工作的，并且也为它做出贡献。这是我早期的动机，那是2003年，差不多正好是20年前。然后Alex和我谈过Jeff，他说你对卷积神经网络（CNN）突破的兴奋使你申请了图像竞赛，Alex有训练网络的编码技能。你能谈谈这个吗？我不想陷入历史，但这很有趣。  

**卷积神经网络与ImageNet的突破**

**Ilya**
：简而言之，我意识到如果你在一个大的神经网络上训练一个足够大的数据集，这个数据集指定了一些人们执行的复杂任务，比如视觉（Vision），还有其他的，然后你只需训练那个神经网络，你就一定会成功。逻辑是不可简化的，我们知道人类大脑可以快速解决这些任务，而人类大脑只是一个有慢速神经元的神经网络。所以我们知道某个神经网络可以很好地完成这些任务。所以我们只需要找到一个较小但相关的神经网络并在数据上训练它，计算机内部最好的神经网络将与我们执行这项任务的神经网络相关。所以这是一个关于大的和深的神经网络可以解决任务的论点，并且我们有工具来训练它们，这是在Jeff实验室完成的技术工作的结果。所以你结合两者，我们可以训练这些神经网络，它需要足够大，所以如果你训练它，它会工作得很好，你需要数据来指定解决方案，ImageNet具备所有的条件。Alex有这些非常快速的卷积核，ImageNet有足够大的数据，有真正的机会做一些前所未有的事情，并且完全成功了。

**从监督学习到Transformers的演变**

**Ilya** ：是的，那是监督学习和卷积神经网络。2017年，“Attention is All You
Need”论文问世，介绍了自注意力机制和Transformers。

**主持人** ：GPT项目是什么时候开始的？是否有关于Transformers的直觉？能谈谈这个吗？

**Ilya**
：作为背景，在OpenAI的最早期，我们就在探索预测下一个东西是否是你所需要的全部。我们用当时限制更多的神经网络来探索它，但希望是如果你有一个神经网络可以预测下一个单词、下一个像素，实际上这是关于压缩的，预测就是压缩，预测下一个单词并不是，嗯，让我想想最佳的解释方式，因为有很多事情在进行，它们都是相关的，也许我会换一个方向。我们确实对尝试理解预测下一个单词能走多远以及是否会解决无监督学习非常感兴趣。回到GPT之前，无监督学习被认为是机器学习的圣杯。现在它已经完全解决了，没有人再谈论它，但它曾是一个神秘的圣杯。所以我们在探索这个想法，我对它非常兴奋，认为如果能很好地预测下一个单词，那将会给你无监督学习，如果了解数据集的一切，那将是非常棒的。但是我们的神经网络并不适合这个任务，我们使用的是递归神经网络（RNN）。当Transformer问世时，几乎是在论文发布的第二天，我们就意识到Transformers解决了递归神经网络学习长期依赖的问题，这是一个技术问题，但我们立即切换到了Transformers。所以GPT项目继续进行，然后随着Transformer，它开始工作得更好，我们意识到需要不断扩大规模，我们做到了，这就是最终导致GPT-3的原因，也是我们今天所处的位置。

**扩展与深度学习的突破**

**主持人** ：是的，我想问一下，我被这些历史吸引住了，但我也非常感兴趣。我想谈谈大语言模型或大型模型的一些问题或缺点。Rich
Sutton写过关于扩展（Scaling Laws）的文章，说我们只需要扩展，不需要新的算法，他对你有影响吗？还是这是一个平行的思考轨道？

**Ilya** ：不，我会说当他发表他的文章时，我们很高兴看到一些外部人士有类似的想法，我们认为它表达得非常好。但我实际上认为“苦涩的教训”（the
bitter
lesson）所表达的内容被过度强调了，至少我认为人们从中得出的结论被过度强调了。人们的结论是无论你做什么，只需扩展，但那并不完全正确。你必须扩展一些具体的东西，你必须有一些能够从扩展中受益的东西。深度学习的巨大突破在于它为我们提供了第一个可以有效利用扩展并从中获得回报的方式。在此之前，人们会用大型计算集群做什么？我猜他们会用它进行天气模拟或物理模拟，但仅此而已，也许制作电影，但没有人真正需要计算集群，因为你能用它们做什么。深度神经网络的事实是，当你使它们变大并在更多数据上训练它们时，它们会变得更好，这为我们提供了第一个有趣的扩展对象，但也许有一天我们会发现有一些小的改变，使我们扩展的对象变得更好。现在，随着事后诸葛亮的优势，你会说这是否算数，它只是一个简单的改变，但我认为真正的说法是扩展什么是重要的。现在我们只找到了一个扩展对象，它给了我们回报。大语言模型的限制在于它们的知识包含在它们训练的语言中，大多数人类知识是非语言的。我认为每个人都同意这一点，我不确定Chomsky是否同意，但大语言模型有一个问题，我的理解是它们的目标是满足提示的统计一致性，它们没有与语言相关的现实的基本理解。

**主持人**
：我问ChatGPT关于我自己的问题，它识别出我是一个记者，在各大报社工作过，但接着谈到了我从未获得的奖项，读起来很漂亮但与现实不符。有没有什么正在做的研究来解决这个问题？

**幻觉与知识的局限性**

**Ilya**
：是的，在我评论你提问的直接问题之前，我想评论一下问题中的一些早期部分。我认为即使是像语言模型这样的东西，谈论它们的限制也非常困难，因为两年前人们自信地谈论它们的限制，完全不同。所以保持这个背景是很重要的，我们有多自信今天看到的这些限制两年后还会存在？我并不那么自信。还有一个评论我想对问题的一部分发表，这些模型只是学习了统计规律，因此它们并不真正了解世界的本质。我对此有不同的看法。换句话说，我认为学习统计规律比表面上看起来要重要得多。我们之所以最初不这么认为，是因为我们还没有，至少大多数人，那些没有真正花时间研究神经网络的人，在某种程度上是统计的：统计模型是什么？你只是拟合一些参数：到底发生了什么？但我认为有更好的解释。预测作为压缩也是一种统计现象，但为了预测，你最终需要了解产生数据的真实底层过程，程度足够好以进行预测。所以我认为这是一个很大的混淆，如果它只是统计的，如果这是解释，那么它就不会像现在这样好用。现在，AI中一些最难的任务只是通过统计学习直接完成的。所以我不认为这是思考这些限制的好方法。

有了这个背景，让我直接回答你的问题。你提出的关于这些模型生成幻觉的点是完全正确的。我认为无论我们对它们的机制理解多好，生成错误信息这一点我们都不喜欢。我们正在做一些非常具体的事情来改进这一点，我们通过进一步的人类反馈强化学习（reinforcement
learning from human
feedback）来训练这些模型，所以有一种新的人类反馈层，我们希望这将显著减少生成幻觉的情况。我们也在探索其他一些想法。

**主持人**
：你提到你有一个不同的解释，我能不能再深入一点？我的理解是，通过预测，你最终得到了一种模型，它实际上对所涉及的对象和语言的事物有一种概念上的理解，不仅仅是文本的拼接。换句话说，最终它会解决“狗在草地上”或“草地在狗上”这种基本问题，因为你有一个世界模型。Yann
LeCun有一个更强的观点，即你有一个单独的模块，尝试模拟这个世界，或者尝试做因果推断。你认为在未来五年内我们会看到这些方法吗？

**Ilya**
：我同意你所说的大部分内容。我认为Yann是对的，他关于世界模型的想法是我们如何在这里前进的重要想法之一。我还认为无监督学习，这个曾经的圣杯，现在也有了结果。回到前面一个问题，我认为这将是一个结合。人类也是如此：有很多事情你必须解释，并结合来自不同感官和观察的数据。我们今天看到的语言模型是预测的一部分。我认为未来将是一个结合

  

  

**⭐星标AI寒武纪，好内容不错过**⭐****

**用你的****赞****和****在看****告诉我～**

  

  

  

![](https://mmbiz.qpic.cn/sz_mmbiz_png/ICgnAptln0X53k92kQa8BeRQk0S3ZibtTFrf0vHLrLXqJpB3miaFf0HDXX1YjWgfQ3GdhgYuKAQTg746xfnBxxcg/640?wx_fmt=png)

  

👇👇

