# 不懂技术也不怕，我们现在用简单的语言，来聊聊GPT到底给我们的生活带来什么变化？？

文章作者: AI深度研究员
发布时间: 2024-04-05 10:00
发布地: 上海
原文链接: http://mp.weixin.qq.com/s?__biz=Mzg5NTc4ODkzOA==&mid=2247489241&idx=1&sn=0a6b0f916bb71680888b823fbac3335e&chksm=c00ba43cf77c2d2aca8bbbe96d937e571705f484d38272b43af097655921d107662e8556552f#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/iaqv2tagPYAiaiaibQeFjc0HngNgDt3PHQaKPsxEhdic5gbqGPsnxRdrsHGvuNIeHbUzicuNm9ibK9ykibudibMwtO76yhg/300

一个非常好的演示视频，通过可视化清晰的介绍了 LLM 的核心 Transformer
架构的原理。包括词嵌入、自注意力机制等关键技术。对了解GPT-3等大型语言模型的内部结构很有帮助。

现在你有一个能够自动写作的魔术笔。你只需要告诉它你想要写的主题，它就能够迅速地写出一篇文章。这听起来像是魔幻故事中的情节，但实际上，这种魔术笔在现实世界中已经存在了，它就是人们常说的
GPT。

# GPT 的全名是 Generative Pre-trained Transformer。

让我们一步步解释这个名字：

**“Generative”（生成式的），就像这个魔术笔能够“生成”新的文章一样，GPT 能够创造出全新的文本。**

**“Pre-
trained”（预训练的），意味着在这个魔术笔被交到你手上之前，它已经通过阅读和学习大量的书籍、文章等材料“训练”过了，所以它拥有丰富的知识基础。**

**“Transformer”（变换器），是这个魔术笔内部的“大脑”，一种非常先进的技术架构，让它能够理解和生成语言。** **

把这些组合起来，GPT
就是一种拥有高级“大脑”的魔术笔，它不仅学习了大量的知识，还能够根据你的要求创作出新的文本。无论是写一篇博客、撰写一封邮件，还是创作一个故事，GPT
都能够帮助你完成，而且效果出奇的好，就像它自己也是一个知识渊博的作家一样。

## Transformer的定义及其在AI领域的应用

现在有一套超级复杂的乐高模型，这套乐高模型由很多小部件组成，每个部件都有其独特的位置和作用。为了完成这个任务，你需要一个超级助手，能够清楚地记住每个部件的位置，知道哪些部件需要先连接，哪些部件之间相距很远但却紧密相关。这样的超级助手，在人工智能世界中就是“Transformer”。

最开始，Transformer
被设计来做“翻译”工作，就像是把一篇中文文章翻译成英文。但很快人们发现，它像个多面手，不仅在翻译上表现出色，还能帮忙写作、找出文章中的特定名字或地点，甚至还能听懂你说的话，或者给图片加描述。Transformer
能够做到这一切的秘密武器叫做“自注意力机制”，这让它在处理一连串的信息时，能够记住每一部分的内容，无论这些内容是不是紧密相连。

就像那个帮你组装乐高的超级助手，Transformer
能够记住每个小部件（或者说，信息的每个小片段），并理解它们之间的关系，即便这些关系跨越了很长的距离。这就是为什么 Transformer
在很多不同的领域都能大放异彩——它就像是拥有超级记忆力和理解能力的机器大脑。

## Transformer的工作原理简介

如果Transformer 是一位在做美食的大厨。这位大厨要做一道复杂的菜肴，过程大致分为三个步骤：

**首先，** 大厨会准备好所有的食材，这就像是把输入的文本序列转换成一组向量表示，每种食材（或每个词）都有其独特的特点和味道。

**接下来，**
大厨开始烹饪，通过自己的技巧（自注意力机制）和厨房里的工具（前馈神经网络）来处理这些食材。在这个阶段，大厨会不断尝试、调整，确保每种食材的味道能够和其他食材完美融合，这就像是对向量进行变换和更新，让每个词不仅保留自己的意义，还能够与文中其他词的关系得到加强。

**最后，**
当菜肴准备好后，大厨会根据最终的味道（更新后的向量）来做最后的调整，这可能包括加点盐、撒些香草等，以确保最后的菜肴不仅美味，还能让人回味无穷。这就类似于根据更新后的向量生成输出分布，用于预测下一个词——确保下一步的行动（或下一个词）最能够体现前面的所有准备和工作。

在整个过程中，大厨的烹饪技巧（自注意力机制）发挥了至关重要的作用，它使得大厨能够动态地判断哪些食材（词）之间的搭配最佳，从而更好地捕捉整道菜肴的风味（上下文信息）。

## Transformer处理文本信息的过程

Transformer 在这里是一个高级的拼图游戏专家。这个游戏的目标是根据给定的图片（一段文本）去拼出一个完整的故事（生成连贯的文本片段）。

**首先，**
专家需要把这个图片切割成许多小块（单词或子词），在游戏中，这些小块被称为“token”。接下来，每一块小拼图（每个token）都会被转化成一个有着多层颜色和纹理的3D模型（词嵌入），这样做的目的是把简单的图片块转换成更加丰富、能够被电脑“感知”的形式。

**然后，**
这些3D模型会经过一系列的精细加工过程（多个编码器层），在这一过程中，每一层都会仔细考量每个模型与其它模型之间的关系，确保每一块拼图都能够准确反映出整个故事的上下文信息。

**最终，** 解码器就像是专家用来预测下一块拼图应该放哪里的工具，它基于之前已经拼好的部分和编码器提供的信息来预测下一块拼图（下一个最可能的token）。

通过不断重复这个过程，拼图游戏专家（Transformer）就能够一步步地拼出一个连贯、完整的故事，就像是从一段文本生成另一段连贯的文本片段一样。

## 词嵌入(Word Embedding)的概念和作用

如果我们把每个词都想象成星空中的一个星星。这些星星（词）不是随机分布的，而是根据它们之间的关系聚集在一起，形成了不同的星座（向量空间）。词嵌入就像是一个天文学家，它的任务是将这些星星放入宇宙中，使得意义相近的词就像是彼此靠近的星星，聚集在一起。

比如，“国王”（king）和“皇后”（queen）这两个词，就像是同一个星座中相邻的两颗星，因为它们的意义相近，所以在这个宇宙的向量空间中彼此靠得很近。而“国王”和“苹果”（apple），它们就像是处于完全不同星座的星星，因为它们的意义相差甚远，所以在宇宙中的距离也会更远。

更神奇的是，词嵌入还能揭示词之间的类比关系，就像是发现星座之间的隐藏联系。比如，如果我们从“国王”指向“男人”（man）的方向走一段，再向“女人”（woman）的方向走相同的距离，我们最终会到达“皇后”这颗星，揭示了一种“男人对女人，国王对皇后”的类比关系。

通过这种方式，词嵌入为处理文本数据的神经网络提供了一张星图。这张星图帮助网络理解和利用词语之间的语义关系，进行更加精准的推理和预测，就像是导航星空，寻找意义的联系一样。

## 深度学习模型的基本结构和特点

我们把深度学习模型比作一座由许多楼层组成的大楼。在这座大楼里，每一层楼（层）都有其特定的工作，它会接收来自下一层的信息（输入数据），进行一些变化（变换），然后把结果传递给上一层楼。

楼层之间的楼梯（连接）是通过一种特殊的方法（矩阵乘法）建造的，而楼梯上的每一块砖（元素）就代表了这座大楼（模型）的一个小部件（参数）。当这座大楼在建造过程中（训练过程），我们会根据大楼的最终形态（输出）与我们预想的设计图（期望输出）之间的差异，不断调整每块砖的位置（更新参数），以确保最终建成的大楼尽可能接近我们的设计图。

在这个建造过程中，我们使用一种称为反向传播算法的技术，这就像是从大楼顶部开始，一层层向下检查每层楼的工作成果，根据顶层的差异来逐层调整下方楼层的工作方式，直到整座大楼（模型）的结构都按照设计图（期望输出）进行了优化。

通过这样多次迭代的“建造”过程，大楼（模型）会逐渐学会如何根据设计图（数据）建立起正确的结构（学习到数据中的规律和模式）。深度学习模型最神奇的地方在于，它们能够自己找出建造大楼（数据表示方式）的最佳方法，而不需要工程师（我们）手动指导每一步，这使得它们在构建复杂的结构（处理图像、语音等复杂数据）时表现得非常出色。

## Softmax函数的作用和计算过程

你和你的朋友们去了一家自助餐厅，每个人都从各种菜品中选了一些自己最喜欢的菜。现在，你们想要决定每种菜品的受欢迎程度，但是只能通过观察每个人盘子里的菜量来判断。这时候，Softmax函数就像是一个聪明的算法，帮助你把每种菜的受欢迎程度转换成一个概率分布。

**首先，**
Softmax函数通过“取指数”这个步骤，把观察到的每个菜量（输入值）变得更加明显。就好比说，如果有些菜品本来就比其他的多一点点，通过取指数，这种差别会被放大，让我们更容易看出哪些菜品更受欢迎。

**然后，**
为了确保这个受欢迎程度可以被合理地比较，Softmax函数会做第二步：“规范化”。它把每个菜品的“放大后的受欢迎程度”除以所有菜品的“放大后的受欢迎程度”总和。这样一来，每种菜品的受欢迎程度就被转化成了0到1之间的概率值，所有菜品的概率加起来正好是1，正好反映了每种菜在大家心中的地位。

Softmax函数的一个优点是，它能够让那些本来就受欢迎的菜品（输入值较大）获得更高的概率，而那些不太受欢迎的菜品（输入值较小）的概率则接近于0。这就像是在说：“是的，大家都更喜欢这些菜，而那些菜几乎没人吃。”这让整个自助餐的选择变得容易解释，并且帮助你们做出更好的决定，比如下次聚餐时考虑大家的喜好，预测哪些菜品会更受欢迎。

## 生成模型预测下一个词的过程

想象一下，有一个会讲故事的魔法盒子（像GPT这样的生成模型）。你只需告诉它故事的开头，它就能继续讲下去。这个魔法盒子创造故事的方式有点像填词游戏，但更加高级和神奇。

**首先，**
魔法盒子会回顾你给它的故事开头，然后在它的魔法书里（模型的知识库）查找所有可能的下一个词。每个可能的词都有一个概率，这个概率告诉魔法盒子接下来最可能使用哪个词。这就好比魔法盒子在想：“基于目前的故事，下一个词最可能是什么呢？”这些概率就是它对不同词的偏好程度的反映。

**接着，** 魔法盒子会从这些可能的词中挑选一个，就像是从魔法帽中抽出一张卡片。这个过程叫做“采样”，它决定了故事接下来会怎么发展。

**然后，** 这个新词就被加入到已经生成的故事中，魔法盒子就这样一步一步地继续讲故事，直到故事讲完（达到预设的长度）或遇到明显的结束信号（比如句号）。

为了让故事更加多样和自然，魔法盒子还有一个调节旋钮，就是所谓的“温度”。调整这个旋钮可以改变故事的风格——转到“热”一点，故事就变得更加随机和创意；转到“冷”一点，故事就会更加紧扣主题，更少出现意外转折。通过这种方式，魔法盒子能够创造出既连贯又充满创意的文本。

## GPT-3的参数量和嵌入矩阵

# GPT-3
仿佛是一座巨大的图书馆，里面藏有1750亿本书（模型参数），每本书都包含了语言的秘密，比如单词的意义、句子是如何构成的，以及语言的各种规则和模式。这座图书馆的目标是帮助机器理解和使用人类的语言。

在这座图书馆中，有一本特殊的索引册（嵌入矩阵），它记录了50257个不同的词或字词（就像图书馆的书籍目录），每个词都对应一个非常长的代码（12288维向量），这些代码就像是每个词的秘密身份，包含了该词的所有特征和信息。

这本索引册里的每一个数字（元素）都可以改写（是可学习的参数），图书馆的管理员（模型）通过不断地阅读和学习（训练过程），会调整这些数字，使得意思相近的词在索引册中的代码也相近。这就好比是通过整理和更新索引册，让查找和使用书籍变得更加高效。

因为这座图书馆拥有如此多的书籍（参数量巨大），它能够收集和学习比以往任何图书馆（模型）更多的知识，从而更好地理解语言中的细微差别。这就是为什么 GPT-3
在处理语言任务时，能够展现出非常出色的能力——就像是一位拥有广博知识的图书馆管理员，能够准确无误地提供你需要的任何信息。

## 模型训练中的"Temperature"超参数

这里有台音乐盒，里面的一个特殊的旋钮，就是“temperature”（温度）。这个旋钮可以控制故事的风格——是让它充满惊喜还是更加稳定和可预测。

如果你把温度旋钮调得较高，音乐盒就像是被赋予了自由精神，故事会变得非常多样化和丰富。在这种设置下，音乐盒不拘泥于常规，甚至可能讲出一些意想不到的、充满创造性的情节。这就像是在探索一个未知的故事世界，你永远不知道下一刻会发生什么。

反之，如果将温度旋钮调低，音乐盒就会变得更加保守，它会倾向于重复那些“安全”的、概率最高的词汇和情节，生成的故事可能就比较单一，缺乏新意，就像是一遍又一遍地讲同一个老掉牙的故事。

找到合适的温度设置，就能在保持故事连贯性的同时，赋予它适当的丰富性和创造性。比如，当温度设置接近于0时，你可能得到一个逻辑性很强、条理清晰的故事，但可能稍嫌平淡；而当温度设置较高时，故事就可能充满了惊喜和新奇，但同时也要注意不要让故事变得太离奇或缺乏逻辑。

在实际使用中，根据你想要讲述的故事类型或特定的需求，调整这个“温度”旋钮至理想状态，就可以让这台神奇音乐盒帮你讲出既连贯又新奇的故事。

总之，在我们日常生活中，GPT通过学习大量的文本数据，它能够帮我们写文章、回答问题，甚至创造出新的故事。技术的发展，GPT应用将会更加广泛，为我们的学习、工作和娱乐带来更多便利和乐趣。

* * *

![](https://mmbiz.qpic.cn/mmbiz_png/iaqv2tagPYAhtRhTOjz2QwH4dIlC3YUcYbaicMEwjqQqh06Yhdd7EH3r9wiaMRArLz0a6Zhx6uiaUD7hguPfbY0nAg/640?wx_fmt=png&from=appmsg)

**一年多打磨，我们的AI团队现向外界开放服务，旨在助力每个企业与个人引领时代潮流，将先进科技与创新想法完美融合!**

##  告别昂贵服务费和缺人烦恼,再见漫长交付周期

## 无限创意风格,分分钟生成专业级作品

## 感受 AI 带来的全新工作体验！

**_欢迎各大品牌方、媒体、科技企业、知名IP等合作_**

** _合作请联系负责人微信：Milo-1101_**

** _\--END--_**

  

