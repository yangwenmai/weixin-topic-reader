# 炸裂谷歌DeepMind和OpenAI员工表示：OpenAI的人工智能已经接近人类智能水平，AGI应该在2027年问世

文章作者: AI深度研究员
发布时间: 2024-06-05 10:01
发布地: 未知地区
原文链接: http://mp.weixin.qq.com/s?__biz=Mzg5NTc4ODkzOA==&mid=2247490814&idx=1&sn=f460da89adf9fe89ab90635ae85f2c7f&chksm=c00bae1bf77c270d826fdd28771052bd6d56e2042c22ec51208702c7990f583444e98b578faa#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/iaqv2tagPYAgeXW0TD4EMxicJpouWG7CrTmCMeswfSOsVSgMSGzbsRRy1gTwQIZ0w2gwTQHeJeQicibuJROud7P5ZQ/300

![](https://mmbiz.qpic.cn/mmbiz_jpg/iaqv2tagPYAgeXW0TD4EMxicJpouWG7CrTOvR6tNl74K5lcpgEYAbyGM5XrjAJSRpGdQ1Q2XJ20liawDOxx6OnFDA/640?wx_fmt=jpeg)

  

这篇公开信刊载在righttowarn.ai网站上，信件标题为《对先进人工智能发出警告的权利》由顶尖AI公司员工共同完成，包括 人工智能之父Geoffrey
Hinton, 图灵奖得主 Yoshua Bengio等。他们呼吁对AI行业进行全面改革，包括提高透明度和保护吹哨人。

  

原文如下：

前言：OpenAI的人工智能系统已经接近人类智能水平，通用人工智能AGI有50%的可能性在2027年就会问世，而不是此前预测的2050年！

这封公开信指出，人工智能的发展可能会带来一系列的风险，例如加剧现有社会的不平等、助长操控和虚假信息传播，以及失控的自主人工智能系统可能导致人类灭绝等。

  

签署公开信的多位OpenAI前任雇员曾从事人工智能安全工作，为公开信背书的业内大佬则有“人工智能教父”Geoffrey
Hinton、因开创性AI研究而获得图领奖的Yoshua Bengio，以及AI安全领域的顶尖专家Stuart Russell。

  

据署名签字公开信的OpenAI前任员工对媒体介绍，这家位于旧金山的人工智能热门公司存在“为争夺谁第一个把最强大AI产品推向市场而不计后果”的鲁莽企业文化，无法有效察觉和治理AI系统的潜在风险，还用剥夺吹哨人前员工股权等强硬手段阻止他们表达对AI技术的担忧。签字公开信的OpenAI现任员工全部选择匿名，就是因为担心被打击报复。

  

公开信的签署者之一、今年4月从OpenAI人工智能治理岗位离职的Daniel
Kokotajlo指出，辞职是因为“对OpenAI能否负责任地行事失去了信心”。他称OpenAI的人工智能系统已经接近人类智能水平，通用人工智能AGI有50%的可能性在2027年就会问世，而不是此前预测的2050年：

  

“世界还没有准备好（迎接如此高级的AI智能），我们也没有准备好，我担心（这些商业公司）将不管不顾地一意孤行，并为自己的行为寻找借口。先进的人工智能毁灭人类或对人类造成灾难性伤害的可能性高达70%。”

  

他称，尽管OpenAI已与微软联合开展“部署安全委员会”等安全协议，旨在公开发布新AI模型之前进行重大风险审查，但并没有因此减慢新产品的发布速度。与投入更多时间和资源用来防范人工智能潜在风险相比，OpenAI仍在急于改进模型并进行商业化推广。

  

微软甚至两年前在印度悄悄测试Bing搜索引擎包含未发布GPT-4的新版本，引发了一些搜索引擎对用户行为异常的报告。OpenAI原本不知道最先进大语言模型被投入测试，知道后也没有阻止微软更广泛地推出该产品。不过，OpenAI发言人对上述说法提出异议。

  

信中写道，人工智能公司拥有强大的经济利益驱动它们继续推进人工智能研发，同时却对保护措施和风险水平的信息讳莫如深。公开信认为，不能指望这些公司会自愿分享这些信息，因此呼吁内部人士站出来发声。

  

由于缺乏有效的政府监管，这些现任和前任员工成为能够让这些公司对公众负责的少数群体之一。然而，由于严格的保密协议，员工被限制发声，只能向可能并未妥善处理这些问题的公司反映问题。传统的举报人保护措施并不适用，因为此类措施侧重于违法行为，而目前令人担忧的许多风险尚不受监管。

  

员工们呼吁人工智能公司为揭露人工智能风险的人士提供可靠的举报人保护措施，具体包括：

  

•不得创建或执行阻止员工对风险相关问题提出批评的协议；

  

•提供可验证的匿名程序，使员工能够向董事会、监管机构和相关领域的独立组织提出与风险相关的担忧；

  

•支持开放批评的文化，允许员工在保护商业机密的前提下，向公众、董事会、监管机构等方面提出与技术相关的风险担忧；

  

•在其他程序失效后，避免对公开分享风险相关机密信息的员工进行报复。

  

共有 13 名员工签署了这封公开信，其中包括 7 名前 OpenAI 员工、4 名现任 OpenAI 员工、1 名前谷歌 DeepMind 员工和 1
名现任谷歌 DeepMind 员工。

  

OpenAI 曾因员工发声而威胁取消其既得权益，并要求员工签署严格的保密协议限制他们批评公司。

  

OpenAI则发布声明称，为公司能提供最强大、最安全人工智能系统的历史成就感到自豪，并相信其内部应对风险的科学方法，“我们同意，鉴于这项技术的重要性，严谨的辩论至关重要，我们将继续与世界各地政府、民间社会和其他社区进行接触。”

# 往期回顾

[1、[对话节目：AI之父Geoffrey
Hinton为什么坚信模型越大，AI越能够像人一样更有创造力？]![](https://mmbiz.qpic.cn/mmbiz_png/iaqv2tagPYAhKPk3rcVLgIibHkuX1KyLFcic3aA22s6108LbTkM2HYWricduudBdsaiaUVAk8DJW10EOhccghz4rGWA/640?wx_fmt=png&from=appmsg)](https://mp.weixin.qq.com/s?__biz=Mzg5NTc4ODkzOA==&mid=2247490393&idx=1&sn=1f326befbceb9f158ed0a0af4b2500f8&chksm=c00ba9bcf77c20aacd95c53dac49ddb0766d6ac03c00d394a228d32d496d2ed6e8831eb468b8&scene=21#wechat_redirect)

[2、[视频访谈节目：OpenAI创始人Sam
Altman亲述GPT-4o细节,展望科技浪潮下新生存模式!]![](https://mmbiz.qpic.cn/mmbiz_png/iaqv2tagPYAhKPk3rcVLgIibHkuX1KyLFcD7JGXlKhlaqlm5ibOkCpDs5Wradgibs8wp3yGQj9KGHJz3lWYTGWPAKA/640?wx_fmt=png&from=appmsg)](https://mp.weixin.qq.com/s?__biz=Mzg5NTc4ODkzOA==&mid=2247490338&idx=1&sn=56ad0861af3bd36997324913c95cebc8&chksm=c00ba9c7f77c20d12f9b7ecd19e07cc34934a430ce7bcb7206ff939efd5a4382eedcded9b1ec&scene=21#wechat_redirect)

[3、[从观望者到行动者的转变，红杉资本2024年AI大会指明普通人抓住AI机遇的路径]![](https://mmbiz.qpic.cn/mmbiz_png/iaqv2tagPYAhKPk3rcVLgIibHkuX1KyLFcYfwnO0gibLMMSyDqXgxqz5OLugDzhibaxZNYNiaw4nq7VP1pgR4ZwgB6Q/640?wx_fmt=png&from=appmsg)](https://mp.weixin.qq.com/s?__biz=Mzg5NTc4ODkzOA==&mid=2247489090&idx=1&sn=35b4df58a275f7ed62074ec183e28f3c&chksm=c00ba4a7f77c2db193fc8efcd78a3327f3cb92db6b595b26d1bbb9cdbaee0a0d717a2f6f860e&scene=21#wechat_redirect)

* * *

![](https://mmbiz.qpic.cn/mmbiz_png/iaqv2tagPYAhtRhTOjz2QwH4dIlC3YUcYbaicMEwjqQqh06Yhdd7EH3r9wiaMRArLz0a6Zhx6uiaUD7hguPfbY0nAg/640?wx_fmt=png&from=appmsg)

****

**我们的AI团队现向外界开放服务，旨在助力每个企业与个人引领时代潮流，将先进科技与创新想法完美融合!**

#  告别昂贵服务费和缺人烦恼,再见漫长交付周期

# 无限创意风格,分分钟生成专业级作品

# 感受 AI 带来的全新工作体验！

** _欢迎各大品牌方、媒体、科技企业、知名IP等合作_**

** _合作请联系负责人微信：Milo-1101_**

** _\--END-**_-_**_**

