# 吴恩达教授访谈：利用AI技术突破学习障碍，将你的创意无缝转化为功能完备的APP

文章作者: AI深度研究员
发布时间: 2024-10-03 10:00
发布地: 上海
原文链接: http://mp.weixin.qq.com/s?__biz=Mzg5NTc4ODkzOA==&mid=2247493419&idx=1&sn=80f446501de70dbb2f4b357c7356ad33&chksm=c00855cef77fdcd8654a8fa28b2d1eadaee99933bcf14c325a5234e4f58c05e95baf44109385#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/iaqv2tagPYAhm1CxSF3DwvBpoQ3TFUEUawSxkTaicwkwBIX7VJicINf0IpjeMNRRDuSkiciaa0y2QFcUIQckMhicV2sg/300

**（关注公众号并设为🌟标，获取最新人工智能资讯和产品）**

全文约8,000 字，阅读约需 20分钟

斯坦福大学的吴恩达教授在最新一期采访中深入探讨了这一主题，特别强调了数据工程对数据中心AI的重要性。

吴教授畅想了AI在教育中的诸多应用，如AI导师等创新概念。他认为，教育领域即将迎来一场巨大的变革，尽管目前还难以准确预测其具体形态。在讨论未来工作的变化时，吴教授以编程为例，强调了学习如何利用生成式AI作为编程助手的重要性。他指出，未来的软件工程必然是与AI助手协同工作的模式。无论是使用GitHub
Co-pilot、Cursor，还是直接从GPT-4、Claude或Gemini复制粘贴代码，编程学习将不再是一个孤立的过程。

吴教授呼吁教育机构应该调整教学方式，使之与未来技术发展保持一致。这不仅适用于计算机科学，也涵盖了化学工程、医学等其他领域。他认为，这对学术机构来说是一个巨大的挑战，但同时也是一个机遇

特别是在编程方面，吴教授希望借助编程助手和生成式AI的力量，使编程学习变得前所未有的容易，从而让更多人能够掌握这项技能。他观察到，会写一点代码的人现在比以往任何时候都更有价值，无论是软件工程师还是其他领域的专业人士。

  
  

## 文稿整理

**主持人，：** 吴恩达教授，很高兴见到你。

**嘉宾吴恩达：** 我也很高兴见到你，最近怎么样？

**主持人：** 嗯，挺激动的时刻，对吧？

**吴恩达：** 是的，的确如此，谢谢你今天邀请我参加这个节目，很高兴见到你。

# 数据驱动的AI

**主持人：** 嗯，今天我们要讨论一个对你非常重要的主题，那就是数据驱动的人工智能（Data Centric
AI），还要讨论数据工程在数据驱动的人工智能中的关键作用。那么，我们先从头开始吧，带我们回顾一下你对数据驱动的人工智能的初期想法吧。我记得在此之前，我们更多的是处在一个以模型为中心的世界里，对吗？

****吴恩达：** **
是的，我认为在过去的几十年里，人工智能的进展主要是由人们从互联网上下载数据集，然后花很多时间发明新的数学方法或者新的模型来提高在这些数据集上的表现。这本身并没有什么错，通过这种方法，人工智能确实取得了很大的进步。但是，我认为包括你、我以及很多其他人工智能从业者都意识到，如果我们想要构建一些实际的、可以交付的产品，有时对数据的精心打磨要比对模型进行优化更有成效。所以我想要创建并推广“数据驱动的人工智能”这个术语，以整合许多已经在进行中的工作，重点在于打磨数据而非模型。看到数据驱动的人工智能社区的快速发展，真是令人兴奋。

**主持人：** 我得问一下，因为这似乎是现在讨论的核心问题，生成式人工智能和数据驱动的人工智能有什么联系吗？例如，在训练大型语言模型时，你对此有何看法？

******吴恩达：**
我认为它们非常相关。事实上，从训练基础模型开始，到训练后期的微调，甚至到某些模型的部署和使用阶段，我们似乎一直都在处理数据问题。我知道在媒体上，大家谈论的很多是扩展法则（scaling
laws）以及如何构建更大的 Transformer
网络来训练更多的数据。这当然是一个重要的部分，但当我与那些真正参与这些模型日常工作的人交谈时，发现他们的大部分精力——我甚至想说超过50%的工作——其实都集中在如何获得合适的数据以供这些基础模型使用。当然，即使某人已经训练好了一个大型语言模型或基础模型，后续仍有大量的数据工作要进行微调，特别是在实际部署和使用时，比如在少量样本学习（few-
shot
learning）中，也涉及大量与数据相关的思考。所以虽然不是一切都与数据驱动的人工智能相关，但即使在生成式人工智能和基础模型中，数据的重要性也比人们普遍认为的要大得多。

**主持人：** 其实我们可以聊很久，你继续说吧。

****吴恩达：** ** 好啊，也许可以分享一个有趣的例子。在 Meta 发布的 LLaMA 3.1 论文中，有很多有趣的点，其中之一是 Meta
使用了一个较早版本的模型，也就是 LLaMA 3，然后通过一种代理工作流程（agentic workflow），让 LLaMA 3 生成用于训练 LLaMA
3.1
的编程谜题数据。这一直是个难题，如何使用合成数据来训练基础模型。通过这个代理工作流程，模型可以长时间思考并反复迭代，直到得到一个很好的结果，然后训练下一代模型，使其能够快速得出同样优秀的答案，而无需多次思考。我觉得这是一个非常好的方法，用于创建训练基础模型的数据。当我与那些负责训练非常大型基础模型的朋友交谈时，他们的大脑中一直在想的问题是：我能否与出版商签订合适的许可协议来获得数据？这些数据中，我该在哪些数据上投入资金购买？还有关于偏好调优（RHf）或者
DPO 的问题，如何使模型与人类价值观对齐？这些标签方案是什么，数据又从哪里来？虽然在训练 Transformer
网络方面有一些创新，但数据仍然是核心问题。

# 数据时代AI和传统AI

**主持人：** 听起来有很多关于如何获取数据来训练这些模型的思考。你最初提出“数据驱动的人工智能”这个概念的时候，那还是在 ChatGPT
之前的时代。我想问一下，你如何区分“数据驱动的人工智能”和经典的人工智能，尤其是深度学习和生成式人工智能之间的差异？在你看来，数据驱动的人工智能和经典人工智能有区别吗？还是说最终它们都是相同的东西？又或者它们只是某种技术连续体的一部分？

******吴恩达：**
我认为数据驱动的人工智能在不同的数据类型和模态下确实有所不同。例如，视觉领域的人工智能与文本领域、音频领域的人工智能是不一样的，尤其是处理那些人类自身也难以处理的结构化数据时，方法也会有所不同。不过，尽管这些领域有差异，但它们都有一些共通的原则。比如，如何系统地工程化数据以构建成功的人工智能系统？像误差分析这样的技术，用来找出数据中的不足，从而获取更多的数据。此外，还有一些数据标注技术，用来获取高质量数据。然后在小规模高质量数据和大规模低质量数据之间，我们必须做出权衡。我看到这些主题在从训练计算机视觉模型到在少样本学习中选择放入大型语言模型提示的小部分样本时，都有体现。此外，我还看到这些主题在监督学习和生成式人工智能中都有体现。很多企业现在都在积极思考如何整理他们的数据基础设施。而生成式人工智能的出现，无疑给很多公司和董事会带来了推动力，他们更迫切地想要把数据基础设施整理好。我认为这是一种积极的变化，增加了紧迫感。人们现在说“我们必须解决数据问题”，这无疑让你们正在做的事情变得更加紧急。

**主持人：** 这对你们的书销量应该也有帮助吧？

****吴恩达：** ** 非常有帮助。我们刚刚经历了一波炒作周期，现在正好赶上了另一波热潮。

**主持人：**
但这确实很有趣，因为我感觉现在人们更关注基础，确保他们能够投资于支持分析、机器学习和人工智能的基础设施。但我觉得公司要真正意识到这一点并不容易。特别是在2010年代，很多公司想跳过数据积累，直接进入机器学习领域，而我们也都看到了结果。所以，数据工程在这一切中显得尤为重要。你对数据工程在当前人工智能中的角色有什么看法，Andrew？

****吴恩达：** **
我认为数据工程是至关重要的。我看到在这个领域存在明显的人才短缺。当我在美国和其他国家旅行时，接触到一些非常大的、非常盈利的公司——有时是科技公司，有时不是——发现有很多聪明的人在进行业务，但却在如何架构数据方面遇到了困难。我认为这确实很困难。我们要做出很多决策，比如如何存储数据，使用哪个云服务，合适的数据库方案是什么，成本和性能之间的权衡是什么。另一个挑战是，大多数公司并不愿意花费数百万美元仅仅为了改进数据。因为你希望改进数据是有目的性的，所以希望企业能够进入这样的循环，当你在人工智能、机器学习、生成式人工智能方面开始取得胜利时，同时也通过这些应用来推动数据架构的改进，这样你的基础就会变得越来越好。所以，有些企业高管会说：“好吧，让我投入大量资金来修复我的所有数据，这样一切就会很美好，我就会有很棒的
AI
了。”我觉得你和我都会建议他们不要这么做，对吧？你确实需要在数据上进行投资，但数据通常是为特定的目的而构建的。否则，我们可以优化太多方面——让它更快、更分布式、更强大等等，这里有太多决策要做。但如果你有一个或几个用例作为目标，那就能帮助数据团队做出正确的优先级决策，以改进数据基础设施，这样就能为很多人构建各种令人兴奋的应用奠定一个很棒的基础。

**主持人：** 没错，我经常给公司提出类似的建议。我猜想你也经常给很多企业提供这样的建议吧？而且现在每个公司似乎都必须有一个 AI
故事，无论你喜不喜欢，作为公司，你都必须想办法与 AI 一起工作。在这个话题上，我也想听听你关于小型语言模型和 AI
代理的看法，当你和朋友们讨论这些时，你都听到了什么或看到了什么？

****吴恩达：** ** 我认为代理式工作流（agentic workflows）是 AI
最令人兴奋的发展方向之一。现在很多人使用大型语言模型的方式是我们给它一个提示，然后期望它为我们写一篇文章，这有点像对一个人说：“嘿，帮我写一篇文章吧，从第一个字到最后一个字全部一次性写完，不要用退格键。”你可能可以这么写，但大多数人不是这样做的。相比之下，代理式工作流可能会让大型语言模型先进行头脑风暴和列出大纲，然后再问它是否需要进行任何在线研究，如果需要，就去下载一些网页内容并将其纳入大型语言模型的上下文中，然后再写出初稿，接着对初稿进行评审和批改，等等。我们看到，很多代理式工作流的输出质量比直接生成的要高得多，这在很多应用中都很有用。实际上，在我们内部的一些
AI 项目中就使用了代理式工作流。例如，我领导的 AI
基金在医疗、法律合规、处理各种复杂文件等项目中，都依赖代理式工作流，否则这些工作几乎无法完成。最近，我觉得比较有趣的是，GPT-4.o1
的预览版刚刚发布了，Anthropic 也在几个月前做了一些类似的事情，他们微调了大型语言模型，以便在生成过程中能够输出“思维标记”（thinking
tokens）。通过这种方式，语言模型在输出最终答案之前会花更多时间进行思考。

这确实是一个令人兴奋的方向，尤其是对那些需要深度推理的应用程序来说。几个月前，已经有一些模型通过破解的方式实现了类似的功能。比如，Anthropic
通过使用 XML
标签和“思维标记”让语言模型在输出答案前进行更深层次的推理。不过现在已经有人通过破解的方式让模型能够查看标签，并展示它的内部思维对话。我觉得这应该是公开的消息，并不算是什么新内容。这种方式确实很聪明，但我认为
OpenAI 在这次的新模型发布中将其提升到了一个全新的高度。我记得大概一两周前，有一个反射式 70B
模型被炒得很热，但初步的宣传并不完全准确。不过，他们也在探索类似的技术，尽管宣传有些过头，初期结果也有些不准确，但这种技术的底层方向还是非常有趣的。所以，我认为这确实是一个热门的趋势。代理式工作流是人们可以实施的，而且现在也有可能通过微调大型语言模型，让它能够在内部进行链式思维推理（Chain
of Thought
reasoning），并且可能会使用一些标签来进行思考。这些思考的结果可以选择是否展示给终端用户。所以，我觉得这是一个非常有趣的方向。当然，很多实现这种工作的基础仍然是数据集，你需要通过数据来展示正确的思维过程，不同类型的任务会有不同的思维方式。真的是激动人心的时代。

# 数据再训练

**主持人：**
真有趣。我正打算问一个问题，和你刚才讨论的内容有些相关。我觉得现在大家面临的一个问题是，如果我们把模型的输出作为数据再用于训练，权重就会劣化，对吧？这有点像是在录制一个带有噪声的信号。实际上，已经有一些研究论文证明了这个问题。那么，你怎么看待这个问题，尤其是现在越来越多的网络内容是由机器生成的？

****吴恩达：** **
好问题。我认为，挑选高质量的网络数据是一项持续的工作。如果你使用大型语言模型生成文本，然后用这些数据训练另一个模型，这在模型蒸馏（model
distillation）中是可行的。如果你有一个大模型生成了非常有深度的文本，然后你想训练一个小模型来模仿大模型的思维方式，这就是模型蒸馏的工作原理。不过，正如你提到的，Matt，不可行的是，如果你用一个模型生成数据，然后再用这些数据训练另一个模型，并期望这个新模型表现得更好。实际上，一些研究者已经表明，如果你重复这种过程，生成数据、训练新模型、再用新模型生成数据、用这些数据训练下一个模型，最终你会得到模型崩塌（model
collapse），模型最终生成的内容会变得非常无趣。不过，有一种方法确实可行，就是你不要直接复制粘贴一个模型的输出到下一个模型的训练数据中，而是使用一种代理式工作流。比如，第一个模型可能会写一篇文章，反思它的内容，做一些网络搜索，然后对其进行批改和改进，经过一番努力后得到一篇相当不错的文章。而你希望下一代模型能够直接生成这篇文章，但付出的努力比第一个模型少得多。这种方式确实有效。虽然把这种方法比作人类思维可能有些危险，但我记得小时候为数学竞赛练习时，我会花很长时间去解一个数学题，但一旦自己解决了，我就会发现下次我可以用某个捷径来更快地解决类似的问题。所以，训练自己的思维是可以的，前提是你学会如何快速完成曾经花费很长时间才能做好的事情。这种方式对语言模型似乎也适用。

**主持人：** 所以，这就像是一种自我管理的方式，不只是依赖数据训练，而是通过批评和反思，像你说的那样，形成一种代理式的、更加深入的思考过程？

****吴恩达：** **
没错，正是如此。我认为很多大型公司在训练大型基础模型时，确实保留了很多他们操作的细节，这些都是专有的。不过，当我和来自不同公司的多人交谈时，确实感觉到他们的关注点不仅仅是在微调基础模型或者确保
GPU
的可靠性，虽然这些也是重要的部分，但他们的大部分思维还是在解决数据问题。我认为，人工智能模型不仅仅是模型，而是模型加数据的结合。其实，很多人都有这样的经验，当你深入了解
Transformer
神经网络的数学原理，尤其是注意力机制时，常常会有一个有趣的现象：很多人第一次学的时候都会问，“我不明白，怎么就这几行数学公式就能展示出大型语言模型的智能行为？”我认为答案在于，这种“魔力”不仅仅来自于
Transformer
神经网络本身，虽然它确实很聪明。大型语言模型的很多智能其实并不来自神经网络的架构，而是来自于数据。这就是为什么当人们想要理解大型语言模型时，通常的反应是“让我先研究一下
Transformer
神经网络”，但当他们终于搞懂了所有数学原理后，却仍然感到困惑，不明白这些数学怎么会表现得如此智能。我认为，很多智能其实来自这些模型吸收了大量由人类生成的文本数据，这些数据才是产生智能或看似智能的原因。

**主持人：** 我懂了。你有很多朋友都在这个领域里工作，你现在看到 Transformer 架构以外的东西正在发展吗？还是我们会在这个架构上停留一段时间？

****吴恩达：** ** 这是个好问题。我认为 Transformer 架构有很强的势头，最近 ssrm
模型虽然已经存在了一段时间，但还没有真正起飞，不过仍然有足够多的研究者在研究它。我认为这个方向很有趣，尤其是在处理超长输入上下文方面。另外，我认为不太可能大规模应用的模型是用于文本生成的扩散模型。最近我看到了一篇关于这方面的论文，感觉非常有意思。目前，用于图像生成的主流模型是扩散模型，它们通过生成一张模糊的图像，然后不断“去噪”来逐渐清晰化图像。而有人设想了一种方法，可以生成一段模糊的文本，然后逐渐“去噪”使其变得清晰。当这个模型被训练成
GPT-2 大小时，它的表现似乎优于 GPT-2，不过还不确定在大规模应用时效果如何。我不确定它最终能否成功，但如果我们长期依赖
Transformer，也不会有太大问题。不过，随着 ssrm 和扩散模型的探索，以及其他一些新的尝试，我认为我们的社区还是有机会找到一些更好的解决方案的。

# 教育中的AI

**主持人：**
这确实是很有趣的探索。稍微换个话题，教育也是你的一大热情所在，你之前也围绕教育创办了公司。你觉得现在生成式人工智能在教育中扮演什么角色呢？比如，如果我想学习一个主题，或者我在教授一个主题，它会如何融入其中？

****吴恩达：** ** 是啊，我觉得教育领域正在迎来一场变革，虽然我还不确定它具体会是什么样子。有几家公司已经做了一些尝试，比如 Khan
Academy 推出了 Kigo，看起来效果不错。Cera 有个叫 CER Coach
的工具，实际使用起来也非常好，如果还没试过的话，我强烈推荐。我自己用了很多次，感觉出奇的好。不过，我觉得这只是其中的一个产品理念。还有 Coursera
使用生成式 AI 来为企业定制课程，很多公司通过它来为企业需求量身定制课程。所以类似这样的想法有很多，比如 AI 导师（AI
TA）等等。我觉得教育领域可能会有更大的变革，但我现在还无法准确预测它会是什么样子。我和大学领导交流时，我们经常会谈到工作未来的变化。以编程为例，我认为我们所有人都应该学会如何使用生成式
AI 作为编程助手来学习编程。我知道一些学校还在讨论是否要在编程课程中禁止使用
ChatGPT，但老实说，未来的软件工程一定是与编程助手共同工作的。我们可以选择使用 GitHub Co-pilot、Cursor，还是直接从
GPT-4、Claude 或者 Gemini 复制粘贴代码（其实我也常常这么做）。但重点是，你永远不必独自学习编程。

所以我们应该调整教学方式，更多地与未来的技术发展保持一致。我认为我们在教程序员时，应该让教学方式与未来接轨，而不是固守过去的做法。这不仅仅是计算机科学领域，在教育中，我们还应该思考未来的化学工程师会做什么，未来的医生又会做什么。我觉得这对于学术机构来说是一个巨大的挑战。但具体到编程，我希望几乎每个人都能学会编程，因为有了编程助手和生成式
AI，我希望编程比以往任何时候都更容易。会写一点代码的人，现在的价值也比以往任何时候都要高。我看到软件工程师的生产力显著提高，使用生成式 AI
可以做更多的事情。而且在我的团队里，像市场人员、投资者这样的非软件工程师也学会写一点代码，比如下载网页、合成数据、获取见解。我发现这些会写一点代码的人往往能做更多的事情。所以，我们推出了免费的
AI Python 初学者课程，帮助人们第一次学习编程。

**主持人：** 我看到一些开源项目的维护者抱怨，有人使用 ChatGPT
生成代码，然后尝试运行时发现代码不工作，结果是因为代码中有非常基础的错误，比如变量名不一致，可能只是大小写不同，导致代码无法运行。这些人不懂编程，所以不知道如何调试这些错误。我们应该如何教学生使用这些工具，同时确保他们对编程有足够的了解，掌握编写好代码所需的细节呢？

****吴恩达：** ** 是的，我也知道社交媒体上经常爆出一些“我用 AI
写了代码，但它没工作”的例子，虽然他们做的事情很酷，但确实存在这样的问题。我觉得至少目前，这些情况还是例外。我确实看到了这些例子，但希望将来这样的例外能越来越多。不过，我发现会一点代码的人比完全不会代码的人进展要快得多。如果你知道一些基本的编程概念，比如“异常”这个词是什么意思，掌握这些概念的人能做的事情远远多于那些完全不会代码、只靠提示的人。技术的进步确实在改变这种界限。我认为
Anthropic 的工具非常聪明，它帮助把基础模型带到部署阶段。Replit 也在这方面做了很多工作，降低了部署的难度。我个人开发了很多
Streamlit 应用，因为 GPT-4 在编写 Streamlit
代码方面非常出色，所以我不必担心语法问题，可以在几分钟内就把东西部署到云端。所以，我认为掌握一些编程概念的人进展更快，遇到瓶颈的可能性也更小。

**主持人：** 我们大概还有三分钟的时间，接下来一年你最兴奋的是什么？

****吴恩达：** **
有很多让我兴奋的事情，但如果让我选一个，那就是应用。我认为还有很多基础工作要做，包括更好地训练基础模型、改进技术、以及数据工程方面的大量工作。但我觉得，很多人能从中获得最大价值的地方会是应用层面。当然，我们需要继续打磨基础设施，缺乏懂得如何构建数据、基础模型、基础设施的人才，这些工作要继续进行。我认为，最终我们这个领域会以我们能否交付有用的应用来评判我们的成功。所以，我花了很多时间在应用上，但在做应用时，我有时会有很强的意见，觉得“我们真的需要把这个数据基础设施做好”或者“我们真的需要把这个编排层搞好”。不过，我认为我们已经开始看到应用的浪潮正在兴起。还有一点很有趣，大家在新闻里看到数十亿甚至几十亿美元花在
GPU 上，用来训练基础模型，很多人觉得做 AI
很贵。但事实是，由于其他人已经花费了这些钱，现在在数据基础设施和应用开发上的成本其实很低。所以我认为在应用层面，经济学上对那些想要构建和部署东西的人来说是非常有利的。

**主持人：** 太棒了，Andrew，和你聊天总是很愉快。感谢你抽时间来参加我们的播客。

****吴恩达：** ** 很高兴和你们聊，谢谢 Joe，也谢谢 Matt。

**主持人：** 好的，保重。

* * *

原视频链接：https://www.youtube.com/watch?v=0EmzuLylr7I&t=7s

 _**对了，喜欢就别忘了点赞、收藏、转发支持一下！期待在评论区听到你的观点和看法!**_****

# 往期回顾

[1、[独家视频访谈《人类简史》尤瓦尔·赫拉利：阔别六年重磅力作《智人之上：AI简史》，帮你从大历史视角看待AI对我们的巨大影]](https://mp.weixin.qq.com/s?__biz=Mzg5NTc4ODkzOA==&mid=2247493051&idx=1&sn=34b6451dd89a9d9b2ddc19f3ba925f6d&chksm=c008575ef77fde488aaab25163d72838a7e577c46988983d0d8dfffe6128dc26bc9d52d28f6e&scene=21#wechat_redirect)

[3、[演讲视频：2024年第65届国际奥数大会上，陶哲轩再次表示当前AI进展惊人，智能水平已与人类相当]](https://mp.weixin.qq.com/s?__biz=Mzg5NTc4ODkzOA==&mid=2247492830&idx=1&sn=72ae1cffdfc56f212d4b5fdbd3b37120&chksm=c008563bf77fdf2db99c1b71f962e897567f2e2380bdde257b06c0715a093e3fdfb6c881d978&scene=21#wechat_redirect)

[3、[专访OpenAI创始成员Andrej
Karpathy：相比较与人类工作，他相信当前AI技术在某些方面能力已经远超人脑]](https://mp.weixin.qq.com/s?__biz=Mzg5NTc4ODkzOA==&mid=2247493005&idx=1&sn=bb094aca1c2161bd5931ce8ce439ecad&chksm=c0085768f77fde7e455fb821b71e7cba7bd37d64e2b7df8358ab1624e92b65199757207f1ddb&scene=21#wechat_redirect)

* * *

![](https://mmbiz.qpic.cn/mmbiz_png/iaqv2tagPYAhtRhTOjz2QwH4dIlC3YUcYbaicMEwjqQqh06Yhdd7EH3r9wiaMRArLz0a6Zhx6uiaUD7hguPfbY0nAg/640?wx_fmt=png&from=appmsg)

****

**我们旨在将先进科技与创新想法完美融合!**

**想要掌握人工智能，但不知从何开始？告诉我们你的需求，学习AI让你抓住这波浪潮**

##  告别昂贵服务和缺人烦恼,再见漫长交付周期

## 无限创意,分分钟生成专业级产品

## 感受 AI 带来的全新工作体验！

 _**欢迎各大品牌方、媒体、企业和个人等**_

 _**请联系负责人微信：Milo-1101**_

 _**\--END--**_

****未经许可不得转载，务必保留公众号原文链接和公众号按钮****

