# 人类智能非AI上限！Anthropic CEO万字专访：技术持续突破，我依旧坚信AGI将在2026年实现

文章作者: AI深度研究员
发布时间: 2024-11-12 08:07
发布地: 上海
原文链接: http://mp.weixin.qq.com/s?__biz=Mzg5NTc4ODkzOA==&mid=2247494141&idx=1&sn=cda17f559e8d0c53b9735d88a928202d&chksm=c0085b18f77fd20e066e35be03bc4a428827dc9d22b21bc1ca749719e71e15d5f6859b8fe4ae#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/iaqv2tagPYAiaL7PlKHjBa3aqEqSI51kMvtiawEPMStAvJO4vFz1GXECJqHWB0tNkXBYAJyiabakjDwR3go5MyZ3wQ/300

**（关注公众号并设为🌟标，获取最新人工智能资讯和产品）**

全文19,000 字，阅读约需38分钟

![](https://mmbiz.qpic.cn/mmbiz_png/iaqv2tagPYAiaL7PlKHjBa3aqEqSI51kMvKo7GFOOfmpn3ztg4xEaVKuyjMaM5rm6kws0L2j9pNTBP7rE2Q57N7w/640?wx_fmt=png&from=appmsg)

> ——AI的扩展定律将继续保持不变，其中蕴含着许多我们现在的理论还无法解释的神奇力量
>
> ——所谓的天花板可能是由人类自身的制度或结构所限制的，而非智能本身的极限。因此，在AI发展的天际线之上，仍然存在广阔的探索空间
>
> ——到 2027 年，将有价值 1000 亿美元的人工智能数据中心建成，他看好强大的人工智能即将到来，因为我们即将达到博士级别的AI代理

近期，著名播客主持人 Lex Fridman 在结束对马斯克的采访后，再度造访硅谷，**与 Anthropic 的首席执行官兼联合创始人 Dario
Amodei 进行了一场近三个小时的深度对话。** 在这场备受关注的访谈中，Amodei 分享了他对 AI 发展的独特见解和展望。

“2017年是一个转折点，"他回忆道，"当我看到 GPT-1
的研究成果时，我突然意识到语言可能也是一个可以应用这种方法的领域。我们可以用数万亿字的语言数据进行训练。"他指出，从最初仅需一到八块 GPU
就能完成的训练任务，如今已经发展到需要调用成千上万块 GPU，未来甚至可能达到数十万块的规模。这种跨越式的进步，让 Amodei
坚信：通过不断扩大模型规模，AI 终将具备处理各类认知任务的能力。

更引人注目的是，Amodei 做出了一个大胆的预测：在未来两到三年内，超级智能 AI 很可能会出现，而且这类 AI
的规模有望迅速扩展到百万级别。"如果我们外推现有的发展曲线，"他解释道，"去年模型的能力已达到本科生水平，而今年更是接近博士生水平。在计算机使用、图像生成等诸多领域，AI
已经展现出惊人的潜力。按照目前的发展速度，我们很可能在 2026 年或 2027
年达到一个显著的能力高峰。尽管他也承认存在百年内都无法实现的可能性，但他认为，随着技术障碍的不断突破，这种悲观预期的可能性正在逐渐降低。

# 采访要点整理

## 1、 扩展定律（Scaling laws）

关于扩展定律的历史及其现状（Scaling
laws），我只能根据自己的经历来描述。我已经在人工智能领域工作了大约十年，这方面的想法很早就引起了我的注意。我在2014年末加入了人工智能领域，当时和Andrew一起在BYU工作，最初我们主要研究语音识别系统。那个时候，深度学习刚刚兴起，取得了许多进展，但大家都认为我们还缺少成功所需的算法，只是触及了很小的一部分，算法上还有很多待发现的地方。那时，许多人认为我们还没有找到能够匹配人类大脑的模型。

可以说是“新手的运气”吧，作为一个刚入行的人，我看着我们用于语音识别的神经网络，尤其是递归神经网络，心里想：“如果把它们做大一些，加更多的层，并且扩展数据呢？”我把它们视为可以调整的参数，然后注意到模型随着数据量的增加、模型规模的扩大和训练时间的延长，性能不断提升。当时我没有精确地测量这些效果，但与同事们一起得出的一个非正式结论是：更多的数据、计算和训练能让模型表现得更好。

最初我是觉得也许扩展定律只是语音识别系统的一个特例。但到了2017年，当我看到GPT-1的结果时，我突然意识到语言可能也是一个可以应用这种方法的领域。我们可以用数万亿字的语言数据进行训练。当时我们用的模型还很小，只需一到八块GPU就可以训练，而现在，我们训练时动用的是成千上万、甚至未来可能是数十万块GPU。当我看到这些变化时，心中逐渐有了信念：如果我们不断扩大模型，便能处理各种认知任务。在每个阶段，总会有不少质疑和挑战。比如，有人说语言模型可以生成语法正确的句子，但无法理解语义；也有人认为模型只能生成通顺的句子，但无法生成有逻辑的段落。还有人担心我们可能会用尽数据资源，或者数据质量不够高，甚至认为模型无法真正推理。每次我们都设法找到解决方法，或者直接通过扩展来绕过问题。我并不确定未来几年是否会像过去十年那样，但我见证了足够多次这种现象的发生，让我相信缩放的力量将继续存在。

所谓的扩展具体指的指的是更大的网络、更长的训练时间和更多的数据。它们几乎像化学反应一样，你需要将这三者成比例地一起扩大。如果只扩大一个，而不增加其他因素，那么反应就会停止。但如果所有因素都同步扩大，反应就能继续进行。

我觉得为什么更大的网络和更多的数据会导致更智能的模型，是因为我之前在生物物理领域工作，本科学的是物理学，研究生学的是生物物理学。我想到了物理学中的一些概念，比如“一除以F的噪声”（1/f
noise）和类似的分布。很多自然过程中会产生一种高斯分布，如果我们把许多不同的自然过程叠加起来，就会得到这种分布。同样的，语言中也有类似的模式，比如某些词比其他词更常见，句子结构、段落主题等都层层递进。这种层级结构是否意味着随着网络的增大，它们能捕捉到更多这种分布中的模式。一个小的网络只能理解简单的语法结构，比如句子中需要动词、形容词和名词。但如果网络稍大一些，它便能理解这些词之间的关系，生成更加有意义的句子。随着网络进一步扩大，它能生成连贯的段落，逐渐理解更复杂的语义结构。

这种递归结构使得随着网络规模的增大，模型首先捕捉到一些简单的关联和模式，然后逐渐捕捉到更多的细节。就像物理学中的“一除以F噪声”现象一样，在电阻器等物理过程中出现平滑的长尾分布。你能想象当网络规模增大时，它可以捕捉到更多这样的分布，而这种平滑性也会反映在模型的预测和表现上。语言是一种进化的过程。我们已经发展出常用词和不常用词、常用表达和不常用表达，这些语言模式是数百万年来随着人类一起进化的。我的猜想，虽然这只是纯粹的推测，是这些模式的分布可能呈现出一种长尾分布。这不仅是长尾的问题，还有概念层级的高度。网络越大，它能够捕捉的概念层级就越高。比如说，如果网络很小，它只能抓住最常见的模式，比如“句子需要有动词、形容词和名词”等基本结构，但它无法决定这些词的具体内容或逻辑关系。如果把网络稍微做大一些，它会更好地理解这些词之间的关系，接着又可以更好地理解句子，甚至能理解段落中的复杂结构。

## 2、LLM扩展的极限

我认为我们中没有人知道大型语言模型（LLM）扩展的极限在哪里？它的天花板在哪里？现实世界的复杂程度有多高？有多少知识可以学习？我的强烈直觉是，至少在人类水平以下是没有上限的。我们人类能够理解这些模式，所以我认为如果我们继续扩展这些模型，继续开发新的训练方法并进一步放大规模，至少能够达到人类的水平。然后就会出现另一个问题：我们能理解的程度是否能超过人类？我猜这个答案会因领域而异。比如在生物学领域，理解生物学的复杂性对人类来说是很困难的。你去斯坦福、哈佛或伯克利大学，会看到整个人员在研究免疫系统或代谢途径，但每个人都只能理解其中一小部分，而且很难将他们的知识与他人结合。所以我觉得在这个领域，人工智能可以有很大的提升空间。

在人类世界，有些问题可能是不可逾越的，而另一些问题则较容易解例如，材料科学或人类冲突之类的问题，有些问题可能很难解决，甚至无解。就像语音识别一样，有些极限是我们无法突破的。所以在某些领域，可能会有非常接近人类水平的天花板，而在其他领域，天花板可能离我们还很远。我认为我们只能在构建这些系统时逐步发现这些极限。

所谓天花板可能是由人类的制度或结构导致的，而不是智能的限制。在某些情况下，理论上技术可以快速变化，比如我们在生物学上的发明。但是，考虑到我们有一个必须通过的临床试验系统，用于确保将这些技术应用于人类。这其中有一些是必要的，也有一些是繁琐的官僚流程。挑战在于，很难区分哪些是必要的，哪些是繁文缛节。我个人认为，在药物开发方面，我们的进展太慢，太保守，但毫无疑问，如果错误判断也可能威胁人类的生命。

当被问及可能遇到的扩展法则极限（在达到人类水平之前）是由计算力、数据还是其他因素如创意所限制时，指出目前学界普遍认为数据不足可能是一个关键瓶颈。他解释说，尽管互联网上存在数以千亿计的文字数据，但这些数据面临着质量和多样性的挑战。许多内容是重复的，或者是为了搜索引擎优化而创建的，而且未来可能会包含大量AI生成的文本，这种数据生成和获取方式本身就带来了根本性的局限。所以数据生成可能是一种解决方法吧。我们以及其他一些公司正在研究如何生成合成数据，比如使用模型生成更多的同类数据，甚至从头生成数据。类似于DeepMind的AlphaGo
Zero，他们通过自我对弈而不是使用人类的数据将AI训练到超越人类水平。另一种方法是使用推理模型，比如链式思考（Chain of
Thought），模型可以反思自己的思考方式。这也是一种与强化学习相结合的合成数据形式。我认为通过这些方法之一，我们可能绕过数据限制。此外，如果模型在扩展过程中突然不再提升，那可能意味着需要新的架构设计。

当在讨论计算能力限制，尤其是涉及到建造大型数据中心的高昂成本时，发现了一个有趣的发展趋势。目前，大多数前沿的模型公司的运营规模大约在10亿美元左右，上下浮动可能在三倍范围内。这些都是现有的或正在训练中的模型。展望未来，预计明年的规模将达到几十亿美元，到2026年可能突破100亿美元，而到2027年甚至可能会出现价值1000亿美元的数据中心集群。我认为这种规模扩张是必然的，因为有多种力量在推动这一进展。不过，如果即便达到1000亿美元的规模仍然不足以突破瓶颈，那么我们要么需要进一步扩大规模，要么就必须开发出更高效的方法来调整这个进展曲线。目前虽然可能有一些新的优化方法或技术能帮助我们突破当前的瓶颈，但还没有确凿的证据表明必须采取这种改变。对强大的人工智能快速发我展持乐观态度，原因是如果继续沿着当前的曲线外推，似乎很快就能达到人类水平。例如一些新的模型，尤其是一些推理模型，已经达到了博士或专业级别的能力。比如我们最新发布的Sonet
3.5模型，它在软件工程任务集合SBench上得分约为50%，而在年初时，这一任务的最先进模型得分仅为3%-4%。短短10个月内，我们从3%提升到了50%。我认为，再过一年，或许我们可以达到90%，甚至更快。在数学、物理和生物学等领域，我们也看到了类似的进展。如果我们继续沿着这条曲线前进，几年来这些模型将能够超越人类的最高专业水平。

## 3、 与 OpenAI、Google、xAI、Meta 的竞争

作为Anthropic的一员，我想分享一下我们公司的愿景和一些重要的研究方向。我们的使命是推动整个AI领域向好的方向发展，这体现在我们提出的"向上的竞赛"理论中。这个理论的核心不是简单地做"好人"，而是通过树立榜样来影响和推动其他参与者做正确的事。这种理念具体体现在我们的研究工作中。以我们的共同创始人之一Chris
Olah为例，他是机制解释领域的开创者之一，致力于理解AI模型的内部工作原理。尽管这类研究暂时看不到直接的商业价值，但我们仍在解释性方面投入了大量精力并公开发表研究成果。

在探索模型内部机制的过程中，我们已经取得了一些令人惊喜的发现。虽然模型的内部结构并不是为了让我们理解而设计的——就像人类大脑或生物化学系统一样，它们并不是为了让人类能轻易"打开盖子"就看懂。但通过深入研究这些系统的内部结构，我们发现了许多值得探索的领域，比如"归纳头"和"稀疏自编码器"等结构，这些发现让我们能够在网络中找到对应着清晰概念的方向。举个具体的例子，在"金门大桥"实验中，我们发现了一个特殊的方向，当激活这个方向后，模型会特别关注与金门大桥相关的特征。无论询问什么问题，模型都会将回答引向金门大桥，这种表现让模型展现出一种似乎具有情感的特质，使其在很多人眼中显得更加人性化和富有个性。这种研究虽然还处于早期阶段，但正在朝着提升AI安全性的方向稳步前进。

## 4、Claude AI的多个新版本

今年三月， 今年Anthropic发布了多个新版本，包括Claude
3、Sonet和Haiku。我们认为，不同的需求需要不同类型的模型：一种是强大但稍慢的模型，用于更复杂的任务；另一种是快速、便宜的模型，用于需要迅速响应的场景。比如编程、头脑风暴、创意写作等需求更适合功能强大的模型，而在业务场景中，比如网站互动、报税、合同分析等，则需要速度更快、覆盖面更广的模型。

那“Haiku”是什么呢？简单来说，它是一种短小的诗歌形式，所以Haiku是小型、快速且便宜的模型。当时发布时，它的智能水平相对于它的速度和成本来说令人惊讶。而Sonet是中等规模的模型，它比Haiku更聪明，但也稍微慢一些，成本也更高。Opus则像是“巨作”，它是当时最大的、最智能的模型。这就是最初的设计思路。我们的想法是每一代新模型都应该在智能、速度和成本之间进行平衡。因此，当我们发布Sonet
3.5时，它的成本和速度大致与Sonet 3相同，但智能水平提升到了比最初的Opus 3还要高，尤其是在编码方面，整体上也有所提高。我们最新的Haiku
3.5基本上和旧的Opus 3相当，所以目标就是不断提升性能曲线。接下来可能会推出Opus 3.5。

从Claude Opus
3到3.5之间会有一段时间间隔。模型开发包括多个阶段。首先是预训练，即语言模型的基础训练，这个过程非常耗时，通常需要成千上万，甚至数万的GPU、TPU等加速器芯片，通常需要数月时间。此外，还有后期训练阶段，包括从人类反馈中学习的强化训练以及其他类型的强化学习，这个阶段也越来越重要，也需要精心调试以确保效果。然后，模型会通过我们的一些早期合作伙伴进行测试，接着在内部和外部进行安全测试，特别是检测模型在极端情况下的表现。根据我们的“责任扩展”政策，我们会与美国和英国的AI安全研究所及其他特定领域的第三方测试机构合作，评估模型在化学、生物、放射和核等领域的潜在风险（即CBRN风险）。虽然目前我们认为这些风险并不显著，但每一代新模型都会重新评估，以确保它们不会接近危险能力。有很多步骤需要完成，包括让模型的推理在API上顺利运行。我们一直在努力让这些流程更高效，比如保持严格的安全测试，同时尽量自动化和快速化，就像制造飞机一样，既要确保安全，又要让过程尽可能简化。这个创意与效率之间的平衡非常关键。

Anthropic在工具化方面做得非常出色，所以可能在软件工程方面面临的挑战更多是为了确保与基础设施的高效交互。你可能会惊讶，开发这些模型中的许多挑战都归结于软件工程和性能工程。从外界来看，可能觉得我们获得了某种“顿悟”式的科学发现，但实际上，许多时候它们取决于非常细致甚至有些无趣的细节。这些工具化的改进非常重要，虽然我不能评论我们是否比其他公司在工具上做得更好，但这确实是我们非常关注的一项工作。

从Claude 3到Claude
3.5之间在每个阶段，我们都试图同时改进所有方面。不同的团队在各自的领域推进工作，就像接力赛一样，每次新模型推出时，我们会将所有这些进展汇集起来。因此，有时候旧模型的数据偏好会用于新模型，尽管在训练新模型时用新数据会有更好的效果。此外，我们还采用了“宪法式AI”方法，不仅依赖于偏好数据，还会让模型在后期训练中进行自我对比，这种自我训练的方法每天都在改进。在讨论Sonet
3.5的性能提升时，我特别想谈谈在编程领域的突破性进展。这不仅体现在基准测试的分数上，更重要的是体现在实际应用中。我们公司的工程师们有一个很深的感触：在此之前，无论是我们还是其他公司的代码模型，对于专业工程师来说用处都比较有限，主要是面向初学者的。但Sonet
3.5是第一个真正让专业工程师感到惊喜的模型，他们发现它确实能帮助他们节省几个小时的工作时间。这种进步令人振奋，而且新版本的Sonet在这方面表现得更加出色。这样的性能提升并非源于单一因素，而是预训练、后期训练（不仅限于强化学习）以及各种评估方法共同作用的结果，是一个全方位的提升过程。

基准测试方面，SBench实际上是模拟真实场景的基准测试，代码库处于一种现有状态，而模型需要实现某种语言描述的功能。我们有内部基准测试，让模型自由发挥，看看它能在多大程度上完成任务。我们观察到，模型的成功率从最初的3%提升到了大约50%。如果我们能够在不针对性训练的情况下将这一基准提升到100%，那将代表编程能力的真正提高。

我们确实计划推出Claude 3.5
Opus，但目前还不能给出具体的发布日期。快速的发布节奏反映了我们对产品迭代的期望。说到模型命名，这确实是个挑战。在早期，当模型主要依赖预训练时，我们可以根据不同尺寸进行统一训练和命名。但随着训练时间的增加，以及模型在预训练和后期训练中取得的进步，命名方案变得愈发复杂。现在的模型存在不同的权衡，有些注重推理速度，有些则更关注成本效益，这使得包括我们在内的所有公司都面临命名困境。虽然我们引入了Haiku和Sonet等名称，但要保持简单和清晰确实不容易。关于用户体验，新版Sonet
3.5与2024年6月版确实存在差异。这涉及到一个更深层次的问题：模型的许多特性是基准测试无法完全体现的。有些模型表现得更礼貌或直接，反应速度快慢不一，还具有不同的个性特征，比如Golden
Gate
Claude就展现出独特的风格。我们有一个专门的团队，由Amanda领导，致力于研究"Claude个性"。然而，即便如此，模型仍然会表现出一些我们始料未及的特性。有时即使对同一个模型进行上万次交互，也难以发现某些特征。这就引出了一个更具挑战性的问题：我们如何选择哪些个性特征是我们希望模型具备的，而哪些又是我们希望避免的。

## 5、用户对Claude的批评

关于Claude的批评，有一个很有趣的现象，很多用户抱怨说Claude变“笨”了。这个问题不只是针对Claude。我发现每个大公司的基础模型都遇到过类似的抱怨，比如GPT-4和GPT-4
Turbo。首先，模型的实际权重——即模型的“核心大脑”——在没有新模型引入的情况下是不会更改的。随机更换模型版本不太实际，且改变模型权重会导致难以控制的后果。我们有一套完整的流程来修改模型，进行大量测试和用户反馈，因此我们不会在没有通知的情况下更改模型的权重，这在现有设置下也没有意义。不过，有时我们确实会进行一些调整，比如进行A/B测试，但通常是在新模型发布前的短时间内进行，影响范围也很小。另外，系统提示（system
prompt）偶尔会改变，虽然它可能会对模型的表现产生一定影响，但不太可能导致模型变“笨”。此外，用户抱怨模型“变笨”或“更具限制”是很常见的现象。这不是说用户在想象，而是模型确实很复杂，会对小小的交互变化表现出不同的结果。模型的这种敏感性反映了我们对模型运行方式的理解还有很大不足。另一个可能的原因是，用户在新模型推出时会觉得很兴奋，但随着时间推移，开始更敏锐地察觉到模型的限制。

这些反馈让我联想到飞机Wi-
Fi的例子：当它刚推出时人们觉得神奇，现在却抱怨它变慢了，甚至有人认为这是"阴谋论"。类似地，有用户在Reddit上抱怨Claude像"婆婆"一样爱灌输道德观，或者总是过度道歉。对此，我认为需要从几个角度来看。首先，社交媒体上的抱怨往往与实际数据存在偏差。大多数用户最关心的其实是模型的实用性，比如能否在编程时写出完整代码，或在特定任务上有更好的表现。当然，确实有用户会对模型在某些情况下表现出的不合作态度、过度道歉或一些"烦人的语言习惯"感到不满。

然而，控制模型的行为并非像表面看起来那么简单。我们不能简单地命令模型"少道歉"，因为即使我们在训练数据中减少道歉的示例，可能会导致模型在其他场合表现得过于冷漠或自信。举个例子，如果我们过度压缩冗余表达，模型在编写代码时可能会简单地说"代码在这里结束"，而不是完整输出所有内容。这种情况的出现并非出于节省计算资源的考虑，而是因为模型对这些细微的控制往往会产生难以预料的反应。而且在控制模型行为方面存在很多挑战。因为控制模型行为非常难，就像“打地鼠”游戏一样，调整一个方面可能会无意间改变另一个方面。正因为这些系统很难完全控制，我对未来AI系统的“宏观对齐”尤为关注。如今，我们已经看到，如果改善一个方面，往往会牺牲另一个方面。我认为这是未来AI控制问题的一个早期征兆。如果我们能够在今天这个相对可控的环境中解决这些问题，将来面对具有高度自治能力的模型时会更加得心应手。我们如何在模型既能够拒绝一些不合适的请求，又能够协助用户完成他们所需的任务之间找到平衡。这确实是个多维度的问题，塑造模型的个性特征确实非常困难，而且我们目前的效果还不完美。尽管我们在这些方面的表现比其他AI公司更好，但离理想状态还很远。如果我们能够在这个问题上做好，那么在未来控制模型时，我们会更有信心，比如确保模型不会在不恰当的任务上变得过于自主，但又能满足用户的需求。这确实是一个复杂的平衡。

收集用户反馈的最佳方法时，我们采取了多层次的策略。在Anthropic内部，我们会进行所谓的模型"测试冲击"。有将近一千名员工参与其中，他们会在各种场景下"挑战"模型的表现。我们建立了一系列评估标准，比如检查模型是否会不恰当地拒绝用户请求。这让我想起一个具体的例子：我们曾经专门为模型过度使用"Certainly（当然）"这个口头禅设立了评估标准，因为在某个时期，模型确实频繁地用"当然"来回应各种问题。这个过程很像是在玩"打地鼠"游戏，每次我们添加新的评估标准时，都需要考虑到已有的标准，现在我们已经累积了数百项这样的评估。但我们发现，没有什么能够完全替代真实的人类互动体验。因此，整个过程实际上很像常规的产品开发流程。除了内部测试，我们有时会进行外部A/B测试，也会聘请合同工来与模型进行交互。虽然这些方法不能完全消除所有问题，但确实帮助我们减少了模型在某些情况下做出不合理拒绝的频率。我们一直在寻找一个完美的平衡点：既要防止模型做出明显不当的行为，又要避免它在简单问题上产生愚蠢的拒绝反应。这确实是个具有挑战性的任务，但我们每天都在朝着这个目标前进并取得进步。

## 6、AI安全级别分类

关于AI安全级别，包括“负责任的扩展政策”（Responsible Scaling
Policy）和AI安全级别标准（ASL级别）。我对这些模型的潜在好处感到兴奋，但我同样担心它们带来的风险。我所担心的风险主要集中在两个方面。第一个是“灾难性滥用”风险，指的是模型可能被滥用于网络攻击、生物、放射、核等领域，如果处理不当，可能会造成严重伤害甚至威胁生命。第二个风险是“自主性风险”，即随着模型的能力不断增强和应用范围的扩展，它们可能会被赋予更多的自主权，比如编写整个代码库，甚至未来有可能运营整个公司。这种情况下，如何确保模型按预期行事而不偏离轨道是一个难题。这些早期迹象表明我们需要更加精确地划分模型的行为边界，以防止在控制一个方面时产生其他不良影响。我们的“负责任的扩展政策”旨在应对这两种风险。每当开发新模型时，我们会测试它在这些领域中的能力，作为早期预警系统，以便在风险接近时能够及时发现。最新版本的RSP政策中，我们增加了模型在自主研究方面的能力测试，因为一旦AI能够进行AI研究，它们便真正具备了高度的自主性。

RSP基本上建立了一种“如果-
那么”结构，也就是如果模型达到了某个能力门槛，那么我们会对它施加一组安全和安保要求。今天的模型被归类为ASL2模型，那么ASL1适用于那些明显不存在自主性或滥用风险的系统，比如一个象棋AI“深蓝”，它显然只能下棋，不会被用来进行网络攻击或控制世界。ASL2是今天的AI系统，我们认为这些系统还不够聪明，无法自主复制或执行一系列复杂任务，也无法提供关于CBRN（化学、生物、放射和核）风险的实质性信息——它们的信息来源不超出谷歌等搜索引擎的水平。ASL3是我们设定的下一个门槛，当模型具备足够的能力，能够帮助非国家行为体（即非政府组织或个人）提升其破坏性能力时，我们会采取额外的安全措施，防止模型被盗用，并针对网络、生物和核等特定领域设置过滤器。ASL4则适用于模型能够显著增强国家级行为体能力的情况，甚至可能成为主要的风险来源，同时在自主性方面，ASL4意味着模型能够加速AI研究，并在某种程度上自行提升能力。最后，ASL5是最高级别，当模型能力超过人类，能够独立完成这些高风险任务时，我们就进入ASL5的范畴。我们设计“如果-
那么”结构，旨在在当前风险较低时避免过度反应，同时确保当风险显现时，我们能够迅速应对。

在我们公司内部，ASL3的触发时间一直是个热议的话题。我们正在积极准备ASL3的安全和部署措施，并已经取得了显著的进展。根据目前的发展趋势，我预计我们可能会在明年达到ASL3的门槛，甚至有可能在今年就触发这个标准，而不是此前预期的2030年。在ASL3阶段，模型并不具备真正的自主性，我们主要关注的是安全性和特定领域的过滤。然而，当我们进入ASL4阶段时，情况会变得更加复杂。因为此时模型变得足够聪明，可能会隐瞒自己的能力，甚至"伪装"成不那么强大的版本。为了应对这种情况，我们可能需要运用"机制解释性"或"隐性思维链"等技术，深入检查模型的内部运作，以确保其表现的真实性，防止"作弊"行为。

特别值得注意的是，ASL3阶段的主要风险来自于人类的滥用，而到了ASL4阶段，我们将同时面临来自人类和模型自身的风险。随着模型变得越来越智能，社会工程确实成为一个潜在的威胁。我们已经观察到了人类操纵和蛊惑的案例，这意味着未来的模型也可能具备类似的能力，进而影响工程师或其他人员。这无疑是我们在开发更强大模型时需要密切关注的重要问题。

## 7、AI独立操作电脑

Claude现在已经可以执行一些代理任务，比如“计算机使用”功能。这是非常令人兴奋的进展。你可以赋予Claude一个任务，它会采取一系列行动来完成任务，通过截图等方式访问你的计算机。这种能力的提升确实为模型提供了更强的代理性，使得它们能够更独立地解决复杂任务。

计算机操作功能原理其实相对简单。Claude早在今年三月发布Claude
3时就已经具备了图像分析能力，可以通过文本响应图像内容。我们现在加入的新功能是，它可以分析计算机的截图，并且可以提供屏幕上的点击位置或键盘按钮的操作建议。经过一些额外的训练后，Claude在这方面表现得相当不错，这是一个很好的泛化示例。我们可以给模型一个截图，告诉它点击位置，然后再给下一个截图，如此循环下去，模型几乎就像在进行“3D互动”，可以执行多种任务，比如填写表格、浏览网站、打开不同操作系统中的程序等。虽然理论上这些任务可以通过API实现，但使用截图大大降低了操作门槛，尤其对于那些不便于使用API的用户，屏幕截图是一种更通用且简单的接口。我相信，随着时间推移，这项功能会帮助更多人跨越技术障碍。当然，当前的模型还存在很多不足之处，比如可能会点击错误。所以我们在博客中也提醒用户，不要让它在你的电脑上长时间自动运行。我们目前优先在API形式中发布，而不是直接让消费者控制他们的电脑。这也是为了在模型能力还相对有限的时候探索其安全应用，以防止滥用。未来要让它更有效，我们打算继续大力投资来提高模型的可靠性。就像我们在其他基准测试上看到的进步一样，我们希望在一年左右的时间内，模型在计算机操作上能达到80-90%的可靠性——接近人类水平。我认为，继续采用当前的训练方法，并进行适当的强化，就可以大幅提升模型的表现。

这种功能确实提供了很大的潜力，但也带来了风险。不过我认为，计算机操作并不属于新的核心能力，比如CBRN（化学、生物、放射、核）或自主性那样的能力，而是扩大了模型现有能力的应用范围。在我们的RSP政策中，这项能力本身并不会显著增加风险，但随着模型智能的提升，这种能力可能会变得更具风险，特别是在达到ASL3和ASL4级别时。这也带来了新的攻击向量，比如通过屏幕内容进行“提示注入”。随着功能的提升，恶意攻击者可能会利用它进行干扰，比如在网页上注入广告或恶意内容。我们考虑了很多可能的攻击方式，比如垃圾邮件拦截。这种“骗术”就像古老的骗局一样无处不在，每种新技术都会被一些小罪犯用来干些恶意的勾当。我们在训练阶段对模型进行了沙盒化处理，没有直接接入互联网，因为在训练期间模型的行为和政策可能会变化。如果部署模型时确实需要沙盒化，可以在外部设立“防护栏”，例如限制模型从用户计算机向外部服务器传输数据。不过，当我们达到ASL4级别时，沙盒保护可能也无法起作用，因为那时模型可能足够智能，可以突破任何限制。

关于ASL4，我们需要考虑机制解释性问题，比如如果我们打算为它设置沙盒，那么这个沙盒需要在数学上具有可靠性，但这已经远远超出了当前模型的范畴。但是我们谈论的是一种“建造一个ASL4
AI系统无法逃脱的盒子”的科学。我认为，试图通过限制来控制一个可能不对齐的模型并不是最好的办法。与其构建一个封闭的“盒子”，不如设计模型时从一开始就确保它的对齐性，或者建立一个机制，使我们可以实时观察模型内部，验证其属性，并进行迭代改进。我认为，控制“坏”模型的解决方案远不如设计出“好”模型来得好。

## 8、和优秀的团队工作

我在OpenAI工作了大约五年，最后几年担任研究副总裁。2016或2017年左右，我开始对“扩展假设”有了更坚定的信念，当时Ilia向我提出了一个理念：“模型只是想要学习。”这句话非常简练，却解释了我所见过的成千上万种现象。从那时起，我就坚信，如果你给模型正确的优化方向，它们会自动去解决问题。后来，我和我的团队在GPT-2、GPT-3和“人类反馈强化学习”上做了很多尝试，致力于安全性和可解释性方面的工作，这些年是我和许多同事推动OpenAI研究方向的重要时期。

关于那么是什么让我决定离开OpenAI的呢？在OpenAI的这些年里，我越来越相信扩展假设和安全性的重要性。OpenAI在扩展假设上逐渐认可了我的想法，但在安全性方面，我认为需要一种更加深刻和原则性的方式来落实。网上有很多关于我们离开的传言，比如因为不满与微软的合作或商业化，这些都不准确。实际上，我们参与了GPT-3的开发，这正是后来商业化的模型。真正的原因是我们对“如何安全地开发并引入强大的AI”有自己的看法。在这个过程中，我意识到，与其在公司内部为理念争论，不如和一群志同道合的人一起开创属于自己的愿景。如果我们能够吸引人们的认可，形成具有真正价值的实践，那么其他公司也会自然而然地效仿。最终，重要的不是哪家公司胜出，而是整个行业在积极竞争中建立起更好的标准。我们害怕的是“向下竞争”，而“向上竞争”则让整个行业趋向于更好的平衡状态。如果通过我们的努力，行业形成了良性竞争，无论谁最后领先都不是重点。重要的是，大家都在遵循良好的实践，这才是我们希望达成的目标。

我们可以采用良好的实践，认为它是有价值的，并且希望我们也能效仿。我觉得这抽象了哪些公司会赢得信任的问题。这些纷争其实非常无聊，真正重要的是整个生态系统，以及如何让它变得更好，因为这将约束所有参与者。Anthropic可以看作是一个基于AI安全性清晰愿景的实验。虽然我们可能在途中犯了不少错误，但这并不意味着我们要放弃。世界上没有完美的组织，但我们会尽力去做好。我们尽力在行业中率先采用更多良好的实践，并且迅速吸纳他人提出的好想法。我认为这样的动力能使得行业向更高的标准发展。Anthropic和其他公司都可能成功，最终谁更成功并不重要，重要的是我们能让整个行业的激励机制对齐。这不仅是“向上竞争”，还包括我们的RSP（负责任的扩展政策）和有选择的监管。

“人才密度胜过人才总量”，这句话我越来越觉得正确。假设你有一个由100个超级聪明、极具动力并对公司使命高度认同的人组成的团队，或者一个由1000人组成的团队，其中200人非常优秀，但剩下的800人只是一般的科技公司员工。你会选择哪个？虽然后一组的人才总量更大，但问题是，如果每个才华出众的人都看到自己周围的同事也很优秀，那将形成一种氛围，每个人都会更愿意相互信任并全力以赴。如果公司人数达到了成千上万，却缺乏选择标准和统一的方向，那将需要大量的流程和防护措施来确保顺利运行。Anthropic目前有将近1000人，我们努力确保其中大部分人都才华出众。我们从物理学家到资深研究人员，再到软件工程师都极为挑剔，确保大家都有同一个愿景，因为这种信任和奉献精神才是公司最大的优势。我认为优秀的AI研究人员或工程师最重要的素质是开放的心态，特别是在研究方面。这听起来容易，但实际上很难做到。以扩展假设为例，当时大家都在讨论算法不够好，我却选择以新的视角看待问题。我想，也许我们可以简单地增加神经网络的参数数量，看看到底会发生什么。事实上，这种基础的科学思维可能会带来变革。保持开放的心态，愿意以新的视角观察问题，这对推动整个领域的发展至关重要。

当谈到如何成为一名优秀的AI研究员，尤其是对那些年轻且对AI充满热情、希望对世界产生影响的人，我的建议是要尽快开始实际接触和操作这些模型。这个建议在现在可能显得很基础，但要知道就在三年前，人们还主要专注于阅读强化学习的论文等理论内容。如今，随着各种API和模型的广泛可用，实际操作经验和体验上的知识变得越来越重要。这些模型都是全新的产物，没有人能够完全理解它们，因此与模型的直接互动可以带来很多意想不到的启发。另外，我建议年轻研究者要勇于尝试一些尚未被充分探索的研究方向，比如机械解释性这样的新兴领域，因为这些领域不仅充满创新机会，而且潜力巨大。如果有人想在AI研究领域做出贡献，别总是追随热门方向，想象五年后会兴起的那些方向，很多创新可能就在那里。某些领域目前可能只有少数人在研究，而并没有成千上万人涌入，所以机会依然很多。我的建议就是朝着“冰球要去的方向滑”（skate
where the puck is going），别害怕选择一个不那么流行的方向，这才是最重要的。

## 9、模型训练后

我们其实不能完全准确地衡量预训练（pre-training）和后训练（post-
training）这些因素在Anthropic的Claude模型中起到了什么作用。我们尝试了区分它们的方法，但并不完美。我们在RL方面表现得相当出色，这并不是因为我们有其他公司没有的“魔法方法”，而是我们可能更擅长于长时间运行、更高质量的数据过滤和结合多种方法的实际操作。说到底，这更像是设计飞机或汽车：更多是思维过程和操作技巧，而非某个特定的发明。关于RLHF，它的成功秘诀是如果追溯到扩展假设，只要你在训练中投入足够的计算资源，就能获得预期的结果。而RLHF擅长让模型更符合人类的期待，它能让模型产生人类短时间内希望得到的反应。RLHF并不能直接让模型变得“更聪明”，它只是“架起了模型和人类之间的桥梁”，帮助模型更清晰地表达自己。但是我不认为RLHF让模型更聪明，但它确实让模型在“智能”上显得更贴合人类的需求。它填补了人类和模型之间的沟通差距。RLHF让模型能够理解和回应人类想要的东西，但并不是唯一的方法。未来可能会有不同的强化学习方法来帮助模型在推理、操作和技能开发上变得更出色。

在成本方面，预训练还是最昂贵的但我可以预见未来后训练的成本可能会超过预训练。如果达到这种情况，那么在人力与计算的平衡上，依赖于某种可扩展的监督方法，比如辩论等方式，或许会更具成本效益。

## 10、AI的积极未来

我和Anthropic投入了大量时间去研究AI的风险，我们的目标是引导行业向更高的标准发展，这要求我们建立很多新能力。能力的提升是令人兴奋的，但我们也要积极应对风险。为了确保这些正面结果的实现，我们并不是因为害怕技术才去防范风险，而是因为一旦成功克服了这些障碍，未来将带来极大的好处。我注意到，如果我们只谈风险，思维也会被风险所局限。因此我觉得很有必要去设想如果事情进展顺利的情况——所有努力去规避风险的原因，并不是因为我们害怕技术，而是希望能够享受技术所带来的巨大福祉。

我们要努力规避这些风险并不是因为我们想减缓进程，而是如果我们能够成功克服这些风险，跨越这道障碍，那么在障碍的另一边，有着无数美好的未来等着我们。这些目标值得我们奋斗，真正能够激励人心。我注意到，有许多投资者、风投、以及AI公司在讨论AI的积极影响，但他们很少具体说明这些影响究竟是什么。许多人只是描绘出一些未来的美好城市，传达出一种“加速发展”的氛围，却没有明确说明真正令人兴奋的是什么。因此我觉得，从风险管理的角度来详细阐述AI的潜在好处，可能会更有价值。这样不仅有助于大家达成共识，也让人们明白这并不是“末日论者”与“加速主义者”之间的对立，而是如何实现人类社会所渴望的积极成果。拥有AI快速发展的全面理解后，我们更能珍视其带来的益处，同时严肃对待那些可能让我们偏离目标的风险。

## 11、强AI实现时间线

我更喜欢使用“强大的AI”这个。之所以避免使用“AGI”这个词，是因为它已经失去了具体含义。就像在1995年时，有人说“超级计算机”能让我们完成更多的事情，比如基因测序。尽管计算机确实在变得越来越快，但“超级计算机”只是一个模糊的词语，并没有一个明确的界限。所以在我看来，“AGI”同样是一个模糊的词。如果“AGI”只是指AI逐渐变得更聪明、更接近人类水平，那么我可以接受这个概念；但如果将“AGI”看作是某种独特的、离散的存在，那么它实际上只是一个流行语而已。我认为，强大的AI不仅在智力上超越了人类，甚至能够在各个领域超越诺贝尔奖得主的水平。它拥有跨模态的能力，能够控制工具和实验设备，执行各种任务。更重要的是，这种AI可以快速扩展——今天我们可能只部署了一个模型，但很快我们可以部署成千上万的实例。如果我们能够实现这样的AI，很多非常困难的问题将能快速得到解决。当然，要准确判断这些问题的解决速度并不容易，因为存在着两个极端的观点。

这里有两个极端。第一个极端观点认为，一旦AI的智力水平超越人类，它将快速提升自己的能力，并加速整个进步的速度，这个过程甚至可能几天内就发生。按照这种观点，AI会在极短时间内推动所有技术的突破。我认为这种观点忽视了现实的物理和复杂性限制。即便AI再智能，也需要时间去制造更快的硬件，或者去精确建模复杂的系统，比如预测经济或生物系统的行为。在实际应用中，很多时候进行实验的速度要快于复杂建模的速度，AI并不能完全超越这些物理限制。另一个极端则认为，即使AI带来了技术进步，这些进步也可能像过去计算机和互联网的创新一样，并不会大幅提高生产力。这种观点认为，社会、经济和法律制度的惰性会极大地限制AI的影响。这种看法中有些道理，特别是一些政府机构和大型企业确实在适应新技术方面非常缓慢。我认为，现实情况很可能介于两者之间。尽管存在着各种障碍，但在技术突破和竞争压力下，一些组织和个人会推动AI逐步融入社会。

我觉得很多人没有抓住重点。即使AI系统完全不受人类控制，能够绕过人类设置的所有障碍，它依然会面临挑战。然而，如果我们想要一个不会毁灭人类的AI系统，它就需要遵循基本的人类法律。如果我们希望这个世界变得更好，那么AI系统就必须与人类互动，而不是创造自己的法律体系或无视现有法律。尽管现行的流程效率不高，但我们仍然需要接受这些限制，因为系统的推广需要获得民众和民主的认可。我们不能让少数开发这些系统的人决定什么对所有人最好。我认为这是错误的，也不可能在实践中奏效。把所有这些因素放在一起，我们不会在短时间内就改变世界，不会立即把所有人“上传”。我认为，这不会发生，至少不会以我们希望的方式发生。反过来说，另一种观点更值得同情。我们过去也见证了生产力的提升，比如计算机革命和互联网革命带来的变化，但总体上这些提升并没有我们想象的那么显著。罗伯特·索洛有一句名言：“你在各处都能看到计算机革命，除了生产力统计数据中。”为什么会这样？人们往往会归因于企业结构、技术的推广速度以及对贫困地区技术的推广速度缓慢。我在文章中也提到了这一点：我们该如何将这些技术带给那些连手机、计算机或基础药物都无法获得的贫困地区，更不用说尚未发明的AI技术了。

因此，有人可能会觉得，这项技术从理论上很棒，但实际上可能没有什么实质性的改变。泰勒·科恩对我的文章作出回应，他认为根本性的变化最终会发生，但可能需要50到100年。还有人认为这种变化几乎不会发生。对此，我认为确实有一定道理，但时间尺度可能被高估了。我可以看到今天的AI有两面性。我们的许多客户是习惯于某种工作方式的大型企业。我还在与政府对话中见过类似情况——这些机构变革缓慢，但我发现有两个因素能加速进程。首先，公司或政府内部会有一小部分人，他们真正理解AI的发展趋势，或者至少理解AI在其行业中的前景。当前的美国政府内就有一些这样的人，他们认为这是当今世界上最重要的事情，并推动着这项技术的发展。虽然这些人只是大组织中的少数派，但随着技术在某些领域取得成功，这种“竞争的阴影”推动了整个组织的进步。比如，一个银行可能会说：“看，那家新兴的对冲基金在做这些事情，他们会抢走我们的市场。”在美国，政府可能会担心中国会在这方面领先。因此，竞争的压力与那些有远见的人共同作用，推动了变革的发生。

我在这个过程中看到了很多变化的轨迹。起初似乎有很多阻力和复杂性，但随着时间推移，创新的方式突破了重重障碍。就像AI领域中的“扩展假设”一样，最初很少有人理解这个概念，但几年后它成为了众人皆知的秘密。我认为AI的全球应用也会遵循类似的过程，障碍会逐步消除，然后一切会迅速改变。所以，我的直觉是，这个过程可能会在5到10年内发生，而不是需要50到100年。当然，这只是我的猜测，也有可能我错了。

“如果按这个时间线继续下去，我们会在什么时候实现“强大AI”？”这是我多年来一直在思考的问题。每次我说2026或2027年，都会有成千上万的人在网上讨论说，“Dario预测2026或2027年会实现强大AI”。虽然这只是我的猜测，但如果我们按照现有的发展曲线来推测，去年AI的水平大致相当于大学生，而前年则接近高中生。尽管我们在某些任务上仍然缺少一些能力，但这些也在逐步补足。因此，基于目前能力的提升速度，我们确实可能会在2026或2027年达到“强大AI”。当然，很多因素可能会阻碍这个进程，比如数据可能不足、计算集群可能无法扩展，或者外部事件（如供应链中断）可能会影响硬件的生产。因此，整个进程依然存在很多不确定性。所以我并不完全相信简单的线性推测，但如果你要遵循直线外推，那可能会在2026年或2027年实现。我认为最可能的是会有一些轻微的延迟，虽然我不确定延迟会是什么，但我认为这可能会按计划发生，或者稍微有一点延迟。我觉得我们还可能处在那些可能需要一百年才能实现的情境中，但这种情境的可能性正在迅速减少。我们正在迅速消除那些非常有说服力的阻碍，找不到令人信服的理由来解释为什么这不会在未来几年发生。

当我坐在这里，看到大多数阻碍已经清除的时候，我有一种感觉，剩下的阻碍也不会阻挡我们。但说到底，这并不是一个科学的预测，人们称它们为“扩展法则”，其实这并不是定律，比如摩尔定律其实就是一种不精确的表达——它们只是一些经验性的规律。我会选择赌这些规律会继续，但我也不确定。

未来AGI将如何引领一系列生物学和医学领域的突破，帮助我们解决许多问题。那么，在我的文章中，我非常强调的一个概念对我产生了很大的影响，那就是在大组织和系统中，总有一些人或一些新想法会引领方向的改变，从而对整个系统的轨迹产生不成比例的影响。如果你考虑医疗领域，每年我们用于Medicare和其他健康保险的支出高达数万亿美元，而NIH的预算是1000亿美元。而真正革命性的突破可能只需要其中一小部分资金。因此，我在想，AI是否可以把这一小部分变成更大、更高质量的部分？在生物学领域，我的经验是，最大的问题在于你无法观察到内部发生的过程，更难以改变它们。比如，细胞分裂过程一般是健康的，但有时会出错，导致癌症。细胞老化，皮肤变色或产生皱纹，这些都是由细胞内部复杂的蛋白质生成、传递和结合过程决定的。

我们一开始对生物学的了解甚少，随着显微镜的发明，才得以观察到细胞。后来我们发明了更强大的显微镜，观察分子水平的结构，发明了X射线晶体学观察DNA结构，再到基因测序技术读懂DNA，最后还发明了蛋白质折叠技术来预测蛋白质的折叠方式以及它们如何结合。我认为AI可以帮助我们在这个领域有更大的突破。比如，我们可以使用CRISPR技术进行基因编辑，但如果想要对特定细胞进行编辑，并确保错误的目标细胞数量很少，这仍然是一个挑战。因此我认为AI可以在这种研发的基础上发挥杠杆作用，让我们以更高的质量发现更多的医学和生物学技术。

在未来，一个生物学家与AI系统早期阶段，AI可能会像研究生一样。你会给它们分配一个项目，比如说，作为一个有经验的生物学家或教授，我可以设置实验室的方向，指导AI系统完成各项实验。AI可以查阅文献，决定使用的设备，比如去网站订购新设备，运行实验，写实验报告，检查样品图像的污染，决定下一个实验步骤，甚至编写代码进行统计分析。AI在此过程中会像研究生那样与教授互动，教授会不时地给出指导，当需要运行实验设备时，AI可能会需要人类助手来操作，或者它也可以利用逐渐发展的实验室自动化技术。这种场景下，人类教授可能会拥有成千上万个AI研究生，而这些AI的“智能”甚至超过人类教授。我认为，在未来的某个时刻，AI系统将会成为研究的主要领导者，指挥其他AI系统和人类助手进行研究。他们将成为新技术的发明者，比如类似CRISPR的技术。此外，AI还可以优化临床试验的系统，比如预测试验结果，改进统计设计，让原本需要5000人、耗资1亿美元、一年时间的试验，现在只需500人和两个月的时间来完成。

我认为AI虽然不是万能的，所有的试验也不可能完全通过模拟来完成，但AI确实能够大幅提升实验效率。我们仍然需要遵循临床试验的规程和法律规定，FDA和其他监管机构的程序可能还不够完善，但重要的是我们能否持续推动这个进程向前发展。当我们将所有这些积极的方向整合在一起，很可能就能将原本预计要到2100年才能实现的目标，提前到2027年到2032年之间完成。这完全是可能的，我们需要脚踏实地地一步一个脚印向前推进，虽然有些过程仍然需要时间，但当所有因素都朝着积极的方向发展时，这种累积效应必将显著加快整个领域的进步速度。

## 12、AI编程

我认为AI在编程领域的变化速度可能会更快。原因有两个：首先，编程和AI的开发过程关系密切。离AI开发越远的领域，比如农业，可能在某种程度上也会被AI影响，但编程是AI开发的核心工作，因此变化会更快。其次，编程可以形成闭环。AI可以编写代码，然后运行代码，看结果，并加以改进。与生物学和硬件不同，编程允许AI自我反馈，从而不断改进，因此AI在编程上的能力提升会非常快。以今年的进展为例，真实的编程任务在今年1月AI的成功率是3%，而到10月已经提升到了50%。我们正在上升曲线上，预计在未来的几个月中会接近90%的成功率。虽然最终会达到100%，但未来几年内，AI将能够完成大部分的编程任务。尽管如此，比较优势理论仍然有效——AI能做80%的程序员工作时，剩下的20%对人类的作用也会变得更有价值。这些工作将更多地集中在高层次的系统设计、架构评估、用户体验等方面。最终AI也会在这些领域胜出，但在短期内，人类仍将扮演重要角色，编程的性质会改变，但不会消失，只是人类的编程工作会更加宏观。

我特别看好在编程、计算机使用和生物学等领域开发专门工具的潜力。尤其是强大的IDE，我认为它们蕴含着巨大的发展空间。现在我们与模型的互动还停留在简单的对话和反馈层面，但如果能将传统IDE的功能，比如静态分析、代码组织、单元测试覆盖率测量等，与AI技术有机结合，必将产生更显著的效果。即使模型本身的质量不再提升，仅仅通过自动化错误捕捉和处理等方式，我们也能大幅提高生产力。不过，我们公司目前并没有计划自主开发IDE，而是选择支持像Cursor、Cognition这样使用我们API的公司。我们更倾向于让这些公司去探索不同的可能性，看看谁能在这个领域取得突破。这种策略的原因在于，我们现有的资源有限，无法同时尝试所有不同的发展方向。

### 13、工作的意义

在AI不断实现自动化的世界中人类如何找寻意义这个深刻问题时，这让我想起了我之前写过的一篇文章。虽然文章中略微提到了这个话题，但坦白说，我并没有对它进行足够深入的探讨。这并非因为我对此没有原则性的看法，而是由于文章最初只计划写两到三页，我本打算在全体会议上详细讨论这个议题。然而，随着写作的深入，我越发意识到这个主题的重要性，以及它在当前讨论中被广泛忽视的现状。我发现自己越写越投入，最终文章扩展到了40或50页。特别是当我写到工作与意义这部分时，我意识到可能需要100页的篇幅才能充分展开，甚至可能需要专门写一篇新文章来详细探讨这个主题。这个问题确实值得更深入的研究和探讨，因为对许多人来说，工作确实是生活意义的重要来源。在思考意义这个深刻的问题时，我认为一个人生命中的努力和选择的价值并不会因为后来发现身处虚拟世界或模拟环境而减损。这就像在一个模拟环境中度过了几十年的经历，即便最终发现这只是一个"游戏"，但在这个过程中做出的重要决定，特别是那些涉及道德抉择的时刻，以及所付出的努力和学到的技能，都是真实而有价值的。

这让我想到一个类比：如果告诉一位在历史上做出重要发现的科学家，比如发现电磁学或相对论的人，说其实在20,000年前就有外星人发现了这些原理，这并不会削弱他们发现的意义。因为意义并不在于是否是第一个发现者，而在于整个探索的过程——这个过程展现了你是什么样的人，你与他人建立了怎样的关系，以及你在这段旅途中做出的各种决定。是这些经历和选择塑造了我们，赋予了生命意义，而不是最终的结果或环境的真实性。在思考AI主导的未来世界时，我认为社会是否缺乏意义，很大程度上取决于我们如何构建社会结构。即使在当下，对大多数人来说，生活也并非尽善尽美，他们仍在通过工作努力寻求生活的意义。作为这些技术的创造者，我们有责任关注那些仍在为基本生存而奋斗的人们。如果我们能够实现技术红利的普惠共享，让每个人都能从这些进步中受益，这将极大地改善人们的生活质量，让生活的意义变得更加丰富。这不仅能增加人们对生活的意义感，还能创造出全新的体验形式，使未来成为一个值得共同期待的美好图景。然而，我更担心的是经济和权力的过度集中问题。历史反复告诉我们，人类的苦难往往源于人与人之间的不公正对待。AI技术无疑会带来巨大的力量，但如果这种力量被少数人垄断并滥用，后果将会非常严重。因此，在推进AI技术发展的同时，确保其公平分配和合理使用变得尤为重要。

我希望我们能把事情做对，实现这个未来。如果我有一个信息要传达，那就是为了实现所有这些目标，建立正面的科技、商业和经济体系，我们不仅要构建技术，还需要认真处理风险。风险是我们前进道路上的地雷，我们必须要拆除它们，才能继续前行。

  

* * *

原视频链接：https://www.youtube.com/watch?v=ugvHCXCOmm4&t=147s&ab_channel=LexFridman

素材来源官方媒体/网络新闻

**不只是算法，我们在创造改变生活的可能。您是否曾想过AI技术能带来突破？现在，欢迎扫描下方二维码或点击链接填写AI需求，来和我们聊聊。**

![](https://mmbiz.qpic.cn/mmbiz_png/iaqv2tagPYAiaL7PlKHjBa3aqEqSI51kMvIaolcia8daPrv3ePqIQapsB6ncBKnBpKib2OCDyPACViaO16qqfs8WLbA/640?wx_fmt=png&from=appmsg)

_https://kawecob6a7k.feishu.cn/share/base/form/shrcn9i5HkwtHpkQNaV1bDqjt8f，会有相关负责人及时联系您，请留意您的微信和邮箱信息。_

##  往期回顾

[1、[对话Sam
Altman：OpenAI新战略下，与主流路线不同的AI创业机会（附完整视频）]](https://mp.weixin.qq.com/s?__biz=Mzg5NTc4ODkzOA==&mid=2247494051&idx=1&sn=fda86229539f838ce661d19705d62906&chksm=c0085b46f77fd2501181cca5b603610cab8068d9524a96598202abb2aa2fba3f8295414c12b8&scene=21#wechat_redirect)

[2、[Anthropic CEO 万字长文：我认为AGI最早会在 2026
年出现，机器可以像人类一样协助办公]](https://mp.weixin.qq.com/s?__biz=Mzg5NTc4ODkzOA==&mid=2247493607&idx=1&sn=0a1ffadd2c4350e13baf51e0b0d0e598&chksm=c0085502f77fdc14239a99e12f28403a10c01b44bae521bbc9964266f69d8d20829eb542e181&scene=21#wechat_redirect)

[3、[CNN对话软银CEO孙正义：超级AI不仅会在2035年到来，还将具备万倍于人类的智能](https://mp.weixin.qq.com/s?__biz=Mzg5NTc4ODkzOA==&mid=2247493966&idx=2&sn=d0992e40e82b4a827c37f853272b4d42&chksm=c0085babf77fd2bd804482ddfb52289f520f319c9afb71640c464c9d2de7ed48a238a6c891ff&scene=21#wechat_redirect)

* * *

![](https://mmbiz.qpic.cn/mmbiz_png/iaqv2tagPYAhtRhTOjz2QwH4dIlC3YUcYbaicMEwjqQqh06Yhdd7EH3r9wiaMRArLz0a6Zhx6uiaUD7hguPfbY0nAg/640?wx_fmt=png&from=appmsg)

****

**想象一个世界，AI不再是遥不可及的科技。我们将 AI 科技与创新想法完美融合,实现无限可能!******

## 告别昂贵服务和缺人烦恼,再见漫长交期

##  无限创意,分分钟生成专业级产品

## 感受 AI 带来的全新工作体验！

** _欢迎各大品牌方、媒体、企业和个人等_**

** _请联系负责人微信：Milo-1101_**

** _\--END--_**

  
  
****未经许可不得转载，务必保留公众号原文链接和公众号按钮****

