# 谷歌首席科学家 Jeff Dean演讲：人类设计芯片需要18个月，AI只用了1秒钟

文章作者: AI深度研究员
发布时间: 2024-12-18 05:30
发布地: 上海
原文链接: http://mp.weixin.qq.com/s?__biz=Mzg5NTc4ODkzOA==&mid=2247494824&idx=1&sn=a1ff081ee18fa149c8e29cefce479ffe&chksm=c0085e4df77fd75b63ee11c7e0f808e3c9a9e3c2050bdfe649942d4b966326e2e622264e15b2#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/iaqv2tagPYAhQDEJWyCoPt6doCtJGO9sM4ia28jPO6sib8EIT8WQ2B8TFIt9IQCvjZSibe0mlgFmwneSnR41jUWA6g/300

**（关注公众号并设为🌟标，获取最新人工智能资讯和产品）**

全文约3,800 字，阅读约需8 分钟

在 NeurIPS 2024 大会上，谷歌首席科学家 Jeff Dean 带来了一个令人震撼的消息：AI
技术正在彻底改变芯片设计的游戏规则。传统芯片设计需要数百位工程师投入 18 个月时间，而借助 AI 技术，这一过程有望缩短到惊人的 1 秒。

这个突破性的宣布来自 12 月 15 日在加拿大温哥华举办的 NeurIPS 2024 大会。在这场重要演讲中，Dean 不仅展示了 AI
在芯片设计领域的革命性突破，更深入探讨了机器学习如何全面革新计算机系统。他特别引用了 Rich Sutton
的观点：在解决复杂问题时，搜索和学习往往能帮助我们超越传统方法，找到最优解决方案。"

这不仅仅是速度的提升，而是整个计算机系统设计范式的革命。"Dean
指出，传统的计算机系统，无论是操作系统、编译器还是内存分配器，都尚未充分利用机器学习的力量。但这种情况正在发生根本性的改变。早在 2018
年，他就预见了这场变革，并提出了三个核心挑战：**如何将机器学习无缝集成到传统系统、如何在复杂环境中应用机器学习，以及如何确保机器学习决策的可靠性和安全性。**

虽然这些挑战曾经看似难以逾越，但近年来的技术进展证明，革命已经悄然展开。从内存管理到编译器优化，从系统调度到芯片设计，机器学习正在重写计算机系统的每一个环节。这不仅带来了效率的提升，更预示着计算机工业即将迎来一个全新的时代。

  

**演讲文稿**  

##  编译器设计优化

在编译器和自动调优系统领域，机器学习为性能优化提供了前所未有的机会。这种突破首先体现在用机器学习替代传统的启发式编译器选择上。研究者孟婆提出了一种自动调优多遍编译器的创新方法，包含两个核心组件：学习成本模型的评估器和代码优化器。

学习成本模型的评估器能够评估代码在特定编译策略下的运行速度，其独特之处在于有时甚至无需实际运行代码就能做出准确判断。代码优化器和学习策略则负责在高级代码与低级代码的映射过程中，选择最优的编译器参数，以获得最佳性能。

运算符融合是这种方法的典型应用案例。在编译器中，当处理不同形状的数组时，融合操作可能会提升效率，但这种提升并非必然。它取决于多个因素，包括设备的内存带宽和数组的具体形状。传统方法难以权衡这些因素，而机器学习系统则能够根据运行时的硬件特性，智能地决定何时进行融合。

布局分配是另一个重要应用。系统需要决定如何将抽象张量转化为实际张量，并优化内存布局。这个过程涉及复杂的权衡，传统的规则基础方法难以应对。通过机器学习，系统能够根据实际运行环境和需求，自动找到最优的布局策略。这些优化在实践中取得了令人瞩目的成效。在谷歌的机器学习工作负载中，生产模型的性能提升了5%到25%。正如Dean幽默地说："这相当于每个人免费获得了一块TPU。"这种提升不仅显著，而且具有广泛的实用价值。

## 突破性的内存管理创新

内存管理领域的创新同样令人瞩目。谷歌开发的系统采用了革命性的方法,通过机器学习预测和管理对象生命周期。系统的核心创新在于将调用堆栈视为自然语言,使用
LSTM 网络进行分析,从而准确预测对象的生命周期。具体实现机制包括:

  * 实时堆栈哈希处理和快速缓存查找
  * 动态更新预测模型以适应程序行为变化
  * 智能分配策略优化内存使用

系统能够精确预测对象生命周期长短,并据此优化分配策略。短生命周期对象被放入线程本地缓存,长生命周期对象则使用中心化内存管理。系统还会将相似生命周期的对象分组存储,显著减少内存碎片。更重要的是,系统具备自适应能力。如果预测出现错误,比如将对象生命周期预测为"短暂"而实际为"中等",系统会自动调整并更新预测模型。这种方法在实践中取得了显著成效,将内存碎片减少了19%到78%。

## SmartChoice：革命性的轻量级学习决策系统

SmartChoice是谷歌开发的另一项重要创新，旨在将轻量级学习决策无缝集成到现有的复杂系统代码中。这个系统提供了简洁而强大的API，包括上下文类型（用于决策的信息）和手臂类型（待选择的决策集），通过持续的选择和反馈过程来优化决策质量。

系统最令人印象深刻的特点是其极快的决策速度。得益于基于Bandit算法（多臂老虎机问题）的实现，SmartChoice能在几十微秒内完成决策。这种高效性使它能够应用于实时性要求极高的场景。

系统不仅能处理即时反馈，还能巧妙地处理来自分布式系统不同部分的延迟反馈。例如，当用户在网络服务器97上的操作与服务器4上的早期决策相关时，系统能够正确关联这些信息，并在离线处理中更新模型。新的模型会被推送到所有相关服务系统，实现全局优化。

在YouTube的实践应用中，SmartChoice成功优化了视频缓存策略。系统学习如何在全球各地的缓存位置存储最受欢迎的视频，显著降低了用户通过昂贵网络链接访问远程数据中心的需求。在相对适中的计算成本下，系统将缓存未命中率降低了9.1%，这个成果在视频流务领域具有重要意义。除了缓存优化，SmartChoice还被广泛应用于其他场景。在线程计数优化中，系统能够根据请求类型（如航班搜索）智能地决定最佳线程数，在延迟和吞吐量之间取得平衡。在工作分区优化中，系统能够优化广告数据的刷新频率。在用户界面优化方面，系统能根据用户历史行为预测并定制界面元素。

## 推理系统优化：降低成本与延迟

在机器学习推理领域，降低成本和延迟是一个核心挑战。正如Jeff Dean强调的，这直接关系到先进模型能否惠及更多用户。最近发布的Gemini
2.0获得积极反馈，很大程度上就得益于其极低的延迟。

谷歌开发了多项创新技术来优化推理系统。首要的是过度训练小型模型：通过精心调整模型大小，在固定的训练计算资源下进行更多数据遍历（epoch）。实践表明，将模型缩小到原来的三分之一甚至五分之一，虽然显著降低了推理成本，但在适当的训练策略下，模型质量几乎不受影响。

知识蒸馏是另一个重要技术，它能够将大模型的知识有效转移到更小的模型中。这种方法不仅能保持模型的核心能力，还能显著降低推理成本。此外，选择性激活技术允许模型根据不同类型的上下文和输入，智能地激活其特定部分，从而在不同任务中实现更高的效率。

推测性解码是一项特别值得关注的创新，它能使自回归模型的解码速度提升2到3倍。这种方法的独特之处在于，它无需改变模型架构或重新训练，完全通过优化解码算法来实现性能提升。其核心思想是引入一个小型模型快速生成候选标记，再由大模型并行验证这些预测，从而提高整体效率。

## AlphaChip：重新定义芯片设计

在芯片设计领域,谷歌的创新最为引人注目。传统芯片设计需要数百人投入18个月时间,成本高达数亿美元。AlphaChip
系统试图将这个过程缩短到几周。AlphaChip 采用了多项创新技术:

  * 智能芯片分块和组件聚类
  * 强化学习优化布局布线
  * 预训练模型加速设计过程 系统最令人印象深刻的是其预训练能力。如果从零开始,系统需要20小时才能达到较好状态。但通过预训练,系统能在1秒内完成高质量的布局评估,效率超越人类专家。

AlphaChip 不仅在速度上实现了突破，更重要的是开创了芯片设计的新范式。传统的芯片设计流程主要依赖人工经验，从高级架构规范到低级 RTL
设计的转换过程高度依赖硬件工程师的手动工作。而 AlphaChip
通过端到端学习方法，使整个设计过程更加自动化和智能化。一个关键创新是引入了多层次的反馈循环。系统使用快速评估机制，通过低成本代理快速评估设计决策的下游影响，同时保留复杂评估作为整体验证手段。这种方法让设计团队能够快速迭代，同时确保最终设计的质量。

Dean 提出了一个大胆的设想：如果我们在21天内投入1000万美元的计算资源，使用16,000块 TPU 芯片（约15
exaflops的算力），可能彻底改变芯片设计的方式。这种计算密集型方法虽然前期投入较大，但能显著缩短设计周期，降低总体成本。按照他的预测，芯片设计成本有望降低20到100倍。

## 强化学习在系统优化中的应用

强化学习在系统优化中展现出独特优势。以芯片布局为例，AlphaChip
通过强化学习算法确定组件位置，并通过轻量级代理评估总线长度、面积利用率和潜在拥塞等关键指标。更重要的是，系统能够在几秒钟内评估决策影响，使设计过程变得更加敏捷。

在架构探索方面，强化学习同样发挥着重要作用。系统需要在庞大的设计空间中做出多项决策，包括：

  * 缓存层次结构设计
  * 内存带宽配置
  * 计算单元规划

通过结合高级模拟器和强化学习，系统能够快速探索这个复杂的决策空间，并根据下游反馈不断优化设计方案。这种方法在针对特定领域的芯片设计中特别有效，因为它可以充分利用领域特定的知识和约束。在学习综合(learning
synthesis)领域，尽管相关研究尚未正式发表，但谷歌 DeepMind
团队已经在多个竞赛中展示了这项技术的潜力。与传统方法相比，机器学习综合能够自动处理特定的逻辑设计问题，比如五位加法器的最优实现。这种方法的优势在于它能够自动发现和优化关键子系统，而不是试图一次性解决整个芯片设计问题。这种分而治之的策略不仅降低了问题的复杂度，也提高了解决方案的质量和可靠性。

## 面向未来的系统设计

机器学习如何从根本上改变计算机系统的设计和优化方式。从编译器优化到芯片设计，从内存管理到缓存策略，机器学习正在重新定义计算机系统的各个方面。特别值得注意的是端到端学习的重要性。在芯片设计中，早期决策会对最终的布局和布线产生深远影响。通过端到端学习，系统能够在架构探索和综合阶段做出更优决策，从而实现真正的全局优化。未来的发展方向包括：

  1. 深化机器学习在传统系统中的应用
  2. 提升端到端学习的效率和可靠性
  3. 扩大自动化决策的范围和准确性  

谷歌在系统机器学习领域的领导地位，更描绘了计算机系统发展的新愿景。通过将学习和搜索这两个强大工具应用到系统的各个层面，我们正在见证并参与计算机科学史上的一次重大变革。

# 往期回顾

[1、[管理者必读｜跳出AI热潮看本质:厘清技术边界，建立筛选有效应用的决策框架](https://mp.weixin.qq.com/s?__biz=Mzg5NTc4ODkzOA==&mid=2247494430&idx=1&sn=bd0f2d93c68fd57f78617391c57e7531&scene=21#wechat_redirect)

[2、[硅谷顶级分析师演讲：18个月前，盖茨为何将图形界面和AI列为生涯中遇到的两件革命性事物？（附视频）](https://mp.weixin.qq.com/s?__biz=Mzg5NTc4ODkzOA==&mid=2247494438&idx=1&sn=839e9e6689abf85d635b9c51ebfc45e2&scene=21#wechat_redirect)

[3、[斯坦福对话Perplexity
CEO：不做PPT，不拼模型，这家AI公司凭什么值90亿？(附视频)](https://mp.weixin.qq.com/s?__biz=Mzg5NTc4ODkzOA==&mid=2247494490&idx=1&sn=3e0ea49c14802672a4135a6f4b4f4db2&scene=21#wechat_redirect)

* * *

**我们AI团队专注企业AI解决方案**

业务流程AI优化提升运营效率降低人力成本定制AI应用开发场景化解决方案快速交付落地AI转型咨询规划专业评估诊断精准转型方案

联系负责人：Milo-1101（仅限企业客户）

原文链接：https://x.com/nrehiew_/status/1868672595106865576

素材来源官方媒体/网络新闻

**\--END--**

