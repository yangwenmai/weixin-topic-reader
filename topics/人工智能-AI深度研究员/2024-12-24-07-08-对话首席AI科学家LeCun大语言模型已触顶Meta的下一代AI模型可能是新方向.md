# 对话首席AI科学家LeCun：大语言模型已触顶，Meta的下一代AI模型可能是新方向

文章作者: AI深度研究员
发布时间: 2024-12-24 07:08
发布地: 上海
原文链接: http://mp.weixin.qq.com/s?__biz=Mzg5NTc4ODkzOA==&mid=2247494948&idx=1&sn=97dd2656c669352cb0a732cb85f3e903&chksm=c0085fc1f77fd6d73a872c348f82e9e9a527f9b6a6503d201097a38f7d6b4611ed00a2564390#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/iaqv2tagPYAhge21ic13fxHfibfX5ZgWCYxcb1Jf89xfQKoRwwdFau7wP8XcYvVOA6p7SEuCJmBApOicesLsdpYAwA/300

**（关注公众号并设为🌟标，获取最新人工智能资讯和产品）**

全文13,000 字，阅读约需26分钟

最近，在约翰斯·霍普金斯大学新落成的彭博中心，Meta首席科学家Yann LeCun
带来了一个引人深思的观点：鉴于数据问题，大语言模型（LLM）可能已经触及了发展天花板。

作为人工智能领域公认的"教父"级人物，LeCun的这一判断值得关注。他在神经网络领域的奠基性工作，为当今最强大的AI系统奠定了基础。目前，作为Meta首席科学家，他正带领团队开启一个新的方向：大型概念模型（LCM）。"可用的自然语言数据已经被用尽。"LeCun直言不讳，"目前，几乎所有可以公开获得的互联网文本数据都已被用于训练这些LLM。即便开始生成合成数据，也无法将性能提升10倍或100倍。"

传统的LLM通过预测下一个词来工作：收集约20万亿个单词的公开文本数据，用数十亿甚至数百亿参数的神经网络进行训练，根据上下文预测下一个词的概率分布。但LeCun认为，这种方法已达瓶颈。

"这种预测模式并不能保证生成的文本逻辑自洽或避免虚构内容。"LeCun解释道，"虽然业界正通过人类反馈训练和数据库查询来优化系统，但这并非未来AI的发展方向。我们需要一个全新的范式。"

# 采访文稿

**主持人：**
老朋友了，我想我们今天主要会聊人工智能，我想听听你对这些事情的看法。你和埃隆有公开争论，还称特朗普是“病态撒谎者”。与此同时，扎克伯格（Meta
CEO）却在海湖庄园的露台上享受了一顿美餐。谈谈你与即将上任的政府的关系吧。你是否需要开始减少这样的评论？或者你完全不在意？

**嘉宾Yann
LeCun：******嗯，我对很多事情都感兴趣，也对许多问题感到担忧。从政治上来说，我很明显是一个典型的自由派，这在欧洲的政治派别中将我放在正中央，但在美国显然不是。让我对埃隆感到不满的是他开始攻击高等教育、科学机构和科学家，比如Anthony
Fauci。作为科学家，我也同时是 Meta 的高管，我对此非常反感。我非常感谢 Meta
能让我拥有独立的声音，你可以看得出来我并没有在说“企业话术”。这也反映在我创立的研究实验室中，我们发布所有研究成果，并通过开源与大家分享。我们对研究和意见都很开放。这是我的立场。作为科学家和理性主义者，我真的很不满埃隆攻击科学界的行为。

##  1、选择 Meta工作

**主持人：** 现在埃隆正处于事件的中心，你如何应对？

**Yann
LeCun：******我见过埃隆很多次，他可以是个很理性的人。你必须与人合作，无论政治或哲学观点有何分歧。最终，你不得不和人合作，这就是我和他之间的关系发展方向。

**主持人：** 在 Meta，你的职责是什么？

****Yann LeCun：**** 我不负责政策制定。我从事的是基础研究，我不负责内容政策，也不涉及这些事情。我确实和全球许多政府交谈，但主要是关于
AI 技术及其对政策的影响。

**主持人：** 我很好奇，你为什么选择 Meta，而不是传统的大型研究型大学？你如何看待自己的影响力？

****Yann LeCun：**** 我只是一个简单的科学家，做一些研究。同时我也是一名学者。我是纽约大学的教授，同时也在 Meta 工作。差不多 11
年前，马克·扎克伯格找到我，他希望我为 Meta 创建一个专注于 AI
的研究实验室，因为他认为这将对未来产生重大影响，他的判断是对的。我当时提出了三个条件：第一，我不会离开纽约；第二，我不会辞去在纽约大学的职位；第三，我们的所有研究都将公开，我们会发表所有研究成果，并开源代码。他的回答是“没问题”。而且他还补充说：“这已经是公司的
DNA，我们的所有平台代码都是开源的。”
这让我意识到，在拥有足够资源的公司中，这是一个从零开始建立研究机构的机会。我可以按照自己认为合适的方式构建它。我之前在贝尔实验室工作过，所以对如何在工业界进行雄心勃勃的研究有一些经验。我觉得这是一项非常令人兴奋的挑战。

**主持人：** 那我们继续讨论更先进的 AI。最近，特朗普任命了 David Sacks 为 AI
和加密领域的“沙皇”。政府在这个领域显然会扮演越来越重要的角色。对于不太了解的人来说，Sacks 是一名投资者，也是“PayPal
黑手党”的一员，同时也是埃隆·马斯克的老朋友。他的政治立场发生了显著变化。你觉得现在华盛顿需要这样一个角色吗？或者你对此完全不关心？

******Yann LeCun：** 有几个原因。首先，政府不应该制定让开源 AI
平台非法的法规。我认为开源对技术的进步和广泛使用至关重要。我们可以进一步讨论这个问题。其次，我并不反对对基于 AI
的产品进行监管，这是完全可以接受的。但学术界在 AI
的发展中正逐渐落后，主要原因是缺乏计算资源。因此，我认为政府应该为学术界提供更多的计算资源。因为我喜欢做科学研究。在我的职业生涯中，我有很多机会成为管理者。我在
Meta
当了四年的管理者，然后成为首席科学家。坦率地说，我现在的工作是公司里最棒的职位，每个人都羡慕我，因为我是高级副总裁，但没有任何直接下属。我不用管理一个团队或组织。

**主持人：** 那么，在你看来，一个 AI “沙皇”最重要的职责是什么？

****Yann LeCun：**** 确保 AI 的研究和开发不会被视为非法。

**主持人：** 你认为 AI 的研究开发真的会被视为非法吗？

****Yann LeCun：**** 过去两年，关于 AI
是否会变得“本质上危险”的争论很激烈。不是指产品的危险性，而是技术本身是否危险。因此，有人提议对 AI
系统的能力设限。我一直反对这种对研发进行监管的想法。我认为这极其适得其反，基于对 AI 潜在危险的错误理解，比如“AI
未来某一天会接管世界”这样的观点。但我们距离这种情况还很远。任何试图监管研发的行为都非常不利，而且实际上，这些提案会导致行业内少数公司通过监管获取垄断地位。建议政府不仅不要限制，还要提供更多的资金支持。政府应该在提供资源方面发挥更积极的作用。

**主持人：** FDC 新任负责人是 Andrew Ferguson，而国防部长提名了前福克斯新闻主播 Pete
Hegseth。他们一个试图减少监管，一个正在谈论国防部的新 AI 办公室。你认为政府在这个领域的活跃度应该增加吗？

****Yann LeCun：**** 政府需要更加了解和教育自己，当然也需要更加积极，正如我之前提到的那些原因。我们需要有一套工业政策来解决目前所有 AI
芯片都在台湾生产、由一家公司的设计垄断等问题。

## 2、芯片和AI人才的竞争

**主持人：** 关于芯片领域，你认为是否需要让这个市场更加竞争？

****Yann LeCun：****
是的，比如在芯片领域，可能需要采取一些措施让市场更加具有竞争力。另外，还有一个非常关键的问题，这不仅仅涉及美国政府，还涉及全世界的政府。人工智能正在迅速成为一种通用的知识平台，基本上可以作为所有人类知识的存储库。但是，这只能通过自由和开源的平台来实现，这些平台需要从全世界的数据中进行训练。你不能仅仅依赖美国西海岸某家公司墙内的系统，这样的系统无法覆盖印度的
700 种语言或印度尼西亚的 300 多种语言。因此，最终这些平台需要通过全球范围内的分布式训练来实现，同时必须保持开放。

**主持人：** 我知道你对过早的监管抑制创新表示担忧。不过，你签署了一封公开信，反对拜登总统的 AI
行政命令。请谈谈你为什么要这样做，以及你认为政府在这方面应该扮演什么角色？

****Yann LeCun：****
我认为在那个行政命令中有很多完全合理的内容，比如保护隐私等，这些完全有意义。不过，我不赞成的地方在于，行政命令和最初版本的欧盟 AI
法案都设定了一个限制，比如如果你训练的模型超过亿次浮点运算（FLOP），你就必须获得政府的许可。这种基于 AI
本质上危险的假设完全没有道理。我完全不同意这种方法。AI 安全确实有很多重要的问题需要讨论，但对竞争设限毫无意义，这只会扼杀创新。

**主持人：** 2016 年，特朗普曾表示要收紧 H1-B
签证，而这类签证为美国带来了大量技术人才。你在帖子中提到，如果没有这些签证，你和许多科技界的大人物都不会在这里。这会对美国在 AI
领域的全球地位造成伤害吗？

******Yann LeCun：**
当然会。很多人都说，美国在这一点上简直是自断手脚，让人才很难获得留在这里的签证。我自己经历过这些困难。我是法国人，同时也是美国公民。我的法语口音可能暴露了一点（笑）。不过，这确实是一个障碍过程。美国的科学技术实际上受益于从全世界吸引人才，但目前的移民政策却非常不利于这一点。我建议是比如，如果一个人在美国大学获得了研究生学位，可以自动获得留下的资格，至少有一段时间找到工作，从而使他们能够更稳定地生活。事实上，这种政策应该扩大到在任何地方获得高级学位的人，但目前的流程复杂且耗时，很容易让许多人望而却步。因为比如
Sergey Brin（谷歌联合创始人）、Satya Nadella（微软 CEO）、埃隆·马斯克（特斯拉 CEO）等，他们都是移民。

**主持人：** 最近，许多大科技公司都推出了新的大语言模型（LLM）或 AI 功能。我想了解一下 Meta 目前在做些什么，比如最新发布的 Llama
3.3。这与其他模型相比如何？

****Yann LeCun：**** Llama
的主要区别在于它是免费的，也是开源的。开源软件是指其源代码是公开的，你可以修改、编译并免费使用它。根据大多数开源许可协议，如果你对代码进行改进并将其用于产品，也需要以源代码的形式发布这些改进。这种模式让平台软件的发展非常迅速。整个互联网几乎都运行在开源软件上，比如
Linux。开源的复杂性在于构建 AI 系统时，首先需要收集训练数据，然后用这些数据训练一个基础模型（foundation model）。Meta 的
Llama 模型并未分发训练数据或训练代码，但我们分发了已经训练好的基础模型，以及用于运行和微调模型的开源代码。用户无需支付费用或向 Meta
请求许可，就可以自由使用和调整这些模型。当然，法律环境对这一点有一定的限制。

## 3、大模型开源初衷

**主持人：** 为什么开源更好？你的观点是，其他大公司是封闭系统，比如 OpenAI、Anthropic 和 Google。为什么你认为他们选择封闭系统？

****Yann LeCun：****
很可能是为了获得商业优势。如果你希望直接从这种产品中获取收入，并且认为自己在技术上领先，或者能够保持领先，那么保持封闭可能是有道理的。但这不是 Meta
的情况。对于 Meta 来说，AI
工具只是我们一系列体验的一部分，而这些体验主要是由广告收入支持的，因此这不是我们的主要收入来源。我们相信，开源平台会进步得更快，也会更有创新性。实际上，我们已经看到，很多创新是因为人们能够使用
Llama 系统进行实验，从而提出了新的想法。

**主持人：** 有人批评说，Meta 是因为在竞争中落后，才选择开源以赶上对手。你怎么看待这种说法？

****Yann LeCun：**** 这其中有一个有趣的历史背景。首先，行业中除了 Google 外，几乎所有公司都在使用一个名为 PyTorch
的开源软件平台，而这个平台最初是在 Meta 开发的。后来，Meta 将其所有权转移给了 Linux 基金会。所以，像 OpenAI、Anthropic
等公司都在使用 PyTorch。如果没有 Meta，就不会有今天的 ChatGPT 和 Claude。OpenAI 和 Google
现在变得越来越封闭。过去 OpenAI 并不保密，但在过去三年里，他们变得越来越封闭。Google
也有类似的趋势，尽管没有完全封闭。他们试图在秘密中推动技术发展，而 Meta 则选择了开放的道路。

**主持人：** 你提到 Meta 的研究正在超越大型语言模型（LLMs）和聊天机器人，进入下一代 AI 系统。能谈谈这方面的工作吗？LCM 和 LLM
有什么区别？

****Yann LeCun：**** 是的，我们正在开发一种新的模型，被称为大型概念模型（LCM）。这是一种超越传统 LLM 的系统。LLM
是通过预测下一个词来工作的。你收集大约 20
万亿个单词的公开文本数据，对一个拥有数十亿或数百亿可调参数的神经网络进行训练，让它根据前几个单词预测下一个单词的概率分布。LCM
则更进一步，它不仅仅是预测下一个词。我们相信，单纯依赖 LLM 的方法已经达到了性能瓶颈。

**主持人：** 为什么 LLM 达到了瓶颈？

****Yann LeCun：**** 原因很简单：可用的自然语言数据已经被用尽。目前，几乎所有可以公开获得的互联网文本数据都已被用于训练这些
LLM。虽然人们开始生成合成数据，但这并不能将性能提高 10 倍或 100 倍。因此，我们正在开发一种新一代的 AI
系统，它不再仅仅依赖于预测下一个词。自动回归 LLM
工作原理是通过预测文本中的下一个词来工作的。它根据几千个单词的上下文来预测下一个词的概率分布。预测出一个词后，将其作为输入，再预测下一个词，以此类推。这种方式并不能保证生成的文本逻辑自洽或避免虚构内容。行业中很多公司正在致力于让
AI 系统通过人类的反馈训练，去完成特定任务，避免生成无意义的内容。并且这些系统还需要在不知道答案时，能够查询数据库或搜索引擎。但是这并不是未来的 AI
系统工作方式。未来的系统不会仅仅依赖这种方法。

## 4、Meta新产品 Motivo

**主持人：** 我听说 Meta 最近发布了 Meta Motivo，是为了制作更逼真的数字化身。你能谈谈这个吗？

****Yann LeCun：**** 当然可以。我先透露个秘密：我现在戴着智能眼镜。这种智能设备让我们离未来的 AI
助手更近一步。五到十年后，人们可能会佩戴这样的智能眼镜或其他智能设备，这些设备中内置的 AI
助手将帮助我们处理日常生活中的各类事务。这些助手需要具备接近人类甚至超越人类的智能。

**主持人：** 那我们离这种人工通用智能（AGI）还有多远？

****Yann LeCun：**** 其实我们离 AGI
还很远，并不像有些人宣传的那么接近。虽然不是几百年，但也可能需要数十年。目前，我们有能通过律师资格考试或大学考试的
LLM，但我们仍然没有能清理房间或装洗碗机的家用机器人。这并不是因为我们不能造出机器人，而是因为我们还无法让它们足够智能，无法理解物理世界。物理世界对 AI
来说，比语言更复杂。

**主持人：** 语言不是智力的巅峰吗？

****Yann LeCun：**** 其实不是。语言只是离散符号的序列，对 AI
来说相对简单。相比之下，理解物理世界要复杂得多。我们正在开发的新架构和系统，旨在让 AI
通过观察和与世界互动来学习，像婴儿和小动物那样理解物理世界。这意味着这些系统将变得更具“代理性”。“代理性”系统（agentic
systems）是指能够规划一系列行动以实现特定目标的系统。目前所谓的代理性系统还无法真正进行规划，它们只能依赖一些预设的计划模板。

**主持人：** Meta 也在开发一个 AI 搜索引擎，是否是为了超越 Google 的搜索系统？

****Yann LeCun：**** 搜索确实是智能助手的一个重要组成部分，它可以帮助用户找到事实并链接到可靠的来源。这是一个完整 AI
系统的一部分，但我们的目标并不是直接与 Google 竞争，而是为需要 AI 助手的人提供服务。Meta 的 AI
系统除了提高广告效率外，它的长期愿景是让每个人随时拥有一个 AI
助手。这不仅仅是一个广告工具，而是一个全新的计算平台。未来的智能眼镜可能会配备增强现实显示屏，比如最近展示的 Orion
项目。虽然目前还无法廉价生产，但未来它们一定会普及。这些助手将成为我们的助手、顾问，甚至像团队一样为我们工作。它会像一个比我们更聪明的虚拟员工，最终每个人都会有一个虚拟助手，甚至比人类助理更聪明，为你提供支持和帮助。

**主持人：** 这听起来不错，但目前人类助理的成本要低得多。现在 Meta 预计在 AI 上的支出将在 380 亿到 400 亿美元之间，Google
的支出今年超过 510 亿美元，分析师预测微软的支出接近 900 亿美元。这是不是太多了？Marc Benioff
最近告诉我，这简直是在“奔向低谷”。你担心被超支击败吗？而且，花这么多钱来开发更智能的助手，似乎也不是个好生意。

****Yann LeCun：**** 这是一个长期的投资。我们需要建立基础设施，以合理的速度运行这些 AI 助手，满足越来越多用户的需求。目前 Meta
的 Mii 系统已经有 6 亿用户。Llama 的开源版本已经被下载了 6.5 亿次，这个数字令人震惊。此外，有 85,000 个基于 Llama
开发的开源项目，很多项目致力于让 Llama
支持更多语言，比如印度或撒哈拉以南非洲的语言。所以我不认为这些钱花得冤枉。这笔投资主要是为了基础设施建设，因为未来会有大量用户每天使用这些 AI
系统。而且，这些系统越强大，它们的计算成本也越高。所以这是一种必要的投入。

## 5、AI 模型风险

**主持人：** 你之前提到，集中化的专有 AI
模型是一个巨大风险。但也有人批评开源模式，认为它可能被坏人用来传播虚假信息、进行网络战或生物恐怖主义。Meta 对此有责任吗？

****Yann LeCun：**** 这个问题在内部引发了很大的讨论，尤其是在 2023 年初我们发布 Llama 的时候。最初版本的 Llama
不是完全开源的，而是需要用户申请权限并证明自己是研究人员。后来，我们收到来自行业的大量请求，要求下一版本开源，因为这将推动整个行业的发展，催生大量初创企业和新产品。经过数月的内部讨论，最终由马克（扎克伯格）拍板决定在
2023 年夏天开源 Llama
2。这一决定确实为行业带来了巨大的推动力。我认为开源比专有模式更安全吗。因为有更多的人在关注它并优化它。这种开放模式促进了更广泛的创新。当然，有人担心坏人可能利用这些工具做坏事。例如，中国的研究人员曾用
Llama 的旧版本开发了新的模型，但这并不算什么大问题。而且，中国本身也有非常优秀的开源模型，比如 Qwen。

**主持人：** 所以 Meta 对这些工具的使用没有责任？

****Yann LeCun：**** 我们当然有一定的责任。Llama 团队和 Meta 的 Geni
组织对所有发布的系统进行了严格的“红队测试”（Red Teaming），以确保它们在发布时至少是“最低毒性”的，并且尽可能安全。例如，我们最初将 Llama
2 提供给 Defcon 的一群黑客，让他们尝试攻击系统或做坏事。这被称为“白帽测试”（White Hat
Testing）。结果显示，在过去两年内，我们尚未发现任何使用我们分发的模型做坏事的重大案例。但这是“尚未”，未来可能会发生。如果真的会发生，早就发生了。公众可能不知道，其实早在
ChatGPT 出现之前，开源的 LLM 就已经存在很多年了。而当年 OpenAI 发布 GPT-2
时，曾声称不公开是因为它“非常危险”，可能被用来在互联网上传播虚假信息。然而，事实证明，这些担忧有些夸大。

**主持人：** 你的意思是我们不需要开源。你曾调侃其他公司，说他们的系统当时其实并没有多强大。这种担忧真的合理吗？

****Yann LeCun：****
确实，当时有很多关于这些系统可能被用于虚假信息传播的担忧，尤其是在美国选举期间。今年全球有三次重要选举，还有很多关于网络攻击的担忧。但实际上，这些事情并没有真正发生。我不是说不需要担忧，而是说要保持警惕，尽可能防止问题发生。但关键是，传播虚假信息并不需要
AI 助手——看看 Twitter 就知道了（笑）。

**主持人：** 哈哈，我注意到你很擅长“内涵”别人，比如之前你提到 OpenAI
是“秘密主义的戏剧女王”。最近你因为说文化机构、图书馆和基金会应该让他们的内容用于免费和开源的 AI 模型训练而受到了很多批评。你能谈谈这个问题吗？

****Yann LeCun：**** 当然。抛开法律问题不谈，如果你有一个愿景，认为 AI
将成为人类知识的存储库，那么所有的人类知识都必须用于训练这些模型。很多知识要么没有数字化，要么虽然数字化了但无法公开获取。这些不一定是受版权保护的内容，比如法国国家图书馆的大量内容已经数字化，但不能用于训练
AI 模型。

**主持人：** 你不是指侵犯版权，而是更广泛的文化知识获取问题？

****Yann LeCun：****
是的。例如，我的家族来自法国布列塔尼地区，那里有一种传统语言叫布列塔尼语。这种语言正在消失，现在每天使用这门语言的人只有大约 3
万。如果我们希望未来的大型语言模型能够支持布列塔尼语，就需要足够的训练数据。这些数据从哪里来呢？可能需要文化类非营利组织和政府合作来收集数据，并支持将其用于
AI 模型的训练。

**主持人：** 但是这些文化机构可能不愿意把数据交给美国西岸的大公司，那怎么办？

****Yann LeCun：**** 我的个人观点是，我们可以通过分布式的方式来训练全球性的 AI
系统。这意味着世界各地的数据中心可以利用本地数据，为一个全球系统做出贡献，而不需要将数据复制到单一的地方。就像 Linux 的开发一样。Linux
是一个为全人类服务的操作系统，主要由公司雇员在公司支持下贡献代码。AI 系统也可以采用类似的方式，每个人都为这个全球模型做出贡献。

**主持人：** 但是这样的模型如何盈利呢？

****Yann LeCun：**** 就像 Linux 一样，你不会为 Linux 支付费用，但如果你买一台运行 Linux 的设备，比如 Android
手机或配备触摸屏的汽车，你为设备支付费用。同样，AI
的基础模型可以是免费的，盈利是在其上的商业应用。这确实是一个美好的愿景，但目前看起来，权力还是集中在少数几个公司手中。但我们希望通过这样的开放模式，让 AI
成为所有人的 AI，而不仅仅是少数公司的工具。

**主持人：** 但这种情况并没有真正发生，对吗？

****Yann LeCun：**** 从我的角度来看，这实际上是不可避免的，只是时间问题。

## 6、政府对AI监管

**主持人：** 说到辩论，你和其他人工智能领域的“教父”们有过公开辩论，比如图灵奖的共同获奖者 Geoffrey Hinton 和 Yoshua
Bengio。他们最近对 AI 的潜在危险发出了相当戏剧性的警告，呼吁加强政府监管，包括对研发的监管。而你则称这些警告“完全是胡说八道”。为什么你会这么说？

****Yann LeCun：**** 是的，我对这个话题非常直言不讳。Jeff 和 Yoshua 都是我的好朋友，我们认识几十年了。我在 1987-88
年间和 Jeff 一起做博士后，而我第一次见到 Yoshua
时，他还是个硕士生。我们一起合作过很多项目，也因为在深度学习领域的贡献而获得了图灵奖。在很多地方是意见一致的，但在 AI
的“生存威胁”上显然分歧很大。Jeff
认为当前的大型语言模型（LLMs）具备主观体验，我对此完全不同意。我认为他在这一点上完全错了。我们之前也有过技术上的分歧，只是当时没有这么公开。Yoshua
的观点稍有不同，他更多担心坏人会利用 AI
做坏事，比如制造生化武器或其他危险物品。但老实说，这些危险在多年前就被提出来了，而且已经被夸大得不切实际，甚至有些扭曲。

**主持人：** 你之前称那些推动像 SB 1047 法案的人是“妄想症患者”（delusional）。这表达是不是太激烈了？

****Yann LeCun：**** 我并没有说 Jeff 和 Yoshua 是妄想症患者。我所说的是那些极端推动监管的人，比如认为 AI
会在五个月内毁灭人类的人，他们的观点显然是错误的。

**主持人：** 关于 AGI（人工通用智能），Hinton 和 Bengio 认为可能在五年内出现，而你说可能需要数十年。为什么你不担心呢？

****Yann LeCun：**** 首先，毫无疑问，未来某个时候会出现比我们更聪明的 AI
系统。这是一定会发生的事情。但五年、十年还是二十年？这很难说。从我的个人观点来看，最早可能是五到六年，但更可能是十年甚至更久。历史上，AI
的进展总是被低估。我们现在连家用机器人、完全自动驾驶汽车都没有实现，还有很多问题没有解决。要达到人类水平的智能，我们需要一整套新的技术，但目前我们甚至还没有找到通往这条路的方法。

**主持人：** 但如果你错了，那后果可能是灾难性的。

****Yann LeCun：****
如果我认为存在这样的生存威胁，我当然会非常直言不讳，努力警告大家。但事实是，当前的系统并没有表现出这种威胁的迹象。我们距离真正的 AGI
还有很长的路要走。几年后，当我们有了一种蓝图和一些可以令人信服的证明，表明我们可能找到通向人类水平人工智能的路径时。我其实不喜欢称其为“人工智能”（AI），因为人类的智能实际上非常专门化。我们以为自己有通用智能，但事实并非如此。不过，一旦我们有了蓝图，我们就可以更好地思考如何让它安全。这有点像
1920
年代的情景，如果那时有人告诉你几十年后，我们会用接近音速的速度让数百万人穿越大西洋，你可能会问：“那怎么确保安全呢？”但问题是，当时涡轮喷气发动机还没有被发明。我们现在就在类似的情境中。

**主持人：** 你的意思是，AI 的设计不是为了不安全。

****Yann LeCun：****
是的，正如涡轮喷气发动机的设计不是为了不安全一样。涡轮喷气发动机现在非常可靠。最近有一个统计数据，美国航空公司自上一次致命事故以来，人类的总飞行距离达到了
2.3 光年。这是一个惊人的数字，航空非常安全。让 AI 安全的关键在于从设计上确保它的安全。但在我们设计出这些系统之前，讨论如何让它安全是没有意义的。

**主持人：** 你似乎并不担心 AI 想要支配人类。你曾说过，当前的 AI 比家猫还笨。那么，如果 AI
是“愚蠢的”，或者它没有主观意图想要伤害我们，那你认为哪些关于 AI 或 AI 研发的限制是合理的？

****Yann LeCun：****
在研发方面，我认为不需要任何限制。比如，如果你想推出一款可以为你做饭的家用机器人，可能需要设置一些规则，比如当有人在机器人周围时，机器人如果手里拿着刀，就不会乱挥手臂。这就是我们常说的“防护栏”（guardrails）。当前
AI 的设计是“本质上不安全”的。因为当前的 AI
系统很难控制，基本上你需要通过训练让它们表现得符合预期。但我提议了一种新的架构，称为“目标驱动型”（objective-
driven）架构。在这种架构中，AI
系统的唯一目的就是完成目标，并且只能在符合一系列防护栏（其他目标）的情况下完成这些目标。这样可以确保系统的输出和行为都是安全的。很多人会说，这是一个前所未有的挑战，需要发明一种全新的科学。但实际上，我们已经很熟悉这种过程了，这就是“制定法律”。通过制定法律，我们改变人们采取行动的成本，从而塑造他们的行为。同样的原则也可以用于设计
AI 的目标和防护栏。

**主持人：** 你提到，我们需要等到有一个可行的蓝图时，再讨论如何让 AI 安全。那么，对于目前的情况，比如像欧盟那样针对高风险 AI
系统和一般模型的差异化监管，你怎么看？他们已经对一些威胁公民权利的应用，如公共场所的面部识别技术，实施了禁令。你认为这种模式是否适合用来让 AI
更安全，或者你建议等到真的出现问题再去补救？

****Yann LeCun：****
不，不，我并不是建议等到出现问题再去处理。我认为像禁止在公共场所进行大规模面部识别这样的措施是非常好的。除非你是一个威权政府，否则没有人会觉得这是个坏主意。当然，这在某些国家已经存在，但在民主国家，这种禁令非常合理。类似的措施，比如针对某些技术在非法用途上的滥用制定具体规则，也是完全可以接受的。我反对的观点是认为
AI
本身就是一种内在的危险，必须通过监管研发来应对。这种方法是适得其反的。如果未来我们需要那些开源平台来支持民主，那么对研发的监管只会让开源变得风险过高，导致没有公司愿意分发这些平台。结果就是，所有的控制权都会集中在少数几家私人公司手中。

## 7、科技巨头对AI的垄断

**主持人：** 这是否意味着你担心 AI 被少数公司垄断，比如 OpenAI、微软、Google，可能还有亚马逊和 Anthropic？

******Yann LeCun：** 是的，这正是问题所在。如果所有人的数字生活都由这几家公司通过 AI
助手来决定，这对民主来说将是灾难性的。尤其是对于美国以外的政府来说，他们无法接受这样一个未来——所有的文化和价值观都由美国西海岸的几家公司主导。因此，开源平台是必要的，这样各国可以根据自己的文化、价值体系和兴趣，对这些平台进行微调。用户需要选择，而不是被迫接受少数几家公司的产品。

**主持人：** 你最近获得了 2024 年 VinFuture 奖，以表彰你在深度学习领域的变革性贡献。在你的获奖演讲中，你提到 AI
的学习方式与人类和动物不同。你还提到，AI doomers（AI 的悲观主义者）不相信人性本善，而你相信。你能谈谈这一点吗？

****Yann LeCun：****
当然。在未来的某个时刻，我们会拥有一些能够像人类和动物一样学习的系统。它们能够以令人难以置信的速度学习新技能和任务，这种能力目前的机器还无法复制。例如，像特斯拉这样的公司有成千上万甚至数百万小时的驾驶数据可以用来训练
AI 系统，但这些系统仍然无法达到人类的水平。

**主持人：** 所以你认为，AI 在这一点上距离人类和动物还有很远的距离？

****Yann LeCun：**** 是的。尽管我们已经取得了很大的进步，但我们仍然无法造出真正的自动驾驶汽车，除非像 Waymo
那样在特定区域进行限制性运行。

**主持人：** 最后一个问题，你提到你对民主的担忧，同时也对威权政府持警惕态度。那么，为什么你认为人性本善，并且 AI 系统可以朝着好的方向发展？

****Yann LeCun：**** 我认为人性本善是因为历史已经多次证明，人类的法律和社会制度能够塑造行为，并为大多数人创造更好的生活条件。AI
系统也是如此，通过设计合理的目标和防护栏，我们可以确保它们的行为符合社会期望。目前我们无法购买家用机器人，是因为我们还无法让它们足够聪明。原因很简单。我们现在训练的大型语言模型（LLMs）和聊天机器人，依赖的是所有公开可用的文本数据，大约是
20 万亿个单词。这些文本数据的总量约为 (10^{14}) 字节

而发育心理学家会告诉你，一个四岁的孩子在清醒状态下已经经历了 16,000 小时。在这期间，孩子的视觉皮层每秒接收到大约 2 兆字节的数据，总量也是
(10^{14}) 字节。一个四岁的孩子从视觉中获取的数据量，与我们最大的 LLM
从文本中获取的数据量相当。这说明，仅仅依赖文本训练，永远无法达到人类水平的智能。我们必须通过感官输入，比如视频，让系统观察和理解世界。16,000
小时的视频相当于 YouTube 上 30 分钟的上传量，因此数据源是几乎无限的。未来几年的重大挑战是让 AI
系统通过观察视频和与世界互动来理解物理世界。尽管目前尚未完全解决，但未来五年内可能会取得重大进展。所以我认为这是为什么许多公司正在开发人形机器人，尽管它们现在还不够智能，但他们在为未来的
AI 进步做准备。他们相信，当这些机器人可以上市时，AI 的能力将足够强大支持这些机器人。

# 提问环节

**观众1：** 你好，Yann，我是 Michael Robbins。我们之前在 LinkedIn 上有过一些交流。我想问的是，在 AI
和空间计算、环境技术交叉领域的治理方面，你怎么看待未来的发展？

****Yann LeCun：**** 首先，我不是产品专家，也不是政策专家。我从事的是 AI 基础研究。不过，像你提到的 Google
Glass，最初的失败更多是因为社会互动上的问题。相比之下，现在的一些智能眼镜已经克服了类似的障碍，比如在拍照时会有一个小灯亮起，告诉别人你在拍照，这和用智能手机拍照没有太大区别。我个人强烈反对在公共场所大规模使用面部识别技术，因为这是一种对隐私的侵犯。但要解决这个问题，目前还没有“万能子弹”。

  

**观众2：** 从认知科学的角度看，我感兴趣的是如何为理解 AI
的“主观性”奠定基础。更重要的是，AGI（人工通用智能）是否会获得类似于人类感知的能力，而不仅仅是信息处理能力？

****Yann LeCun：**** 这是一个很好的问题。我要稍微说点有争议的内容。我们正在构想的 AI
系统蓝图表明，这些系统未来将拥有“情感”。这是它们设计中不可分割的一部分。为什么 AI
系统会有情感？因为这些系统是以目标驱动的。你给它们一个需要完成的任务，系统的设计目的就是完成这个任务，同时满足一些内置的防护栏（guardrails）。为了实现这些目标，系统需要具备几个核心组件。首先是判断目标是否已经实现的能力。其次是“世界模型”（world
model）。世界模型相当于人类前额叶皮层的一部分，帮助我们预测行动的后果，并规划一系列行动以实现特定目标。如果系统能够预测某个目标是否会被满足，那么它可以评估结果的好坏。当预测结果不好时，系统会感到“恐惧”；当预测结果很好时，会有类似“欣喜”的反应。因此，这种预测和行为的能力，自然会生成类似情感的状态。所以，拥有世界模型、能够进行推理和规划的智能
AI 系统，也会具备情感。

**观众3：** 我想问，你能否分享一两个 AI 的正面例子？比如 AlphaFold 对医学领域的影响，或者最近 Jensen Huang
和印度的安巴尼提到，如果没有 Llama 3，他不会想到能用 AI 教育印度的机会。作为学生和教师的助手，AI 能发挥什么作用？

****Yann LeCun：**** Llama 3，或者我们称为“Llama
家族”，因为它是开源的，已经被广泛用于各种用途。例如，有些人专门针对某个垂直领域或某种语言对它进行微调。在塞内加尔，一位前同事开发了一款医疗助手的聊天机器人，它能用法语、沃洛夫语以及其他当地语言交流。我们还与印度的一些组织合作，希望下一版
Llama 能覆盖印度的 22 到 29 种官方语言。当然，这还不能覆盖印度的全部语言。印度有 700
多种语言，其中大部分是口语，没有书写形式。现在我们有技术让聊天机器人处理这些纯口语的语言，这真是令人惊叹。我上周刚在越南参加了一个奖项活动，类似的努力也在进行，目标是让
AI
支持越南语。这种情况在全球范围内都能看到。人们正在微调这些模型，并将其用于我们从未想到的应用。我认为我们需要一种更有组织、更具全球合作性的伙伴关系，让这些基础模型从一开始就支持所有的语言和文化。这将使各种应用系统的开发变得更加容易。在印度农村地区进行的实验，把这些智能眼镜分发给农民。他们他们非常喜欢这些设备。农民可以用自己的语言提问，比如观察一棵植物，问
AI 助手植物得了什么病，以及如何治疗；或者询问未来一周的天气情况。这种技术的影响可能是深远的。

  

**观众4：** 你多次提到分布式系统作为实现人类水平 AI 的路径。我过去一年中参与训练了很多开源和闭源的 AI
模型，它们在英语和一些语言上的表现很好。但我上周访问了约旦，发现 ChatGPT 和其他 LLM 在那里的表现非常糟糕。我感觉自己过度承诺了 AI
可以解决所有问题。你怎么看待这个问题？

****Yann LeCun：**** 确实如此，目前的 AI
系统在许多语言上的表现还有很大的局限性。分布式系统的目的正是为了应对这一问题。我之前提到的推文是为了激励世界各地的人们合作行动，比如建立本地的数据中心，特别是
AI 专注的数据中心。如果有资源的话，政府可能会在其中发挥重要作用。这些本地化的数据中心将有助于针对特定语言和文化进行优化，最终让 AI
能更好地服务于全球用户。不仅仅是私人行业，全球各地的人们也在收集有助于文化和语言发展的材料。例如，最近由阿联酋的 MBZ 人工智能大学发布了一款基于
Llama 3 的衍生模型。他们对 Llama 3
进行了微调，使其能够首先使用阿拉伯语，特别是阿联酋地区的阿拉伯语，并为医疗服务提供支持。这实际上是一个多模态系统，也可以分析医学图像等内容。这些工作之所以能够进行，是因为
Llama 3 是开源的。如果不是开源的，这些工作就无法实现。

**观众5：** 最后一个问题，有人认为 AI
会改变学习，改变生活，使一切变得更好。可是我们也看到社会中的仇恨、功能失调、孤独感、尤其是女孩的自尊问题，甚至是控制权集中在亿万富翁和政府手中。为什么这次我应该信任你？

****Yann LeCun：****
首先，我不是亿万富翁（笑）。我首先是一名科学家。而且，我必须能够面对自己，保持科学的诚信。我可能是错的，但你可以相信，我不会对你撒谎，也不会因为贪婪等不良动机而误导你。不过，正如科学的本质，我可能会错，而纠正错误的过程来自于不同观点的碰撞和辩论。

**主持人：** 那怎么解释对 AI 的批评，比如说它会让社会充满虚假信息或仇恨言论？

****Yann LeCun：**** 从证据来看，我们并没有看到这种情况。人们确实在传播仇恨言论和虚假信息，但他们会通过各种方式试图传播这些内容，而 AI
是我们对抗这些问题的最佳工具。让我举个例子，以 Facebook 为例。在 2017 年，AI 技术还不足以帮助 Facebook 和 Instagram
识别所有语言中的仇恨言论。当时，自动检测和删除仇恨言论的比例只有 23%。到 2022 年底，这一比例达到了 95%。这一进步正是 AI
技术发展的结果。因此，AI 不是被用来制造仇恨言论或虚假信息的工具，而是对抗这些问题的最佳手段。关键在于让“好人”手中的 AI 比“坏人”手中的 AI
更强大。我确实担心坏人的行为，但 AI 是解决这些问题的利器。

**主持人：** 这是一个很好的回答，非常感谢！

  

AI 小贴士💡 关注 Meta 开源的 Llama 系列模型免费使用和调整这些模型，为项目赋能📚 建议先掌握基础知识再逐步深入以理性和务实的态度看待 AI
的发展

## 往期回顾

[1、[打破传统认知，纽约时报专访 Sam Altman：AGI
进程如何正超预期加速？]](https://mp.weixin.qq.com/s?__biz=Mzg5NTc4ODkzOA==&mid=2247494553&idx=2&sn=984bc37d7eca7dabad8939702b3b995f&scene=21#wechat_redirect)

[2、[专访谷歌CEO
Pichai：公司25%代码都由AI协助完成，2025年将迎来重大技术突破]](https://mp.weixin.qq.com/s?__biz=Mzg5NTc4ODkzOA==&mid=2247494576&idx=2&sn=a9e95867f1e9579a8beb6c03358ae018&scene=21#wechat_redirect)

[3、[陶哲轩对话OpenAI：从数学出发，AI如何改变每个人的工作方式？]](https://mp.weixin.qq.com/s?__biz=Mzg5NTc4ODkzOA==&mid=2247494697&idx=2&sn=4baa728bcf666c12949e46920339fa7c&scene=21#wechat_redirect)

* * *

### 💡 看到很多读者在问"如何开始AI转型"，我们建了个实战派AI团队（成员来自复旦、浙大、华为、阿里等），专注帮企业做"轻量级"AI落地：

  * 🎯 公司该从哪个环节开始用AI？

  * 🛠️ 具体怎么落地才不会踩坑？

  * 💰 投入产出比怎么才最大？

**我们团队专注企业AI解决方案**

业务流程AI优化提升运营效率降低人力成本定制AI应用开发场景化解决方案快速交付落地AI转型咨询规划专业评估诊断精准转型方案

联系负责人：Milo-1101（仅限企业客户）

原视频链接：https://www.youtube.com/watch?v=UmxlgLEscBs&t=1643s

素材来源官方媒体/网络新闻

**\--END--**

  

