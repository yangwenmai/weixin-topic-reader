# 深度对话：谷歌如何打造全球最强AI智能体 “Project Astra”？（附视频）

文章作者: AI深度研究员
发布时间: 2024-12-30 07:12
发布地: 上海
原文链接: http://mp.weixin.qq.com/s?__biz=Mzg5NTc4ODkzOA==&mid=2247495077&idx=2&sn=25c8c456e9185a55144465f196185977&chksm=c0085f40f77fd65626faabb0b6acff1722cf20ea47ff044a20facfd48a39be17fc62d479233d#rd

封面图链接: https://mmbiz.qpic.cn/mmbiz_jpg/iaqv2tagPYAgckkZDn7mppsk9xWdOoRAibGZo0u3Vnmzy54V4bNyFwU4lELOL17HzSMa4Kn21sqYnN7ibIAA2I2pQ/300

**（关注公众号并设为🌟标，获取最新人工智能资讯和产品）**

全文8,000 字，阅读约需16分钟

> **——导语：** 在AI助手和智能体的竞争中，谷歌再次展示了其技术实力。通过 Project
> Astra，谷歌不仅勾勒出了下一代AI智能体的蓝图，更暗示了未来企业竞争的新范式。本期特别邀请 DeepMind 研究总监 Greg Wayne
> 解密这款全球最强AI智能体的打造之路，以及它将如何重塑商业格局。

在全球AI巨头竞相布局智能助手的关键时刻，谷歌 DeepMind 正在下一盘更大的棋。近期，DeepMind 研究总监 Greg Wayne
在与著名科技主持人 Hannah Fry 的深度对话中，首次揭开了其最新研究项目 “Project Astra
”的神秘面纱。这个被称为"下一代AI智能体"的研究原型，不仅展现了谷歌在通用人工智能领域的雄心，更暗示了未来企业竞争格局的重大转变。

Project Astra
的与众不同之处在于其突破性的技术整合——多模态感知、上下文理解、实时推理，以及跨语言交互能力。更具战略意义的是，它摆脱了传统AI助手对特定设备和界面的依赖，预示着一个全新的人机交互范式即将到来。正如
DeepMind 团队所展示的，这种技术进步将重新定义企业的数字化转型路径。

五年前，这样的技术突破还被视为遥不可及的未来。而今天，通过 Project
Astra，我们得以一窥下一代AI助手的商业潜力：从个人知识增强到残障辅助，从技能提升到智能协作，每一项应用都可能催生新的商业模式和市场机会。对于关注AI发展的企业和个人而言，Project
Astra 不仅是一个技术里程碑，更是一个预示行业变革的重要信号。

# 采访文稿

**主持人：** 2025年即将到来，随之而来的还有代理型人工智能的时代。当然，欢迎回来，Greg！

**Greg Wayne：** 你好，Hannah。

## 1、Astra计划

**主持人：** 那么，让我们从头开始吧。什么是Astra计划？

**Greg Wayne：**
Astra计划是一个团队、一个项目、也是一个原型，旨在打造一个拥有视觉、听觉和语言能力的AI助手，它可以与你同在，无论你身处何地。无论是通过智能眼镜、手机还是电脑，它都可以看到你在做什么，并与你交流。就像坐在你肩膀上的一个小助手，就像一个栩栩如生的鹦鹉陪伴在你身边，与您谈论世界。而且比你聪明。这与谷歌助手或Gemini有些不同。早期的谷歌助手主要是为家庭或信息服务的命令与控制系统——比如播放YouTube上的歌曲。而Astra的重点是与现实世界的互动。它是基于Gemini的技术构建的，同时也结合了其他系统。可以说，它与Gemini相辅相成，彼此影响和塑造。

**主持人：** 那么硬件呢？它目前是在智能手机上运行，但将来会出现在眼镜中吗？

**Greg Wayne：**
但不仅仅如此。当这个项目早期启动时，我们就试图探索，如果将AI整合到智能眼镜中，它会有多大用处。在智能眼镜中，这是最亲密、某种程度上最惊人的体验。你会感觉自己被增强了，就像一个智能版的自己与你交谈，回答你的问题。但实际上，软件堆栈对设备的类型是中立的——虽然会针对不同设备进行一定的优化，但它可以运行在手机、电脑或VR头戴设备上。

**主持人：** 我刚刚和它互动时也在想，这对视力障碍人士来说可能会有潜在的帮助，对吧？

**Greg Wayne：**
这是我非常关注的一个方向。我们把这种AI称为“共同存在”或“共享视角”。有时，你需要另一个可以“看”和“听”的智能系统在你身边，但并不是每时每刻都需要。那么，在什么情况下，你需要一个能够与你一起“看”的系统呢？比如，当你能看到但不理解，或者当你看不到的时候。这是一个非常重要的类别。目前全球有数亿视力障碍人士。那么，对于这个群体来说，什么是帮助的黄金标准呢？那就是有人在他们身边，帮助他们应对世界。而这种技术在很大程度上能够复制这种帮助。

**主持人：** 所以，比如说，自闭症患者可能也能用它来帮助理解面部表情？

**Greg Wayne：**
是的。虽然我不会推荐现在就把它当成一种“处方药”，但随着进一步开发，它完全可以用作辅助工具。你还可以用它来训练自己，比如学习理解面部表情，Astra可以给你反馈：“嘿，这是什么意思？”我记得有一次不同的话题，当年我在一个寄宿家庭学习法语时，我一直发不准一些词，比如“街道”（la
rue）和“轮子”（la
roue）之间的区别。我试着模仿寄宿家庭的小伙伴，但他坚持了一会儿后就不耐烦了。但阿斯特拉会无限耐心，帮助你练习这些东西。还有记忆功能，我们有一个我们称为“完美会话内记忆”的系统。相机开启时，它基本上能“摄影式”记住过去10分钟内的内容，同时还能记住你过去谈论的事情。这就是为什么它记得我是Greg。如果我们再次启动它，并问“你还记得上次和Greg一起跟你对话的人是谁吗？”它会记得是你。它甚至能提醒你一些事情比如你要回家时，它会提醒你，“别忘了买橙汁，你早上喝完了。”我觉得现在的阶段更多是在描绘可能性的蓝图，目前这些功能还没有完全实现，但这就是我们下一步可以构建的东西。

**主持人：** 那它在什么环境下会比较困难？比如在嘈杂、昏暗或混乱的环境中，它表现如何？

**Greg Wayne：**
在某些环境下，它确实会有困难，尤其是嘈杂条件下。阿斯特拉会直接处理音频，将声音编码成某种信息包，由语言模型Gemini直接处理。但它目前还不能很好地区分不同的声音来源，比如你的声音和我的声音。嘈杂的环境可能会让它感到困惑，误把旁人的对话当成用户的指令。这的确是未来需要解决的一个重要问题啊。

**主持人：** 当你说能够区分不同的声音时，是指声音波形本身吗？

**Greg Wayne：**
有一个经典的问题叫“鸡尾酒会问题”，它涉及到技术上被称为“源分离”的问题。简单来说，就是将一个声音源与另一个声音源区分开。例如，当一个人在弹吉他，而另一个人在唱歌时，你可以将它们分离成两个轨道——一个是吉他的轨道，一个是人声轨道。同样地，你可以将一个说话者的轨道与另一个说话者的轨道区分开。这可以通过单一模态的音频完成，也可以通过多模态的方式完成，结合多种感知。例如，当我知道是你在说话时，我可以看到你的嘴唇在动，而不是其他人的嘴唇。最终，系统会利用各种线索来调整它对声音的感知。这也许正是让“Astra计划”如此困难的地方，同时也是它的潜力所在。正如你提到的“鸡尾酒会问题”，人类在这方面非常擅长——比如在一个鸡尾酒会上，即使周围有很多声音，你仍然能听清旁边人在说什么。不过总的来说，人类还是在这方面表现得相当不错。而仅仅通过音频解决这个问题是非常困难的，但由于Astra是多模态的，它有视频、有音频，还有后台运行的文本语言模型，因此它可以利用更多的工具来解决问题。我认为通过增加上下文，它应该能够解决更多的歧义。

**主持人：** 那不同语言呢？它目前只支持英语吗？或者只能理解带有清晰口音的语言？

**Greg Wayne：**
对我来说，它主要是用英语的，但实际上它是非常多语言的。由于它是原生音频模型，它已经能够以相当高的水平支持大约20种语言，并且你甚至可以在同一对话中切换语言。它会根据你开始使用的语言进行回应。所以实际上你先用英语开始，然后说了俄语。如果你一开始就用俄语，它可能会更好地用俄语回应。这确实不同于我们目前使用的聊天机器人，这是一个额外的能力。我对用这个系统进行语言学习非常兴奋。比如走在路上，你可以问“那是什么？”它会教你，就像我在学校学习法语时，我们会带着物品到课堂上，用法语讨论这些物品一样。可以想象，当你迷失在一个陌生的城市时，这会是一个很有帮助的工具。

## 2、Astra技术架构

**主持人：** 那么，这一切运行的背后是什么样的技术架构？有哪些不同的组件？

**Greg Wayne：**
首先是一个应用程序，它负责收集视频和音频输入，然后连接到服务器。服务器上运行着多种神经网络模型。比如一个视觉编码器和一个音频编码器，还有一些专门的音频系统，它们负责识别用户是否停止了说话。这些编码器与大型语言模型Gemini协同工作，将感知编码器的信息直接传递给Gemini，由它生成回应。我们还与Gemini团队合作改进了模型，使其更擅长对话和音频处理。听起来像是一个复杂的系统，但也非常强大。我试着想象这个系统如何识别一本书。这里涉及的不同元素实在太多了：计算机视觉、语音识别、大型语言模型、谷歌搜索的支持、负责决策的代理层……而且它几乎是在没有任何延迟的情况下完成所有这些工作。这真是一个异常复杂的系统。确实非常复杂。当然，作为工程师，我们会创建一些抽象层，这样我们就不需要同时考虑所有复杂性。但总体来说，它确实是一个巨大的复杂系统。模型处理的数据只有很少一部分人能够理解，而为什么它会产生某些结果，可能没有人能够完全理解，因为这些结果只是基于基准测试的表现。

**主持人：** 让我谈谈这个项目的历史吧。在本播客的第一季，当时，你从动物界的灵感出发，研究智能，特别是有一种鸟类——西部灌丛松鸦（western
scrub jay），你说它可以为AI的记忆提供灵感。

**Greg Wayne：** 是的，这是一个主动记忆的例子。

**主持人：** 而这正是你在Astra计划中实现的。

**Greg Wayne：**
我觉得，从某种意义上说，智能本质上是一种整体性。当你从事研究智能的职业生涯时，你会以各种方式试图理解它，与之对抗或合作。而这个项目可能是我人生中所有研究方向的最强统一体。尽管它还缺少一个重要部分，那就是它没有一个物理化的身体，无法在现实世界中行动。我认为记忆和感知一直是我长期以来感兴趣的领域。而这个项目将它们结合在一起，同时还能让人们感到与之产生联系。

**主持人：** 那么，你的神经科学背景对Astra计划有多大的启发？

**Greg Wayne：**
神经科学在两个方面发挥了作用。第一，它帮助我们判断是否已经做得足够好，思考“记忆真正的意义是什么？”以及“我们是否已经达成了目标？”第二，它也提供了一种动力，推动我们尝试创造一种与人类兼容、以某种方式类似于人类的智能化身，而不是一个简单的文本接口。比如，我一直对Michael
Tomasello的研究感兴趣，他通过研究人类与类人猿的比较，探讨人类的交流。他的核心观点是，交流的基本前提是两个个体身处同一地点，共同指向关注的焦点，从而推断目标并进行合作。这正是我们在这项技术中试图建模的内容。这更像是一种理论层面的灵感，而不是直接复制的设计。在解决问题或工程设计方面，它需要依赖于技术本身来提出不同的解决方案。

**主持人：** 如果Astra计划与我们几年前讨论过的内容相关联，这个项目的最初火花是从哪里来的？它实际上是什么时候开始的？

**Greg Wayne：** 我想，我记得DeepMind的CEO Demis
Hassabis向公司提出了一个挑战，让我们思考什么是“原型人工通用智能”（proto
AGI）。这个概念指的是，如果我们创造了一个系统，让技术专家可以仔细研究、使用和体验，他们会得出结论：真正的人工通用智能只是时间问题，而非可能性问题。这为我们提供了很多创意空间，比如有人认为智能可以像AlphaZero一样通过与世界互动而产生；而我的想法更关注智能的社会性。作为人类，如果不向他人学习或从书本中学习（本质上也是向他人学习），我们就不算真正聪明。这种社会性智能的理念为我提供了方向。同时，我也想到可以将这种智能与一个以帮助人类为目标的助手结合起来。这两个想法共同引导了我对项目的思考。最终，我意识到，视频可能是系统的最终连接媒介，这也成为了我们发展的重要方向。在整个开发过程中，我想我们确实经历过几个重大突破。项目的第一阶段基本上是一次黑客马拉松，我们用两周时间制作了第一个版本。当时还有一段视频记录，但这个版本非常粗糙。我记得工程师马尔科姆·雷诺兹在办公室里测试Astra，他问：“这是什么？”系统回答：“一棵植物。”然后他又问：“这是什么种类的植物？”系统还是回答：“一棵植物。”（笑）系统当时的灵活性还非常有限。我还记得最初的一个演示有长达7秒的延迟。

**主持人：** 所以，你会说“你好，Project Astra”，然后7秒后它才会有反应？

**Greg Wayne：**
是的，当时的延迟让它几乎无法使用。你会以为系统“走开了”，实际上它只是7秒后才回应。解决这些问题的过程中其中一个重要发现是关于提示词（prompt）的概念。提示词是你给系统的操作指令。这些系统可以理解语言，所以你可以告诉它，“你的名字是Astra，你是一个智能且有帮助的AI助手”。有些信息现在已经内置在Gemini模型中，但有些信息仍然需要通过提示词明确表达。当时，我们并不确定能否很好地为多模态系统设计提示词。一个令人震惊的发现是，当我们告诉系统它可以通过用户的摄像头“看到”世界时，它突然理解了自己的视角。之前它并不理解这个概念，总是犯错误。但当我们明确告诉它，“你是一个通过用户摄像头观察的AI”，它的回答就开始正确了。虽然之后还有很多工作要做，但这一发现表明，我们可以通过提示词有效地定义系统的理解，即使这是一个与之前完全不同的系统。

## 3、原型人工通用智能

**主持人：** 在公司提出开发“原型人工通用智能”的挑战时，有人质疑过这种目标的可能性吗？

**Greg Wayne：**
在人工智能领域，回头看总是很有趣，因为技术发展太快，人们对“显而易见”的看法也变化得很快。现在很多人觉得这是显而易见的，但我想说的是，当时需要做大量的说服工作。从许多不同的角度来看，人们都觉得这个项目有点奇怪。比如，当时的视觉系统处理的图像分辨率只有96×96像素，而我们的屏幕分辨率至少是1000×1000像素。所以系统几乎无法看到细节，更不用说识别植物的种类了。此外，人们还认为，这些系统是否真的能理解它们“看到”的东西，而不仅仅是对其进行分类，这似乎是一个太超前的目标。

**主持人：** 既然当时一切看起来都如此荒谬，你们为什么还要继续？

**Greg Wayne：**
虽然困难重重，但我从未怀疑过这是可能的。尽管有时候进展缓慢，尤其是在Gemini出现之前，事情进展得并不顺利，但我始终相信这个方向是正确的。我可能有点固执，总觉得只要坚持足够久，它一定会成功。我们有一个专门的房间，里面有各种好玩的东西。比如一个小酒吧，Astra可以帮你调制饮品；还有一个艺术画廊，你可以在屏幕上展示不同的画作，与阿斯特拉讨论艺术。

**主持人：** 那延迟问题呢？你提到最初有7秒的延迟，现在是如何改进的？

**Greg Wayne：**
改进发生在多个层面。比如我们优化了视频流的传输速度，确保信息通过应用程序更快地传递。此外，我们在同一计算集群中运行视觉系统、音频系统和语言模型，减少了跨地域的数据调用延迟。所以，为了实现实时理解，你们需要将运行这些模型的计算硬件物理上靠近，这真的是有影响的。让硬件靠近可以显著减少延迟，提升系统的实时性能。将模型集中在一起运行确实是一方面，但还有其他关键改进。例如，我们确保了上下文缓存的机制，系统可以逐步更新它与用户交互的历史上下文。还有一个改进是原生音频处理的使用。之前的系统需要一个语音转文本的识别系统：首先将音频转录成文本，然后调用语言模型生成响应，最后再返回结果。这种流程会增加额外的延迟。

现在，这个系统可以直接处理音频，不需要中间的转录步骤。这使得它能够理解罕见的词汇或名字的发音，比如“Demis
Hassabis”。旧系统经常会将这个名字识别为“Damascus”，但现在它能够识别正确的名字，并利用上下文来进一步确认，比如“DeepMind的CEO是Demis
Hassabis”。另一个例子是有人最近发现的一个小演示：区分“scohn”和“scahn”这两种发音。阿斯特拉可以识别到用户说的是不同的词，而不是简单地将它们转录成同一个词。最后，团队在“端点检测”（endpointing）方面也做了大量改进。这是一种技术，用来准确判断用户什么时候停止说话。系统能够非常准确地感知到用户的语句是否完成，从而决定何时开始回应。此外，还有更复杂的一点是，系统会提前规划回答，即使用户尚未完成发言。它会基于推测预先准备回答，当它确定用户已经说完后，就会立刻发送准备好的答案。

**主持人：** 这太有趣了！因为实际上，人们在说话时，重要的信息往往在句子的中间部分，最后可能会拖延或停顿。而系统可以利用这些时间准备答案。

**Greg Wayne：**
基本上就是这样（笑）。几年前我们讨论这些时，还觉得这些目标过于复杂，而今年它终于开始有效运作了。预判答案并在对话到达相应节点之前就准备好，确实是一个复杂的任务。我们在对话中经常停顿，系统必须使用一定的“语义理解”，结合上下文和声音信号，来判断用户可能已经完成了发言。

**主持人：** 那它的推理能力呢？不仅仅是判断句子是否结束，你觉得Project Astra具备推理能力吗？

**Greg Wayne：**
它的推理主要通过其神经网络的内部结构完成，这个过程非常复杂且难以观察。此外，推理还体现在它生成的对话中，你可以听到它在回答中“思考”的过程。目前也有一些研究在开发具有“内在对话”（inner
speech）的系统，这种系统在与你对话之前会先“自言自语”，但Astra目前在这方面的功能还不多。

**主持人：**
所以，推理模型的发展是否和Astra项目中的其他部分紧密相连？因为感觉这个项目的核心目标就是整合一切，从而实现你所说的“原型人工通用智能”（proto
AGI）。

**Greg Wayne：**
是的，我也希望这个项目能激发对某些推理方面更有力的研究。我们有一个很好的例子：产品经理有一天在午餐时用Astra问，“我的盘子上有多少卡路里？”她的盘子非常复杂，也很精致，上面有六种食物，中间有一些杏仁，还有一块猪里脊，以及一些球芽甘蓝。Astra最初有些犹豫，但当她说“算一个总数”时，系统开始逐一计算，比如“球芽甘蓝有七颗，所以有这么多卡路里”，然后再加上猪里脊。让我印象深刻的是，她在引导系统进行思考。有时候，系统确实需要一些指导。但我认为，我们离拥有一个可以自动完成所有推理的系统并不遥远。比如，系统可以直接看到“七颗杏仁、几颗球芽甘蓝和一块猪里脊”，然后自动计算出总卡路里。这种推理能力目前还不够强，因为我们从未认真尝试去构建这样一个系统。

## 4、AI助手推理和记忆能力

**主持人：**
那我们谈谈记忆吧。关于系统能够记住和回忆的信息，就像它的“记忆”一样——请原谅这种拟人化的说法。我记得在谷歌I/O大会上，它能记住过去45秒内发生的事情。而现在你们将时间延长到了10分钟，对吧？

**Greg Wayne：**
是的，现在是10分钟，甚至在某些方面稍微长一点。不过10分钟是一个较为标准的设置。系统基本上会记录最近10分钟的原始视频数据，它每秒处理一帧图像。这意味着系统保存了大约600帧视频，以及这些帧之间的所有音频记录。限制主要在于芯片上的存储器容量，这种快速存取内存的规模在过去十年间并没有显著提升。

**主持人：** 那么现在它实际上就像一个录像机一样，记录过去10分钟内发生的所有事情？

**Greg Wayne：**
它确实在积极记录这些信息，并可以立即使用。同时还有一个辅助系统，当你关闭系统时，它会总结对话并提取相关的事实。也就是说，它会根据自己的判断来决定哪些信息是最重要的。

**主持人：** 目前，它能够从最近的互动中回忆重要的内容吗？

**Greg Wayne：**
它有一种“双流记忆”机制。一种是关于用户本人的记忆，它会逐步形成对用户的理解。比如，它会记录“用户喜欢巧克力冰淇淋”，并在每次会话后更新这些信息。如果用户后来告诉它“不喜欢冰淇淋了，更喜欢蛋糕”，它会更新为“用户表示不再喜欢冰淇淋，而喜欢蛋糕”。这些信息构成了一个静态的用户画像，描述你的喜好和偏好。另一种记忆是会话总结，比如“星期二8:50，我们讨论了国际象棋比赛”。

**主持人：** 那么，它如何决定哪些信息应该归入哪一类？哪些信息重要到需要记住？

**Greg Wayne：**
系统有一套启发式规则。这些规则本质上是对它的指导。比如，如果用户明确要求它记住某件事，它就一定会记住。这是一个非常简单明确的规则，比如“记住我的门禁密码”。除此之外，它会进行猜测，试图判断用户是否表达了任何有趣或与之前表达不同的偏好，然后基于这些信息进行更新。

**主持人：** 那么，我们来谈谈一些隐私方面的担忧吧。你们是如何应对这些隐私问题的？

**Greg Wayne：**
好的，我认为一个主要的标准就是用户的同意。用户可以访问他们的历史数据，查看存储的内容，甚至删除它们。每次用户删除某些数据时，系统会重新整理关于用户的所有知识。它会重新总结它所知道的所有关于你的信息。所以用户对它了解自己的内容拥有一定的控制权。

**主持人：** 其实在这个播客的前几集里，我们采访了DeepMind的伦理学家Iason
Gabriel。他非常棒，他向我们讲解了关于AI助手伦理的一些复杂问题，以及应该如何设计这些助手以应对这些问题。他的研究对Astra计划有多大帮助？

**Greg Wayne：**
我们把他的243页报告直接输入Astra，它说：“好的，我明白了。”我们与Iason讨论了很多，也与他所在的团队合作了许多。他们不仅研究模型和代理的整体表现，还与一些外部的“红队”（专门模拟攻击系统的团队）合作，以测试在不同情况下系统的表现，特别是进行对抗性攻击。此外，我们还有一层安全过滤器。比如，如果用户对系统说某些不适当的话，或者向它展示不好内容，这些过滤器会被触发，系统不会响应。系统也会对它自己的言论进行过滤，避免产生某些类型的回答，尽管这种情况发生得很少。总的来说，这些问题的范围相当广，但幸运的是，我们还有时间进一步优化这些内容。

**主持人：** 好的。那么接下来的重点是什么？在未来几个月里，你们会集中精力在哪些领域？

**Greg Wayne：**
我对一个叫主动视频处理的方向非常感兴趣。也就是说，系统不仅仅是在用户说话时作出回应，而是可以持续帮助用户。例如，这是“为视障人士提供视觉解释”问题的一部分。你走路时，如果看不到前方的障碍，系统会提醒你：“注意前面的桌子。”它可以在你行动时持续引导你。我们还在研究更多的音频输出，比如所谓的全双工通信，系统能够同时听和说。这种功能可能会有点烦人，比如它可能会打断你，但也更接近自然对话。就像我在听你说话时可能会回应“嗯哼，嗯哼”，这种边听边说是语言确认的一部分。此外，我们还会进一步加强推理能力、深度记忆，以及更复杂的工具调用，让系统可以进行更深入的查询和研究。要改进的东西太多了。

**主持人：** 非常感谢你加入我们的节目，Greg。

**Greg Wayne：** 谢谢你。

## 【往期回顾】

[1、[打破传统认知，纽约时报专访 Sam Altman：AGI
进程如何正超预期加速？]](https://mp.weixin.qq.com/s?__biz=Mzg5NTc4ODkzOA==&mid=2247494553&idx=2&sn=984bc37d7eca7dabad8939702b3b995f&scene=21#wechat_redirect)

[2、[专访谷歌CEO
Pichai：公司25%代码都由AI协助完成，2025年将迎来重大技术突破]](https://mp.weixin.qq.com/s?__biz=Mzg5NTc4ODkzOA==&mid=2247494576&idx=2&sn=a9e95867f1e9579a8beb6c03358ae018&scene=21#wechat_redirect)

[3、[陶哲轩对话OpenAI：从数学出发，AI如何改变每个人的工作方式？]](https://mp.weixin.qq.com/s?__biz=Mzg5NTc4ODkzOA==&mid=2247494697&idx=2&sn=4baa728bcf666c12949e46920339fa7c&scene=21#wechat_redirect)

* * *

### 💡 看到很多读者在问"如何开始AI转型"，我们建了个实战派AI团队（成员来自复旦、浙大、华为、阿里等），专注帮企业做"轻量级"AI落地：

  * 🎯 公司该从哪个环节开始用AI？

  * 🛠️ 具体怎么落地才不会踩坑？

  * 💰 投入产出比怎么才最大？

**我们团队专注企业AI解决方案**

业务流程AI优化提升运营效率降低人力成本定制AI应用开发场景化解决方案快速交付落地AI转型咨询规划专业评估诊断精准转型方案

联系负责人：Milo-1101（仅限企业客户）

原视频链接：https://www.youtube.com/watch?v=78mEYaztGaw&t=40s&ab_channel=GoogleDeepMind

素材来源官方媒体/网络新闻

**\--END--**

