# 2万字硬核｜Anthropic研究员克里斯：模型可解释性如同研究生物学、什么会刺激模型撒谎或作恶？

文章作者: 瓜哥AI新知
发布时间: 2024-11-15 07:03
发布地: 浙江
原文链接: http://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2650003893&idx=2&sn=6d0f22e1640b876757eab62f5305c3b6&chksm=88ba50b2bfcdd9a4830e778a7f2bf82bda967e19cb9fc1de240a308a501629c4498d1691cebb#rd

封面图链接: https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBPuKYFW3oYvSbrXlwh9UDfdxK4EjStOfo2FJorADlnd8iaPCGBpms3aiah3YiaKicnwGC51OACicWic69dQ/300

**👇关注公众号后设🌟标，掌握第一手AI新动态**

****

本文访谈内容整理自**Anthropic研究员克里斯·奥拉** 接受**Lex Fridman**
Youtube频道专访，公开发表于2024年11月11日。原始内容参考：https://www.youtube.com/watch?v=ugvHCXCOmm4

## Anthropic研究员克里斯·奥拉接受Lex Fridman专访

> ★
>
> **内容导读** ：
>
> 此次专访主要围绕大模型的机制可解释性（Mechanistic Interpretability，简称MECH INTERP）展开，Chris Ola
> 主要表达了以下几个核心观点：
>
>   1. **神经网络并非编程，而是“生长”** :
> 神经网络的构建过程更像生物体的生长，而非传统的软件工程。我们设计架构和目标函数（如同支架和光源），但最终得到的是一个我们需研究的“生物体”。这使得理解其内部运作机制成为一个重要的科学问题，也关乎安全。
>   2. **机制可解释性的核心：算法和机制** : 与仅关注模型输入输出关系的传统方法不同，MECH INTERP
> 致力于理解神经网络内部运行的算法和机制，如同反向工程一个编译后的程序，需同时理解权重（指令）和激活值（内存）。
>   3. **梯度下降的“智慧”和研究的谦逊** :
> 梯度下降算法常常能找到比人类更优的解决方案，因此研究者需保持谦逊，采用自下而上的方法，避免预设模型内部结构，而是从底层数据中发现其运作模式。
>   4. **普遍性（Universality）和线性表征假设** :
> 不同模型，甚至生物神经网络和人工神经网络，会形成相似的特征和电路（例如曲线检测器、高低频检测器等），这表明梯度下降找到了一些普遍适用的、对模型媒介无关的抽象方法来解决问题。
> 这基于“线性表征假设”，即模型中方向具有意义，通过向量加减可以组合概念（如Word2Vec 中的“king - man + woman =
> queen”）。
>   5. **叠加假设（Superposition Hypothesis）和多义性神经元** :
> 模型可能利用高维空间和稀疏性，用少量神经元表示大量概念，这解释了多义性神经元（polysemantic neurons）的现象。
> 这类似于压缩感知，通过低维投影表示高维稀疏向量。学习过程可以看作是在寻找高效的压缩表示。
>   6. **字典学习和单义性特征提取** :
> 为了解决多义性神经元的问题，可以使用字典学习（例如稀疏自编码器）来提取单义性特征（monosemantic features）。
> 这是一种自下而上的方法，无需预设特征类型，而是让算法自动发现。
>   7. **从单层模型到大型模型的扩展** : Sparse Autoencoders 方法可以扩展到大型模型（如Claude 3
> Sonnet），并能发现更多复杂、抽象、多模态的特征，包括与安全漏洞、欺骗行为相关的特征。
>   8. **机制可解释性的未来方向** :
> 未来的研究方向包括：理解模型计算过程，解决干扰权重问题；进一步提升字典学习能力，探索“暗物质”特征；建立宏观层面的抽象，将微观机制与宏观行为联系起来；最终目标是构建神经网络的“器官”或“系统”级别的理解。
>   9. **人工神经网络与生物神经网络的比较** :
> 研究人工神经网络比研究生物神经网络更容易，因为我们可以访问所有神经元、控制数据和修改网络结构，但这并不意味着理解人工神经网络就容易。
>   10. **机制可解释性的美学价值** : 理解神经网络的内部结构本身具有美学价值，如同理解生物进化一样，简单的规则可以产生复杂的、美丽的结构。
> 探索神经网络内部的复杂结构，如同探索一个充满未知的宇宙，充满挑战和机遇。
>

## 克里斯·奥拉简介

克里斯·奥拉（Chris
Olah）是Anthropic公司的一位杰出研究员，此前以其在深度学习可视化和解释性方面的独立工作而闻名。他并非Anthropic的创始人，但作为核心研究人员，对该公司在构建安全、可解释和可操纵的AI系统方面的努力做出了重大贡献。

Olah 的加入对Anthropic
至关重要，因为他将自己多年积累的关于深度学习可解释性的专业知识带到了这家公司。他一直致力于开发能够更好地理解和解释大型语言模型内部运作机制的方法。这与Anthropic
的核心目标——构建更安全、更可靠的AI系统——高度一致。 他的工作风格强调清晰性和可解释性，这对于Anthropic
致力于开发可被人类理解和控制的AI系统至关重要。

在Anthropic，Olah
的工作可能涉及到以下几个方面：改进模型的可解释性，以便研究人员更好地理解模型的决策过程；开发新的技术来检测和减轻大型语言模型中的有害行为；以及设计更有效的训练方法，以提高模型的安全性和可靠性。
虽然他的具体工作内容并未公开披露太多细节，但可以推测，他的专业知识在Anthropic 的模型构建、安全评估和风险管理等方面都发挥着关键作用。

## 访谈原文

**主持人莱克斯：** 现在，亲爱的朋友们，接下来是Chris Olah。你能描述一下这个令人着迷的机制可解释性领域，也就是MECH
INTERP，这个领域的研究内容以及它目前所处的位置吗？

**克里斯：**
我认为，目前理解神经网络的一种有用的方法是，我们并没有编程它们；我们并没有制造它们。我们更像是培育它们。我们设计了这些神经网络架构，并创建了这些损失目标。**神经网络架构就像电路生长的支架。它从一些随机初始化开始，然后生长。它几乎就像我们训练的目标一样，像一盏灯，引导着网络的生长**
。

我们创建了神经网络生长的支架，我们也创建了它生长的方向。然而，我们真正创造的是一个我们正在研究的、几乎像是生物实体或有机体的东西。这个过程与任何常规软件工程都大相径庭。最终，我们得到的是一个能够执行惊人任务的产物——例如撰写论文、翻译语言和理解图像。这些是我们不知道如何直接编程到传统计算机中的能力。

这种区别引出了一个有趣的问题：这些系统内部到底发生了什么？对我来说，这是一个深刻而令人兴奋的问题，值得进行科学研究。这是一个深刻的问题，似乎在呼吁我们去探索，当我们讨论神经网络时，这个问题就出现了。此外，我相信它也是出于安全原因的关键问题。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBPuKYFW3oYvSbrXlwh9UDfdJNKw5u8Dw3e5gft1NRky2LG2mribZiaakaIRdqPUZSVWv1eeYEVkO9ibQ/640?wx_fmt=jpeg&from=appmsg)

**主持人莱克斯：** 所以在机制可解释性中，我想它更接近于……也许是神经生物学。

**克里斯：**
是的，是的，我认为这是对的。所以，也许举个我不会认为是机制可解释性的例子，很长一段时间以来，有很多工作都集中在显著性地图上，你会取一张图像并尝试说，“模型认为这张图像是狗。图像的哪个部分让它认为它是狗？”如果你能想出一个有原则的版本，那也许会告诉你一些关于模型的信息。但这并没有真正告诉你模型中运行的是什么算法，或者模型是如何做出这个决定的。如果你能让这种方法奏效，也许它会告诉你一些对它来说很重要的事情，但这并没有告诉你正在运行的算法是什么。这个系统是如何能够做到这件事的，而这是没有人知道如何做到的？

所以我想我们开始使用“机制可解释性”这个术语来试图画出这条界限，或者在某些方面将我们正在做的工作与其他一些工作区分开来。我认为从那时起，它已成为一个涵盖范围相当广泛工作的总称。但我认为机制可解释性的独特重点是真正理解机制和算法。如果你认为神经网络就像一个计算机程序，那么权重就像一个二进制计算机程序。我们想反向工程这些权重，并找出正在运行的算法。

**理解神经网络的一种方法是，它就像拥有这个编译的计算机程序，而神经网络的权重就是二进制代码**
。当神经网络运行时，这对应于激活。**我们的最终目标是理解这些权重**
。机制可解释性的目标是弄清楚这些权重如何对应于算法。要做到这一点，**你还必须理解激活，因为激活就像内存**
。如果你想象一下反向工程一个计算机程序，你拥有二进制指令，要理解特定指令的含义，你需要知道它正在操作的内存中存储了什么。这两个方面是紧密相关的，因此机制可解释性往往同时关注这两方面。

现在，有很多工作都集中在这些主题上，特别是探测，你可能会将其视为机制可解释性的一部分。然而，它仍然是一个广泛的术语，并非所有从事这项工作的人都认为自己是在做机制可解释性研究。机制可解释性的一种可能略微独特之处在于，在这个领域工作的人倾向于认为神经网络在某种程度上受制于“梯度下降比你聪明”的原则。梯度下降实际上非常好；我们理解这些模型的全部原因是我们一开始不知道如何编写它们。梯度下降通常会想出比我们更好的解决方案。

机制可解释性的另一个方面是一种谦逊的态度。我们不会预先假设模型内部发生了什么。相反，我们采用自下而上的方法，我们不假设我们应该寻找特定的事物，并且它就在那里。相反，我们自下而上地观察，发现这些模型中存在什么，并以这种方式研究它们。

**主持人莱克斯：**
但是，能够做到这一点本身就是一个事实，正如你和其他人长期以来所展示的那样，诸如普遍性之类的现象，即梯度下降的智慧创造了特征和回路，在不同类型的网络中普遍创造了有用的东西。

**克里斯：**
这使得整个领域成为可能。是的，这确实是一件非常了不起且令人兴奋的事情，至少在某种程度上，似乎相同的元素、相同的特征和回路一次又一次地形成。你可以查看每一个视觉模型，你都会发现曲线检测器和高低频检测器。事实上，有一些理由认为，相同的东西在生物神经网络和人工神经网络中形成。

例如，一个著名的例子是早期视觉模型中的Gabor滤波器。神经科学家对Gabor滤波器很感兴趣，并对其进行了大量的研究。我们在这个模型中发现了曲线检测器。在猴子的大脑中也发现了曲线检测器。我们发现了这些高低频检测器。随后的一些研究工作在老鼠身上也发现了它们。因此，它们首先在人工神经网络中被发现，然后在生物神经网络中被发现。

众所周知，有一个关于“祖母神经元”或Quiroga等人提出的“哈利·贝瑞神经元”的著名结果。在我还在OpenAI工作并研究他们的CLIP模型时，我们发现了视觉模型中非常类似的东西。你会发现这些神经元对图像中的相同实体做出反应。举一个具体的例子，我们发现存在一个“唐纳德·特朗普神经元”。我想，每个人都喜欢谈论唐纳德·特朗普。他当时非常突出，也是一个热门话题。

因此，在我们研究过的每一个神经网络中，我们都会找到一个专门用于唐纳德·特朗普的神经元。他是唯一一个总是拥有专用神经元的人。有时候你会有一个“奥巴马神经元”，有时候你会有一个“克林顿神经元”，但特朗普总是会有一个专用神经元。它对他的面部照片和“特朗普”这个词做出反应，所有这些东西，对吧？所以它不是对特定例子做出反应；它不仅仅是对他的脸做出反应；它是在对这个一般概念进行抽象。

无论如何，这与Quiroga等人的结果非常相似。有证据表明，**普遍性现象——相同的东西在人工和自然神经网络中都形成**
——如果这是真的，那将是一件非常惊人的事情。我认为这暗示的是，**梯度下降正在某种意义上找到正确的分割方法，许多系统都趋于收敛，许多不同的神经网络架构都趋于收敛**
。

有一些自然的抽象集合，它们是一种非常自然地分割问题的方法，并且许多系统都会趋于收敛于这些方法。这将是我的某种……我不知道任何关于神经科学的知识。这只是根据我们所看到的，我的一种大胆的推测。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBPuKYFW3oYvSbrXlwh9UDfdbx8tAYcObw6rdNJNwl5jem7ZiaQkvBGf3TnnYFdv72ojr7e9zAZjZfg/640?wx_fmt=jpeg&from=appmsg)

**主持人莱克斯：** 如果它与用于形成表示的模型的介质无关，那将是美好的。

**克里斯：**
是的。而且，你知道，这是一种基于……我们只有少数数据点表明这一点的大胆推测，但是，似乎在某种意义上，相同的东西一次又一次地、一次又一次地在自然神经网络和人工或生物神经网络中形成。

**主持人莱克斯：** 其背后的直觉是，为了理解现实世界，词语需要所有相同的东西。

**克里斯：**
[R]是的，如果我们选择比如“狗”这个概念，对吧？某种意义上，“狗”的概念就像宇宙中的一个自然类别，诸如此类，对吧？有一些理由表明，这不仅仅是人类思考世界的方式的一个奇怪的怪癖，我们有“狗”这个概念。或者如果你有“线”的概念，看看我们周围，有线，在某种意义上，理解这个房间最简单的方法就是拥有“线”的概念。所以我认为这就是为什么会发生这种情况。

**主持人莱克斯：** 是的，你需要一条曲线来理解一个圆，你需要所有这些形状来理解更大的东西，这是一个形成的概念层次结构。

**克里斯：**
也许有一些方法可以不用参考这些东西来描述图像，对吧？但它们不是最简单的方法，也不是最经济的方法，诸如此类。因此，系统会趋同于这些策略，这将是我的大胆假设。

**主持人莱克斯：** 你能详细介绍一下我们一直在提到的特征和电路的一些构建块吗？我认为你首先在2020年的论文《Zoom In，电路入门》中描述了它们。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBPuKYFW3oYvSbrXlwh9UDfdeueHDib3gO8O0EftYtPqNQ1SjsoLb8qDwyEXM2LfRDL2vOjeSialgOaA/640?wx_fmt=jpeg&from=appmsg)

**克里斯：** 当然。也许我先描述一些现象，然后我们可以逐步建立特征-符号的概念。

如果你花了几年时间，也许五年左右的时间来研究这个特定的模型，Inception
v1，这个在2015年最先进的视觉模型，但现在已经过时了，你会发现它非常吸引人。它包含大约10000个神经元，我花了大量时间研究Inception
v1的这10000多个神经元。其中一个有趣的观察结果是，虽然许多神经元没有任何明显的可解释的含义，但在Inception
v1中，有相当数量的神经元表现出清晰的内插含义。

**你可以找到似乎专门检测曲线、汽车、汽车车轮、汽车车窗、狗的耷拉的耳朵、面向右边的长鼻子的狗和面向左边的长鼻子的狗等特征的神经元。**
有不同种类的精美的边缘检测器、线检测器和颜色对比检测器，有时被称为高低频检测器。观察这些元素让我感觉像一个生物学家，探索一个新的蛋白质世界，并发现不同的蛋白质是如何相互作用的。

理解这些模型的一种方法是根据神经元来理解。例如，你可以识别一个狗检测神经元或一个汽车检测神经元。事实证明，你可以研究这些神经元是如何相互连接的。例如，如果你检查一个汽车检测神经元，你可能会发现它在上层与窗口检测器、车轮检测器和车身检测器强烈连接。这个神经元正在寻找汽车上方的窗户、下方的车轮和车身上的镀铬装饰——这构成了一种检测汽车的简单方法。前面我们提到想要获取算法，并询问：“运行的算法是什么？”在这种情况下，我们分析神经网络的权重，并推导出一个粗略的汽车检测方法，我们称之为电路。

然而，问题在于并非所有神经元在其表示中都是不可或缺的。**有理由考虑叠加假设，这表明有时分析神经元的组合比隔离单个神经元更合适**
。可能并非单个神经元代表一辆汽车；相反，在检测到一辆汽车后，模型可能会在接下来的几层中隐藏汽车的一部分，以及各种狗检测器。模型可能在此时最小化其关于汽车的工作量，有效地将它们存储起来。

因此，你可能会注意到一个微妙的模式，即许多被认为是狗检测器的神经元实际上有助于在下一层中表示一辆汽车。在这一点上，确定对应于可能被称为“汽车概念”的离散表示变得具有挑战性。因此，我们需要一个术语来指代这些类似神经元的实体，我们希望将其视为理想化的神经元——整洁的神经元，以及模型中可能隐藏的更多神经元。我们将这些实体简称为特征。

所以电路是这些特征的连接，对吧？当我们有汽车检测器时，它连接到窗口检测器和车轮检测器，它寻找下方的车轮和上方的窗户。这是一个电路。电路只是由权重连接的特征集合，它们实现算法。它们告诉我们特征是如何使用的、是如何构建的以及是如何连接在一起的。

也许值得尝试确定这里的核心假设是什么。我认为核心假设是我们所说的线性表示假设。如果我们考虑汽车检测器，它激发得越多，我们就越认为模型越确信存在一辆汽车。或者，如果有一些神经元的组合代表一辆汽车，那么这种组合激发得越多，我们就越认为模型认为存在一辆汽车。

不一定非得如此，对吧？你可以想象这样一种情况：你有一个汽车检测神经元，如果它在1到2之间放电，这意味着一个意思，但如果它在3到4之间放电，则意味着完全不同的意思。这将代表一种非线性解释。原则上，模型可以做到这一点，但这效率有些低，你可能会发现实现这样的计算很复杂。

因此，理解特征和电路框架的一种方法是，我们认为事物是线性的。我们认为，如果一个神经元或多个神经元的组合放电更多，则表示检测到更多特定的事物。这使得权重作为这些实体和这些特征之间边的解释非常清晰。因此，边的权重具有特定的含义。

在某种程度上，这就是核心概念。我们可以脱离神经元的上下文来讨论这个问题。例如，你熟悉word2vec的结果吗？在这种情况下，你会有这样的等式：“国王 -
男人 + 女人 = 王后”。你能够进行这种算术运算的原因在于存在线性表示。

**主持人莱克斯：** 你能稍微解释一下这种表示吗？首先，特征是激活的方向。你能进行“国王 - 男人 +
女人”这种word2vec的操作吗？你能解释一下这是什么意思吗？这是一个关于我们正在讨论内容的如此简单、清晰的解释。

**克里斯：** 大约如此。没错。Thomas
Mikolov等人有一个非常著名的结果，即Word2Vec。并且有很多后续工作在探索这个问题。有时我们创建这些词嵌入，我们将每个词映射到一个向量。顺便说一句，如果你以前没有想过这个问题，这本身就是一件很疯狂的事情，对吧？我们正在进行并表示，返回，你知道，如果你刚在物理课上学到向量，对吧？我会说，哦，我实际上要把字典里的每个词都变成一个向量。这是一个相当疯狂的想法。

但是你可以想象各种将单词映射到向量的方法。然而，当我们训练神经网络时，它们似乎喜欢将单词映射到向量，这样在特定意义上就存在某种线性结构，即方向具有意义。例如，某些方向似乎对应于性别，男性词语将在一个方向上较远，而女性词语将在另一个方向上。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBPuKYFW3oYvSbrXlwh9UDfdzm8Y78iaeBYB99y3gichmAn0micX7jViarP6jUm52fQjJh4S6j0de9n3Sw/640?wx_fmt=jpeg&from=appmsg)

线性表示假设可以认为是说，这从根本上就是正在发生的事情：不同的方向具有不同的含义，将不同的方向向量加在一起可以表示概念。Mikolov的论文认真地考虑了这个想法，其结果之一就是你可以用单词玩一种算术游戏。你可以取“国王”减去“男人”再加上“女人”。你是在试图改变性别。事实上，如果你这样做，结果会接近“王后”这个词。

你也可以做其他事情。例如，你可以取“寿司”，减去“日本”，再加上“意大利”，得到“披萨”，或者进行类似的不同的运算。从某种意义上说，这就是线性表示假设的核心。你可以将其描述为关于向量空间的纯粹抽象事物，也可以将其描述为关于神经元激活的陈述。但从根本上说，它关乎方向具有意义的这个特性。在某些方面，它甚至比这更微妙一些；它主要关乎能够将事物加在一起的这个特性，这样你就可以通过添加它们来独立地修改性别和皇室，或烹饪类型和国家等概念。

**主持人莱克斯：** 你认为线性假设成立吗？是的。这具有尺度性。

**克里斯：**
到目前为止，我认为我所看到的一切都与这个假设一致。就像你可以写下神经网络，你可以在其中编写权重，使它们没有线性表示，理解它们的方式不是通过线性表示。但我认为我见过的每一个自然神经网络都具有这种特性。

最近有一篇论文一直在推动研究前沿。我认为最近有一些工作研究了多维特征，它更像是一组方向，而不是单个方向。对我来说，这仍然看起来像是一种线性表示。然后还有一些其他论文表明，在非常小的模型中，你可能会得到非线性表示。我认为这个问题还有待商榷。但我认为到目前为止我们所看到的一切都与线性表示假设一致。

这太不可思议了。它不必那样。然而，我认为有很多证据表明，至少，这是非常非常普遍的。到目前为止，证据与之相符。我想你可能会说，好吧，克里斯，这有很多，你知道，要写下来。如果我们不确定这是真的，而你正在调查它，就好像它是真的，这难道不危险吗？

但我认为，认真对待假设并将它们推到极致实际上是一种优点。也许有一天我们会发现一些与线性表示假设不一致的东西。然而，科学充满了错误的假设和理论，我们通过将它们作为假设并尽可能地推导它们来学习了很多。我想这某种程度上是库恩所说的“常态科学”的核心。

**主持人莱克斯：** 科学哲学。这导致了范式转变。所以是的，我喜欢认真对待假设并得出自然的结论。缩放假设也是如此……

**克里斯：**
没错，没错。我的一位同事汤姆·亨尼根，他是一位前物理学家，给我做了一个非常好的比喻，那就是热质说，曾经我们认为热实际上是一种叫做热质的东西。而且，你知道，热的物体，你知道，会使冷的物体变暖的原因是热质在它们之间流动。而且，你知道，因为我们习惯于用现代理论来思考热，你知道，这似乎有点愚蠢，但实际上很难设计一个实验来证伪热质假设。而且，你知道，你实际上可以在相信热质说的前提下做很多非常有用的工作。例如，事实证明，最初的内燃机是由相信热质说的人开发的。所以我认为，即使假设可能是错误的，认真对待假设也是一种优点。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBPuKYFW3oYvSbrXlwh9UDfdBQB0Iib5ejCZCdeEFiaaTkSACGPsQw1oSp1kKgMI3Ql1ibWP7yniaxu2rA/640?wx_fmt=jpeg&from=appmsg)

**主持人莱克斯：**
是的。这其中蕴含着深刻的哲学真理。我对太空旅行也是这样感觉的。比如殖民火星，很多人批评这一点。我认为，如果你只是假设我们必须殖民火星才能为人类文明提供备份，即使这不是真的，我认为这也会产生一些有趣的工程甚至科学突破。

**克里斯：**
是的，这确实很有趣。我认为，对社会而言，存在一些人几乎不理性地致力于研究特定假设，这非常有用。因为，你知道，在大多数科学假设最终被证明是错误的情况下，维持科学士气并坚持不懈地努力需要付出很多。很多科学研究都无法成功，但即便如此，这仍然非常有用，有一个关于杰夫·辛顿的玩笑，说杰夫·辛顿在过去的50年里每年都发现了大脑的工作原理。[笑]但我说这话是带着深深的敬意的，因为事实上，这确实让他做出了许多伟大的工作。

**主持人莱克斯：** 是的，他的工作很棒。他现在获得了诺贝尔奖。现在谁还在笑呢？

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBPuKYFW3oYvSbrXlwh9UDfdKOuF3PEIAXoocEyIlia6cicVT7DPGH2ggDRib9YEDBlZrsp8AtfdiaMBvA/640?wx_fmt=jpeg&from=appmsg)

**克里斯：**
我想人们应该能够识别出合适的置信水平。但我认为，这样做也具有很大的价值：假设这个问题是可解的，或者这种方法大体上是正确的，然后坚持这个假设一段时间，并努力推进它。如果社会上有很多人为不同的问题这样做，那么在最终排除某些问题（例如，“这行不通，我们知道有人努力尝试过了”）或获得一些关于世界的知识方面，这将非常有用。

**主持人莱克斯：** 另一个有趣的假设是叠加假设。你能描述一下什么是叠加吗？

**克里斯：**
是的。我们之前谈到了词向量模型（word2vec），我们谈到可能有一个方向对应性别，另一个对应皇室，另一个对应意大利，另一个对应食物等等。这些词向量可能有500维或1000维。如果你认为所有这些方向都是正交的，那么你最多只能有500个概念。

我喜欢披萨，但如果要我说出英语中最重要的500个概念，意大利可能不会是其中之一。至少，意大利并非显而易见地属于其中，对吧？因为你必须包含诸如单复数、动词和名词以及形容词之类的概念。在谈到意大利和日本之前，还有很多东西需要涵盖，世界上还有很多国家。那么，模型如何才能同时满足线性表示假设，并表示比其维度更多的概念呢？

这意味着什么呢？好吧，如果线性表示假设成立，那么一定有一些有趣的事情正在发生。在进一步探讨之前，我还想告诉你一件有趣的事情，那就是我们之前谈到了所有这些多义神经元，对吧？在Inception
V1中，有一些神经元可以很好地检测汽车和曲线等，它们对许多非常连贯的事物做出反应。但也有一些神经元会对许多不相关的事物做出反应，这也是一个有趣的现象。

事实证明，即使是那些非常清晰的神经元，如果你观察其弱激活（例如，激活强度为最大激活强度的5%），它实际上并不是在检测它预期检测的核心事物。例如，如果你观察一个曲线检测器，并观察其激活强度为5%的地方，你可以将其解释为噪声，或者它在那里执行了其他功能。

那么，这怎么可能呢？数学中有一个令人惊奇的概念叫做**压缩感知**
。一个非常令人惊讶的事实是，如果你有一个高维空间，并将其投影到一个低维空间中，通常情况下你无法进行反向投影并恢复你的高维向量，你会丢失信息。就像你无法对一个非方阵求逆一样，只能对方阵求逆。然而，事实证明，如果我告诉你高维向量是稀疏的，也就是说它大部分元素都是零，那么情况就不一样了。在这种情况下，你通常可以以很高的概率找到高维向量。

这是一个令人惊讶的事实，对吧？这意味着你可以拥有这个高维向量空间，只要事物是稀疏的，你就可以将其投影到低维空间，并得到其低维投影。叠加假设认为神经网络中就是这样运作的，例如，词向量中就是这样运作的。词向量能够同时拥有方向作为有意义的事物，并且通过利用它们在一个相当高维的空间中运行的事实，以及这些概念是稀疏的事实（例如，你通常不会同时谈论日本和意大利）。

在大多数情况下，日本和意大利都是零，它们根本不存在。如果这是真的，那么你可以拥有比维度更多的有意义的方向，即特征。同样，当我们谈论神经元时，你可以拥有比神经元更多的概念。这就是叠加假设的高层次解释。

它还有一个更令人惊奇的含义，那就是表示可能就是这样，但计算也可能就是这样，也就是所有这些神经元之间的连接。从某种意义上说，神经网络可能是更大、更稀疏的神经网络的投影。我们看到的是这些投影。叠加假设的最强版本将非常认真地看待这一点，并认为确实存在一个“上层模型”，在这个模型中，神经元非常稀疏且相互连接，它们之间的权重是这些非常稀疏的回路。这就是我们正在研究的，而我们观察到的只是它的投影。我们需要找到原始对象。

**主持人莱克斯：** 学习过程试图构建上层模型的压缩表示，而不会在投影中丢失太多信息。

**克里斯：**
是的，它找到了如何有效地拟合它，或者类似的东西。梯度下降就是这样做的。事实上，这表明梯度下降，你知道，它可以表示一个稠密的深度神经网络，但它也表明梯度下降在极度稀疏的模型空间中令人愉悦地进行搜索，这些模型可以投影到这个低维空间。而大量的研究人员都在努力研究稀疏神经网络，对吧？你可以设计神经网络，对吧？其中的边是稀疏的，激活也是稀疏的。

而且，我的感觉是，这项工作总体来说感觉非常有原则，对吧？这太有道理了。然而，我的印象是，这项工作并没有真正取得很好的成果。我认为一个可能的答案是，神经网络在某种意义上已经是稀疏的了。梯度下降一直都在尝试这样做，**梯度下降实际上在幕后比你更有效地搜索稀疏模型的空间，学习最有效的稀疏模型，然后找出如何巧妙地将其折叠起来，以便在你的GPU上方便地运行，它可以进行很好的稠密矩阵乘法，这是你无法超越的**
。

**主持人莱克斯：** 你认为可以塞进神经网络的概念有多少？

**克里斯：** 取决于它们的稀疏程度。所以参数数量可能存在一个上限，对吧？因为你仍然需要有权重来连接它们。所以这是一个上限。

事实上，压缩感知和约翰逊-
林德斯特劳斯引理等方面都有一些很好的结果，这些结果基本上告诉你，如果你有一个向量空间，并且想要几乎正交的向量，这可能是你在这里想要的东西，对吧？所以你会说，好吧，你知道，我将放弃让我的概念、我的特征严格正交，但我希望它们不要过多地相互干扰。我将要求它们几乎正交。

那么这表明，一旦你设定了一个阈值来确定你愿意接受的余弦相似度是多少，这实际上与你拥有的神经元数量呈指数关系。所以在某个时候，这甚至不会成为限制因素。但这里有一些很好的结果。

事实上，在某种意义上它可能更好，因为这表示任何随机的特征集都可能被激活。但事实上，这些特征具有一定的相关结构，其中一些特征更可能共同出现，而另一些则不太可能共同出现。

所以我猜想，神经网络在打包方面做得非常好，以至于这可能不是限制因素。

**主持人莱克斯：** 多义性问题是如何进入这个图景的？

**克里斯：**
所以多义性是我们观察到的现象，我们观察到许多神经元，而神经元不仅仅代表一个概念。它不是一个清晰的特征。它会对许多不相关的事物做出反应。而叠加可以被认为是一种解释多义性观察结果的假设。所以多义性是观察到的现象，而叠加是一个可以解释它的假设。

**主持人莱克斯：** 所以这使得机制可解释性更困难。

**克里斯：**
对。所以如果你试图用单个神经元来理解事物，而你有多义神经元，你就会遇到很多麻烦，对吧？我的意思是，最简单的答案是，好吧，你正在观察神经元，试图理解它们。这个神经元对很多事情都有反应。它没有很好的含义。好吧，这很糟糕。

你还可以问的另一件事是，最终，我们想要理解权重。如果你有两个多义神经元，每个神经元都对三件事做出反应，而另一个神经元也对三件事做出反应，并且它们之间有一个权重，这意味着什么？这意味着所有这三件事，你知道，就像这九种相互作用正在发生一样？这是一件非常奇怪的事情。

但也存在更深层次的原因，这与神经网络在真正高维空间中运行的事实有关。我说过我们的目标是理解神经网络并理解其机制。你可能会说，为什么不呢？它只是一个数学函数。为什么不看看它呢，对吧？

你知道，我最早做的一些项目研究了将二维空间映射到二维空间的神经网络。你可以用这种美丽的方式将它们解释为弯曲的流形。为什么我们不能这样做呢？好吧，随着你拥有更高维的空间，这个空间的体积在某种意义上与你拥有的输入数量呈指数关系。所以你不能仅仅去可视化它。

我们必须以某种方式将其分解。我们需要以某种方式将这个指数空间分解成许多我们可以独立推理的事物，这至关重要。正是这种独立性使你无需考虑所有事物的指数组合。

事物是单义的，事物只有一个含义，以及事物具有含义——这是使你能够独立思考它们的重点。所以我认为，如果你想知道我们想要可解释的单义特征的最深层原因，那就是真正的深层原因。

**主持人莱克斯：** 因此，正如你最近的工作所针对的那样，这里的目标是如何从具有多义特征和所有这些混乱的神经网络中提取单义特征？

**克里斯：**
是的，我们观察到这些多义神经元，我们假设这就是叠加中发生的事情。如果叠加正在发生，那么实际上存在一种相当成熟的技术，这是一种有原则的做法，那就是字典学习。事实证明，如果你进行字典学习，特别是如果你以某种意义上很好地对其进行正则化的一种有效方法，称为稀疏自编码器，如果你训练一个稀疏自编码器，这些美丽的内插特征就会开始出现，而之前并没有这些特征。所以这不是你一定会预测的事情，对吧？但事实证明，这非常有效。对我来说，这似乎是对线性表示和叠加的一些非平凡的验证。

**主持人莱克斯：** 那么，使用字典学习，你并不是在寻找特定类型的类别。你不知道它们是什么。

**克里斯：**
正确。它们只是出现了。这回到了我们之前的观点，对吧？我们没有做假设。梯度下降比我们聪明。所以我们没有对存在什么做出假设。我的意思是，人们当然可以这样做，对吧？人们可以假设存在一个特征，并去寻找它。但我们没有这样做。我们说我们不知道那里会有什么。相反，我们将让稀疏自编码器发现那里存在的事物。

**主持人莱克斯：** 那么你能谈谈去年十月关于走向单义性的论文吗？那篇论文有很多很好的突破性成果。

**克里斯：**
您这样描述真是太好了。是的，我的意思是，这是我们使用稀疏自动编码器取得的第一个真正成功。我们使用了一个单层模型。结果证明，如果你对其进行字典学习，你会发现所有这些非常好的、可解释的特征。例如，阿拉伯语特征、希伯来语特征、Base64特征，这些都是我们深入研究的一些例子，并且确实证明了它们是我们认为的那样。结果表明，如果你训练一个性能好两倍的模型，或者训练两个不同的模型并进行字典学习，你就能在这两个模型中找到类似的特征。这很有趣。你会发现各种不同的特征。这实际上只是为了证明这个方法有效。我应该提到，Cunningham等人也在同一时期获得了非常相似的结果。

**主持人莱克斯：** 进行这些小型实验，并发现它实际上是有效的。

**克里斯：**
是的，这里有很多结构。所以，也许先退一步，我原以为所有这些机械式插值工作最终会让我找到一个解释，解释为什么它如此困难且难以处理。我们会说，嗯，这里有个迷信问题。迷信真的很棘手，我们有点完蛋了。但事实并非如此。事实上，一种非常自然、简单的技术就奏效了。所以这实际上是一个非常好的情况。我认为这是一个相当困难的研究问题，它存在很多研究风险，而且，你知道，它很可能仍然会失败。但是我认为，当它开始奏效时，相当一部分，相当大一部分的研究风险就被我们克服了。

**主持人莱克斯：** 你能描述一下用这种方法可以提取哪些类型的特征吗？

**克里斯：**
好吧，这取决于你正在研究的模型，对吧？模型越大，特征就越复杂。我们可能会在一分钟后讨论后续工作。但在这些单层模型中，我认为一些非常常见的是语言，包括编程语言和自然语言。有很多特征是特定上下文中的特定单词。所以，“the”我认为真正思考的方式是，“the”很可能后面跟着一个名词。所以，你可以把它看作是一个特征，但你也可以把它看作是预测一个特定名词的特征。还有一些特征会在特定上下文中被激活，例如法律文件或数学文件之类的。所以，你知道，在数学的上下文中，就像，“the”，然后预测向量或矩阵，所有这些数学词汇，而在其他上下文中，你会预测其他东西。

**主持人莱克斯：** 这是很常见的。基本上，我们需要聪明的研究人员来为我们所看到的东西贴上标签。

**克里斯：**
是的。所以，你知道，这唯一的作用就是为你展开事物。如果所有东西都叠加在一起，你知道，叠加在一起，你无法真正看到它，这就是在展开它。但是现在你仍然有一个非常复杂的东西需要去理解。

所以你必须做很多工作来理解这些是什么。有些非常微妙。例如，即使在这个单层模型中，关于Unicode也有一些非常酷的事情，当然，一些语言是用Unicode编写的，并且分词器不一定会为每个Unicode字符分配一个专用的标记。

相反，你会有这些交替标记的模式，每个模式代表Unicode字符的一半。然后你会有一个不同的特征去激活相反的那些，就像，好吧，我刚刚完成了一个字符，去预测下一个前缀。然后，好吧，我在前缀上，预测一个合理的后缀。你必须来回交替。

所以这些单层模型真的很有趣。我的意思是，这是另一件事，你可能会认为，好吧，只会有一个Base64特征，但事实证明实际上有很多Base64特征，因为你可以用Base64编码英语文本。

这与普通文本具有非常不同的Base64标记分布。它还可以利用分词的一些特性。我不知道，还有各种有趣的东西。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBPuKYFW3oYvSbrXlwh9UDfdV7BPP0qJYXFfTvBvCjMBFfyfObHoWzdnm4ibfPYib3rL2MXG3iaa7cicZQ/640?wx_fmt=jpeg&from=appmsg)

**主持人莱克斯：** 为正在发生的事情分配标签的任务有多难？这可以通过人工智能自动化吗？

**克里斯：**
我认为这取决于特征，也取决于你对人工智能的信任程度。有很多工作致力于自动化可解释性。我认为这是一个非常令人兴奋的方向。我们做了相当多的自动化可解释性工作，并让Claude为我们的特征贴标签。

**主持人莱克斯：** 有没有什么好笑的事情，比如它完全正确或完全错误？

**克里斯：**
是的，我认为它很常见地说一些非常笼统的东西，在某种意义上是正确的，但并没有真正抓住正在发生的事情的细节。我认为这是一个相当普遍的情况。我没有特别好笑的一个例子。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBPuKYFW3oYvSbrXlwh9UDfdDAjsEPm3pRQH92DYzCn1iciaicnUf1lXkL4licvdNTsN5uWwsEdrkF3z7A/640?wx_fmt=jpeg&from=appmsg)

**主持人莱克斯：**
这很有趣，它之间存在一个小小的差距，虽然它是正确的，但它并没有完全捕捉到事物的深层细微之处。这是一个普遍的挑战。就像，能够说出正确的事情已经是一项了不起的成就了，但它有时会缺少深度。在这种情况下，它就像ARC挑战，你知道，那种智商类型的测试。感觉像是弄清楚一个特征代表什么是一个你必须解决的小难题。

**克里斯：** 是的。我认为有时它们更容易，有时也更难。所以是的，我认为这很棘手。

还有一件事，我不知道，也许在某种程度上这是我的审美观在起作用，但我将尝试给你一个合理的解释。你知道，我实际上有点怀疑自动化的可解释性。我认为这部分是因为我希望人类能够理解神经网络。如果神经网络为我理解它，你知道，我不太喜欢那样。但我确实有一点，你知道，在某种程度上我有点像那些数学家，他们说，你知道，如果有一个计算机自动生成的证明，它不算数。他们不会理解它。

我也确实认为，这是一种关于信任类型的反思，你知道，当你编写计算机程序时，你必须信任你的编译器。如果你的编译器中存在恶意软件，那么它可以将恶意软件注入下一个编译器，你知道，你会有麻烦的，对吧？

那么，如果你使用神经网络来验证你的神经网络是安全的，那么你正在测试的假设是，好吧，神经网络可能并不安全。你必须担心，是否有什么方法可以让它欺骗你？所以，你知道，我认为现在这不是一个很大的问题。

但我确实想知道，从长远来看，如果我们必须使用真正强大的AI系统来审核我们的AI系统，这是否是我们真正可以信任的东西？但也许我只是在找借口，因为我只是希望我们能够达到人类理解一切的地步。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBPuKYFW3oYvSbrXlwh9UDfdCLiaaQCM2IsbWBR8NwBiaOrjfM54XibUpkjBMW5AibLDM4kyNc2u014G5Q/640?wx_fmt=jpeg&from=appmsg)

**主持人莱克斯：**
是的，我的意思是，尤其是在我们讨论AI安全并寻找与AI安全相关的特征，例如欺骗等等时，这一点非常有趣。那么，让我们来谈谈2024年5月的“可扩展单义性论文”（Scaling
Monosemanticity Paper）。好的，那么将这项研究扩展到应用于Claude 3 Sonnet需要什么？

**克里斯：** 更多的GPU。但我的一个队友汤姆·亨尼根（Tom
Hennigan）参与了最初的规模定律研究。从一开始，他就对一个问题很感兴趣：是否存在互操作性的规模定律？因此，当这项工作开始取得成功，我们开始使用稀疏自编码器（Sparse
Autoencoders）时，他立即做的事情是：研究稀疏自编码器的规模定律是什么？以及这与基础模型规模如何相关？

结果证明，这非常有效，您可以用它来预测：如果您训练一个特定大小的稀疏自编码器，您应该训练多少个token等等。这实际上对我们扩展这项工作有很大帮助，并使我们更容易训练真正大型的稀疏自编码器。虽然不像训练大型模型那样复杂，但训练非常大型的稀疏自编码器也开始变得昂贵。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBPuKYFW3oYvSbrXlwh9UDfdrzUUTSibcxu2ibqro3IsVq2pFam6h3GEDGW8WYjDOmfgeFmJA80nUaKg/640?wx_fmt=jpeg&from=appmsg)

**主持人莱克斯：** 所以你们必须做所有相关的工作？

**克里斯：**
哦，是的，我的意思是，这里也有巨大的工程挑战，对吧？所以，是的，有一个科学问题是如何有效地扩展事物？然后还有大量的工程工作来扩展它。你必须规划它，你必须非常仔细地考虑很多事情。我很幸运能与一群优秀的工程师一起工作。

**主持人莱克斯：** 是的。而且还有基础设施，尤其如此。所以事实证明，缩放定律仍然有效。

**克里斯：**
是的。我认为这很重要，因为你可以想象一个世界，你之后会朝着单义性努力。克里斯（Chris，推测为人物名，原文未解释），这很棒。它在一个单层模型上有效。但是单层模型非常特殊。就像，线性表示假设和迷信假设可能是理解单层模型的正确方法，但它不是理解更大模型的正确方法。所以，我认为首先，Cunningham等人的论文稍微解决了这个问题，并暗示情况并非如此，但可扩展单义性我认为是一个重要的证据，即使对于非常大的模型（我们在Claude
3
Sonnet上进行了测试，当时它是我们的一个生产模型），这些模型似乎也主要可以通过线性特征来解释，并且对它们进行字典学习是有效的。随着学习更多特征，你就能解释越来越多的内容。所以，我认为这是一个相当有希望的迹象。而且你还会发现真正令人着迷的抽象特征。这些特征也是多模态的，它们对同一概念的图像和文本做出反应，这很有趣。

**主持人莱克斯：** 是的，你能解释一下吗？我的意思是，你知道，后门，有很多例子你可以分享。

**克里斯：**
是的，也许让我们先从一个例子开始，那就是我们发现了一些关于代码中安全漏洞和后门的特征。事实证明，它们实际上是两个不同的特征。有一个安全漏洞特征。如果你强制激活它，Claude就会开始在代码中编写安全漏洞，例如缓冲区溢出。它还会针对各种各样的情况触发，例如，它的一些顶级数据集示例包括诸如“--disable
--SSL”之类的东西，这些显然是非常不安全的。

**主持人莱克斯：**
所以目前看来，也许这仅仅是因为示例的呈现方式。这有点像表面上，一些比较明显的例子，对吧？我想，其想法是，随着时间的推移，它可能能够检测到更细微的，比如欺骗或错误之类的。

**克里斯：**
是的，也许我想区分两件事。一个是特征或概念的复杂性，对吧？另一个是我们正在查看的例子的细微程度，对吧？当我们展示顶级数据集示例时，这些是导致该特征激活的最极端示例。所以这并不意味着它不会对更细微的事情触发。

**不安全代码特征，它最强烈触发的那些东西是这些非常明显的禁用安全类型的事情。但它也会触发缓冲区溢出和代码中更细微的安全漏洞。这些特征都是多模态的。**
所以你可以问，什么图像会激活这个特征？事实证明，安全漏洞特征会针对人们点击Chrome以绕过该网站的图像激活，SSL证书可能出错或者类似的情况。

另一件非常有趣的事情是，代码中存在后门特征。你激活它，它就会继续。Claude会编写一个后门程序，将你的数据转储到某个端口或类似的地方。但是你可以问，好的，什么图像会激活后门特征？结果是带有隐藏摄像头的设备。所以，显然，有一整个行业的人在销售看起来很普通但却带有隐藏摄像头的设备，他们还在广告中宣传其隐藏摄像头功能。

我想那是后门的物理版本。所以它向你展示了这些概念是多么抽象，对吧？我只是觉得很悲哀，竟然有整个市场都在销售这样的设备。但我对它提出的作为该特征顶级图像示例的东西感到很高兴。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBPuKYFW3oYvSbrXlwh9UDfdiaeCC9I7BzeTwBhTGEdRUAibkFVvBBtjooqR01lHKqXz0MMx5nvmmRmQ/640?wx_fmt=jpeg&from=appmsg)

**主持人莱克斯：**
是的，它很好。它是多模态的。它几乎是多语境的。它对单个概念的定义广泛而有力。对我来说，一个非常有趣的特征，尤其是在AI安全方面，是欺骗和说谎，以及这些方法能够检测模型中说谎的可能性，尤其是在模型变得越来越聪明的情况下。我想，一个超级智能模型的一大威胁就是它能够欺骗操作它的人，隐瞒其意图或其他任何事情。那么，你从检测模型内部的谎言中学到了什么？

**克里斯：**
是的，我认为我们在某些方面还处于早期阶段。我们发现与欺骗和说谎相关的许多特征。**有一个特征，当人们说谎和具有欺骗性时它就会触发，你强制激活它，Claude就会开始对你撒谎。**
所以我们有一个欺骗特征。我的意思是，还有各种其他关于隐瞒信息和不回答问题、关于权力寻求和政变等等的特征。有很多特征与一些令人不安的事情有关。如果你强制激活它们，Claude的行为就会变得不像你想要的那样。

**主持人莱克斯：** 在模型解释领域，你认为未来哪些方向令人兴奋？

**克里斯：**
嗯，有很多方面。首先，我真的很想达到这样一个阶段：我们可以使用快捷方式，真正理解模型的特性，并利用这些特性来理解模型的计算过程。对我来说，这才是最终目标。已经有一些工作了。我们发表了一些成果。Sam
Marks有一篇论文做了一些类似的工作。可以说，已经有一些边缘性的工作了。但我认为还有很多工作要做。我认为这将是一件非常令人兴奋的事情。

这与我们称之为“干扰权重”的挑战有关，由于某种“迷信”，如果你只是简单地查看特征是否彼此连接，可能会有一些权重在“上层”模型中并不存在，而只是某种“迷信”的产物。所以这是一个相关的技术挑战。

**我认为另一个令人兴奋的方向是，你可以将稀疏自编码器视为一种“望远镜”。它们使我们能够观察到所有存在的特征。**
随着我们构建越来越好的稀疏自编码器并在字典学习方面取得更好的进展，我们看到越来越多的“星星”。我们放大越来越小的“星星”。但是有很多证据表明，我们仍然只看到了极小一部分的“星星”。在我们神经网络的“宇宙”中，还有很多物质是我们目前无法观察到的。

我们可能永远无法拥有足够精密的仪器来观察它，也许其中一些根本不可能观察到，或者从计算上讲无法观察到。所以这是一种“暗物质”，也许不是现代天文学意义上的暗物质，而是早期天文学中我们不知道这种无法解释的物质是什么的那种暗物质。我经常思考这种“暗物质”，以及我们是否能够观察到它，以及如果我们无法观察到它，这对安全性意味着什么，特别是如果神经网络的很大一部分是我们无法访问的。

我经常思考的另一个问题是，归根结底，机制内插法是一种非常微观的内插方法。它试图以非常细致的方式理解事物。但是我们关心的许多问题都是非常宏观的。我们关心的是关于神经网络行为的问题。我认为这是我最关心的事情。

然而，人们可能会关心许多其他更大规模的问题。采用这种非常微观的办法的好处在于，或许更容易问：“这是真的吗？”但是缺点是，它与我们关心的事情相去甚远。所以我们现在有了一个需要攀登的阶梯，我认为一个问题是我们能否找到更大规模的抽象概念，我们可以利用这些抽象概念来理解我们从这种非常微观的办法中得出的神经网络。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBPuKYFW3oYvSbrXlwh9UDfdPaCfErhwYtJE0s1G11CLsriaYd4N4dFXPEV50O3GlD2ce69IQ0aYW8Q/640?wx_fmt=jpeg&from=appmsg)

**主持人莱克斯：**
是的，你写过关于这种“器官”问题的文章。是的，没错。如果我们将可解释性视为神经网络的一种“解剖学”，那么大多数的研究都涉及研究微小的“血管”，观察小规模的单个神经元及其连接方式。然而，许多自然问题是小规模方法无法解决的。相比之下，生物解剖学中最突出的抽象概念涉及更大规模的结构，如单个器官，例如心脏，或整个器官系统，例如呼吸系统。因此，我们想知道，人工神经网络是否存在呼吸系统、心脏或大脑区域？

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBPuKYFW3oYvSbrXlwh9UDfdJNKw5u8Dw3e5gft1NRky2LG2mribZiaakaIRdqPUZSVWv1eeYEVkO9ibQ/640?wx_fmt=jpeg&from=appmsg)

**克里斯：**
是的，没错。我的意思是，如果你想想科学，许多科学领域都在许多抽象层次上进行研究。在生物学中，你有分子生物学研究蛋白质和分子等等。然后你有细胞生物学，然后有组织学研究组织，然后有解剖学，然后有动物学，然后有生态学。所以你有许多许多抽象层次。

在物理学中，你可能有单个粒子的物理学。然后，统计物理学为你提供了热力学等等。因此，你经常在各种科学学科中拥有不同的抽象层次。我认为现在我们有机制内插法，如果它成功了，就像神经网络的微生物学一样。但我们想要更像解剖学的东西。

你可能会问，为什么不能直接进入那个层次？我认为答案是“迷信”，至少在很大程度上是这样。在没有首先以正确的方式分解微观结构并研究其连接方式的情况下，实际上很难看到这种宏观结构。然而，我希望会有比特征和电路更大规模的东西，并且我们将能够构建一个包含更大规模事物的叙述。然后，你可以详细研究你关心的部分。

**主持人莱克斯：** 与神经生物学相反，就像神经网络的心理学家或精神科医生一样。

**克里斯：**
我认为美好的事情是，如果我们能够做到，而不是让这两者有不同的领域，如果能够在两者之间架起一座桥梁，这样你就可以让所有更高层次的抽象都非常扎实地建立在这个非常坚实、理想的、更严格的基础之上。

**主持人莱克斯：** 你认为人脑（生物神经网络）和人工神经网络之间有什么区别？

**克里斯：**
嗯，神经科学家比我们困难得多。有时我只是想感谢我的工作比神经科学家容易多少，对吧？所以我可以记录所有神经元的活动。我们可以用任意数量的数据来做到这一点。顺便说一句，在你这样做的时候，神经元不会发生变化。你可以去消融神经元，编辑连接等等。然后你可以撤消这些更改。这太棒了。你可以干预任何神经元并强制使其活跃，看看会发生什么。

你知道哪些神经元与所有其他神经元相连，对吧？神经科学家想要获得连接组。我们拥有连接组，而且我们拥有比秀丽隐杆线虫更大的连接组。我们不仅拥有连接组，而且我们还知道哪些神经元相互兴奋或抑制，对吧？所以我们不仅仅是二元掩码；我们也知道权重。我们可以取梯度并从计算上理解每个神经元的作用。

等等。我们比神经科学家有太多优势。然后尽管拥有所有这些优势，这仍然非常困难。我有时会想，天哪，如果对我们来说都很难，那么在神经科学的约束下似乎几乎是不可能的。我不知道，也许我的一部分人认为我的团队里有几个神经科学家。也许我觉得，啊，你知道，神经科学家可能更喜欢处理一个仍然非常困难但更容易的问题。

他们可以来研究神经网络，在我们在这个相对容易理解神经网络的背景下（这仍然非常困难）弄清楚事情之后，我们可以回到生物神经科学。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBPuKYFW3oYvSbrXlwh9UDfdGqWKeEm0DauheicoibmOnDShaI63ralkQVa1Ztu8x5Ylwc1z7u3XexzQ/640?wx_fmt=jpeg&from=appmsg)

**主持人莱克斯：** 我很喜欢你写到的可解释性研究的目标有两个：安全性和美感。你能谈谈美感方面的事情吗？

**克里斯：**
是的。你知道，有趣的是，我认为有些人对神经网络感到有些失望。他们会说：“啊，神经网络，它只不过是一些简单的规则。然后你进行大量的工程来扩展它，它就能工作得非常好。”他们会疑惑：“复杂的思想在哪里？”对他们来说，这似乎不是一个非常漂亮、美丽的科学成果。

我有时会想，当人们这么说的时候，我仿佛看到他们在想：“进化太无聊了；它只是一堆简单的规则，你让进化运行很长时间，你就能得到生物学。生物学竟然以这种糟糕的方式出现！复杂的规则在哪里？”但其美妙之处在于，简单性产生了复杂性。生物学遵循这些简单的规则，并由此产生了我们周围看到的所有生命和生态系统——自然的美丽——这一切都源于进化，源于进化中非常简单的东西。

同样，我相信神经网络在自身内部创造了巨大的复杂性和美丽，但人们通常不会去观察或试图理解这一点，因为这很难理解。我认为，如果我们愿意花时间去探索和理解它，那么在神经网络内部存在着极其丰富的结构，充满了深刻的美丽。

**主持人莱克斯：** 是的，我喜欢你的工作。那种感觉，我们正在理解或瞥见内部正在发生的“魔法”，真是太棒了。

**克里斯：**
在我看来，其中一个问题正呼之欲出。我的意思是，很多人都在思考这个问题，但我常常惊讶的是，为什么没有更多的人思考这个问题：我们为什么不知道如何创建能够完成这些任务的计算机系统？然而，我们拥有这些令人惊叹的系统，我们不知道如何直接创建能够完成这些任务的计算机程序，但这些神经网络却能够完成所有这些令人惊叹的事情。这感觉就像是一个明显需要解答的问题。如果你有一点好奇心，就会想，人类现在怎么会拥有这些能够完成我们无法完成的事情的人工制品呢？

是的，它就是我们培育出来的有机事物，而我们不知道我们培育出了什么。

**主持人莱克斯：** 谢谢你致力于安全工作，谢谢你欣赏你所发现事物的美。谢谢你今天的发言，克里斯。

**克里斯：** 太棒了。谢谢你们抽出时间聊天。

**主持人莱克斯：** 感谢收听与Chris Olah的对话，感谢收听，希望下次再见。

 _参考资料: https://www.youtube.com/watch?v=ugvHCXCOmm4，公开发表于2024-11-11_

**👇关注公众号后设🌟标，掌握第一手AI新动态**

****

## 往期精选

  1. [黄仁勋专访：OpenAI在大模型混战中达到“逃逸速度”](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2650001718&idx=1&sn=f8129a622e7611702be2cb23e8ce9418&chksm=88ba5831bfcdd127d06ce6492c821074407f805407b4182ca900916521cb5a4717f2a3d71ee8&token=1339625777&lang=zh_CN&scene=21#wechat_redirect)
  2. [李飞飞与Justin深度解读空间智能：数字世界需要三维表征，才能与现实世界融合](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2650000659&idx=1&sn=c71fb5b4ef501424dddd5e8b4dd5860e&chksm=88ba4414bfcdcd023c691a1adf75127a9fd883ceb305ca14cf97f719acaf999d40fa72f84bf3&token=1492077842&lang=zh_CN&scene=21#wechat_redirect)
  3. [PayPal创始人彼得·蒂尔：人类科技停滞源于原子方面的进展远慢于比特](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2650000240&idx=1&sn=26af6013981677b1e14137260857a6f0&chksm=88ba4277bfcdcb615d746615c262927bf51c43c920ed93fa36274ef87c6264d6548c84647121&token=106920805&lang=zh_CN&scene=21#wechat_redirect)
  4. [谷歌联合创始人布林：巨头们打造的“上帝模型”几乎可以理解一切](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2649999870&idx=2&sn=0c714d804a72a52e002743d949e1685e&chksm=88ba40f9bfcdc9ef78749718480265922f4fba539abf6c9d62a6cd681f405dee9283d2429f84&token=106920805&lang=zh_CN&scene=21#wechat_redirect)
  5. [马斯克：AI将使商品和服务的成本趋近于零](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2649999870&idx=1&sn=752f000a117a705e77950c82bfc4a004&chksm=88ba40f9bfcdc9ef5a5afe4a3ae73d5247bd54ed525dbdbedee1fcf74a6c082165e664a5c4d0&token=106920805&lang=zh_CN&poc_token=HDp86Waj18SFm2Y-xnv_Vqd_4J6emFoh10eH48wg&scene=21#wechat_redirect)
  6. [Karpathy最新专访：人形机器人、特斯拉、数据墙与合成数据](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2649999613&idx=1&sn=b8bdda7afe4c3ca08e324ac5bbd5a2bd&chksm=88ba41fabfcdc8ec0e21dbf4c7eb4d33252da70f47e1cfc9f5e113717911c417c2aebb3d6180&token=106920805&lang=zh_CN&scene=21#wechat_redirect)

  

