# 图灵奖得主杨立昆分享AI前沿思考：AI领域工程实践领先于理论、5-7年内出现能推理和规划的世界模型

文章作者: 瓜哥AI新知
发布时间: 2024-11-30 07:31
发布地: 浙江
原文链接: http://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2650004667&idx=1&sn=f54ac78af6284cae3bc558486990dfd5&chksm=88ba55bcbfcddcaad2dfa3be8eeb4654f3a2f06d4841f2d81dfca375667245fc7b95a6af083e#rd

封面图链接: https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNp4k80trnmte3ialmjNx9nsWhaFicX3pO0PLjPcpsribzicUjW4CLgD7sKATXvEQJRBpYBO7KGHgXDrA/300

**👇关注公众号后设🌟标，掌握第一手AI新动态**

****

本文访谈内容整理自**图灵奖得主杨立昆** 接受**ProductNation/iSPIRT**
Youtube频道专访，公开发表于2024年11月27日。原始内容参考：https://www.youtube.com/watch?v=4V_cJX8sVeM

## 图灵奖得主杨立昆接受ProductNation/iSPIRT专访

> ★
>
> **内容导读** ：
>
> 本文主要围绕Yann LeCun关于人工智能未来发展方向的观点展开，核心观点如下：
>
>   1. **大型语言模型(LLM)的局限性：**
> LeCun认为当前的LLM虽然强大，但在理解物理世界、拥有持久记忆、推理和规划方面能力有限，仅仅是通过扩展模型规模和数据量无法达到人类水平的AI。
> 他认为这是一种“技巧”，而非真正的解决方案。
>   2. **高级机器智能(AMI)的提出：** LeCun提出了AMI（Advanced Machine
> Intelligence）的概念，认为人类智能并非通用智能，而是专业的。AMI的目标是构建能够理解物理世界、拥有常识、推理和规划的AI系统。
>   3. **联合嵌入预测架构(JEPA)：**
> LeCun认为JEPA架构是构建下一代AI系统的关键。JEPA通过寻找世界的抽象表示，消除不可预测的部分，从而实现对世界的预测和理解，并在此基础上构建世界模型，从而实现规划和推理。
>   4. **对“下一个标记预测”的质疑：**
> LeCun对仅仅依靠“下一个标记预测”来达到AGI的观点表示质疑，认为这是一种错误的路径。他认为需要新的范式，例如JEPA架构和目标驱动的架构。
>   5. **工程优先于理论：**
> LeCun强调工程的重要性，认为在AI领域，工程实践往往先于理论解释，深度学习的许多进展都是经验性的，理论解释滞后于实践。
>   6. **对合成数据的看法：**
> LeCun指出，虽然合成数据在训练AI系统中有一定的作用，但过度依赖由AI生成的数据进行训练可能会导致系统崩溃。他认为需要平衡合成数据和真实世界数据的利用。
>   7. **对未来AI发展的预测：**
> LeCun乐观地预测，在5-7年内可能开发出能够推理、规划和拥有世界认知模型的系统，并在十年内有可能达到人类水平的AI。他希望将这样的智能系统部署到每部智能手机和智能眼镜上。
>   8. **对监管的担忧：** LeCun警告不要通过过度监管扼杀开源AI平台的发展，这会阻碍整个领域的进步。
>   9. **对未来研究方向的建议：**
> LeCun建议AI研究应该更多关注基于模型的强化学习和机器人技术，因为这些领域需要解决规划、推理和处理现实世界复杂性的问题。
>

## 杨立昆简介

杨立昆（Yann
LeCun）是深度学习领域的三位奠基人之一，与杰弗里·辛顿和约书亚·本吉奥共同获得了2018年图灵奖，以表彰他们在深度学习方面的突破性贡献。
杨立昆的研究主要集中在卷积神经网络（CNN）的开发和应用上，这是一种在图像识别、自然语言处理和语音识别等领域取得巨大成功的深度学习模型。

他于1980年代后期在贝尔实验室工作期间，率先提出了卷积神经网络的反向传播算法，并将其应用于手写数字识别，取得了突破性的成果。他的工作为之后深度学习的复兴奠定了坚实的基础。
值得一提的是，他所开发的卷积神经网络架构至今仍然是图像识别领域的主流方法。

除了在学术上的贡献外，杨立昆还在工业界发挥着重要的作用。他是纽约大学的教授，同时也是Meta（脸书）首席人工智能科学家，领导着Meta AI
Research团队，致力于推动人工智能技术的进步与应用。他的研究方向涵盖了计算机视觉、机器学习、人工智能等多个领域，并在各个领域都取得了令人瞩目的成就。
杨立昆对人工智能未来的发展方向有着深刻的见解，他积极倡导并推动着人工智能研究的持续发展，被誉为人工智能领域的先驱之一。

## 访谈全文

**主持人Gaurav：** 进入第一个问题，Yann。我认为你最近在IIT
Meta的演讲中提到，目前的LLM有点停滞不前了。那么，从更高的层面来看，接下来是什么？我知道这是一个非常复杂的问题，我可能会把它分解成更进一步的问题，但你认为接下来是什么？就像学术界的打哈欠一样，不要透露Meta正在做的任何常规的事情。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNp4k80trnmte3ialmjNx9nsb7B2sx7PbogoAkicSfZ32adfM7PorozF6pxbOoznXBEDQR2FaN6fkTw/640?wx_fmt=jpeg&from=appmsg)

**杨立昆：**
好吧，这真是我最喜欢的主题。所以是的，LLM很棒，但是它们的能力非常有限，尤其是在理解物理世界（它们做不到）、拥有某种持久记忆或工作记忆，以及推理和规划的能力方面。你可以添加一些技巧，本质上来说。并不是完全的技巧，但我的意思是这就是他的团队目前正在努力改进LLM的方法。所以，一些技巧可以让系统稍微好一点地理解图像和推理，并使用工具来规划如何使用这些工具。但最终，这不是一个完全令人满意的解决方案。

我们正在研究的一个项目叫做高级机器智能，或AMI。在Meta，我们把它发音为AMI，因为有很多说法语的人，而AMI在法语中是“朋友”的意思。所以我们认为这个名字很贴切。我们更喜欢这个AMI的首字母缩写而不是AGI，原因是人类智能根本不是通用的；它是非常专业的。将人类水平的智能称为通用智能是完全胡说八道。很不幸，那艘船已经启航了，但是我们称之为AMI。

AMI的理念，不用说得太技术化，就是训练自然语言理解系统的方法是使用自监督学习。自监督学习基本上包括训练一个系统来填补空白。你取一段文本，以某种方式损坏它，移除一些词语，并用空白标记替换它们，然后你训练一个大型神经网络来预测缺失的词语。这有一个特例，就是LLM或自回归LLM，在这种情况下，你不需要损坏输入。你只需要显示输入，但是你以这样一种方式构建网络，即为了预测一个输出，它只能查看其左侧的输入。

所以实际上，你是在训练系统预测文本中的下一个词。通过这样做，你可以使用系统生成文本。你让它生成一个词，然后你将该词移入输入，现在你可以生成第二个词。将其移入输入，现在你可以生成第三个词，等等。这种方法被称为自回归预测，这就是LLM的基础。

现在，这种方法的缺点是这些系统每个标记都会消耗固定的计算量。所以，如果你问它们一个复杂的问题，答案是肯定还是否定，你将花费相同的计算量，无论问题是难是易。这没有任何意义。我们倾向于将更多资源投入到复杂的问题而不是简单的问题上。人们发现的技巧是他们人为地诱导LLM生成更多标记，以便它将更多计算资源投入到问题中。这被称为“思维链”，你要求系统解释为什么它会产生这个答案，而不是仅仅问它是否。但这只是一个技巧；一个捷径。

问题在于：当你训练一个系统来预测文本中的下一个词或下一个标记（实际上是一个子词单元）时，你不可能准确地预测文本中接下来会是什么词。因此，你训练系统不是产生单个词，而是产生所有可能词的分布。这导致一个很大的数字向量，介于0和1之间，它们的和为1，其中向量的长度是字典中所有可能的标记或词的数量。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNp4k80trnmte3ialmjNx9ns6xjEj4ViaAbzdrwVq96pO4d0xGPRwx0W9Th6BYIgcxjmylEKLzvwD2A/640?wx_fmt=jpeg&from=appmsg)

这使得系统能够在不确定性的存在下进行训练。你可以使用该系统不仅预测单个词串，还可以通过从它产生的分布中采样来预测多个词串。问题是：有一个已有15年历史的旧思想，所以这不是一个新的概念。我已经研究这个问题至少15年了，这个想法包括使用相同的自监督学习原理来训练一个系统以理解物理世界。

取一段视频，以某种方式损坏它，然后训练一些大型神经网络来填补空白并预测视频中缺失的部分。例如，取一个两秒钟的视频，显示前半秒，屏蔽第一秒，不要向系统显示视频的延续部分。然后训练系统来预测视频的后半部分。我们尝试了15年，但这基本上没有产生任何有趣的结果。系统无法预测视频中接下来会发生什么，因为可能有无限数量的可能事件发生，而这些事件不可能被预测。

如果我拍摄这个人群的视频，从我的位置开始，移动摄像机，并在某个点停止，然后问系统视频接下来会发生什么，它可能会预测摄像机将继续移动，表明那里有一个房间，可能还有一堵墙。房间的一侧也可能坐着人。然而，预测无法准确地描述每个人的样子。它无法预测地面的纹理、墙壁上的木板，或者墙壁在哪里，甚至房间有多大。

世界上有很多因素是无法用部分信息准确预测的。因此，当你训练一个系统来预测视频的延续方式时，它所能做的最好的事情就是预测所有可能未来的某种平均值，这会导致图像模糊。

所以它不起作用。所以，当然，解决这个问题的方法是，让我们不要预测单个图像或单个视频集。让我们预测视频帧上的概率分布。问题是，我们不知道该如何做到这一点。这实际上是一个数学上棘手的问题，不仅计算机科学家，而且物理学家和统计学家几十年来一直在努力解决这个问题。本质上，它是不可解的。

我为此问题苦苦挣扎多年。几年前，我们发现了一种方法可以解决这个问题，方法是建立在一个称为联合嵌入架构的旧思想之上。那么，什么是联合嵌入架构呢？这意味着我们不应该试图预测视频中所有缺失的像素。相反，我们应该找到视频的抽象表示，消除我们无论如何都无法预测的大量无关信息。

然后，我们可以在抽象的表示空间中进行预测，在那里进行预测变得可行。这种方法被称为**联合嵌入预测架构，或 JEPA** 。我目前在 FAIR
与我的同事一起进行的所有项目都基于这个理念。我将解释为什么这是一个非常基础的概念，并强调其重要性。

让我们以一个非常复杂的物体为例，比如木星。木星是一颗行星，一颗非常大的行星。我们可以收集到关于木星的大量信息。但如果我们感兴趣的是预测木星的轨迹，那么我们需要对木星的表示非常简单。我们不需要知道它有多大，不需要知道它的密度、成分、温度，这些都不重要。我们唯一需要知道的是六个数：三个位置坐标，三个速度分量。我们可以预测木星几个世纪的轨迹。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNp4k80trnmte3ialmjNx9nsIs1DOK0uhezFJrCMibQBqktIPARaaNsKNsaVibjibpmC0ISUCOSGZzV9w/640?wx_fmt=jpeg&from=appmsg)

假设木星的质量比太阳小得多，事实也确实如此。这告诉我们，预测问题是一个学习世界良好表示的问题，即消除无法预测的部分，保留可以预测的部分。这就是 JEPA
架构的设计目标。我认为这将成为下一代人工智能系统的蓝图。

我们可以从 JEPA 架构中构建一些我称之为（当然，我不只是一个人这样称呼它们）**世界模型**
的东西。什么是世界模型？世界模型是一种对世界的认知模型，它允许你根据t时刻世界的状态和你想象中采取的动作来预测t+1时刻世界的状态。如果你拥有这样的世界认知模型，那么意味着你可以在脑海中想象一系列动作的结果，并查看这一系列动作是否能满足你想要完成的任务。

例如，往杯子里倒水，对吧？我想把这个杯子装满水。我需要拿起杯子，把它转过来，拿起水瓶，打开它，然后倒水，对吧？这是一个非常复杂的动作序列。当然，我们习惯于做这种动作，所以不需要计划。我们习惯于在不经思考的情况下完成它。但是，你第一次遇到的很多问题都需要你计划一系列动作来完成它们。这就是所谓的规划。

你可以将每一个推理问题简化为一个非常类似的问题：找到一系列满足特定条件的动作或符号，该条件表征你是否完成了任务。这里适用优化推理的原则。你在这种情况下解决的问题是一个优化问题。你有一个目标函数来指示你的任务是否完成，一个世界模型来预测世界的状态是否是你想要的，以及你需要优化的动作序列。

这个优化过程允许你搜索一个动作序列，以最小化你设置的目标函数。这就是优化推理，从计算上讲，它比通过神经网络的一堆层进行前向传播的推理（这就是大型语言模型所做的）本质上更强大。

也许通过这种架构，我们将有一种方法来构建能够推理、规划和拥有世界认知模型的机器。也许这些模型将是分层的——这是另一个话题。也许我们将找到一条通往能够从视频中学习世界如何运作、拥有常识、能够推理和规划的系统的道路，也许在某个时候能够达到人类水平的智能。

**我们的计划，这是一个乐观的计划是：我们也许能够在五到六年内（也许是六到七年）开发出能够完成所有这些事情的足够强大的系统，并且也许能够在十年内走上构建这类人类水平系统的道路**
。这就是 Meta 的基础人工智能研究组织 FAIR 的关注重点。我们也在从事其他应用项目，但这才是主要任务。

Mano（坐在房间后面）正祈祷我们最终能够成功，并且 Llama5 或 6，也许是
7，我不知道，将基于这个新理念。我们的目标是在每部智能手机和智能眼镜上部署这些智能系统，让世界上每个人都能与一个智能人工智能助手对话，这个助手可能比他们更聪明，但会帮助每个人在日常生活中。

也许这将增强整个人类的智力，并将人类提升到一个新的水平。怎么样？这个宇宙级的结论如何？

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNp4k80trnmte3ialmjNx9ns581PtKJYOYicOWSZj13ckWyicINvYV7tVouIH84NhR4BojfTdhnHTAQQ/640?wx_fmt=jpeg&from=appmsg)![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNp4k80trnmte3ialmjNx9nsEvPuiatluA5wiblbRjFvL1c2gP4s5pbn21cHuibGeT8bzDVlCFku0mJfA/640?wx_fmt=jpeg&from=appmsg)

**主持人Gaurav：** 谢谢。说到这里，我想问一个问题。我知道你不再是自回归模型的粉丝了，至少就将其应用于 AGI 或
AMI（你所说的那样）而言。我相信你最近肯定看到了，我想几个月前，Jensen 与 Ilya
进行了一次炉边谈话，我认为它已经很火了。所以他举了一个例子，说明他为什么相信下一个标记预测是……他们仍然称其为 AGI，所以它将导致
AGI。这个例子就像，如果你正在阅读一本悬念小说，在结尾处，作为一个人，当名字即将揭晓时，你多少会猜到它。你对此有何看法？我知道我有自己的看法，但是，我们在这里，想知道你对此的看法。

**杨立昆：** 我的意思是，Alonville
是一位杰出的研究人员。他为这个领域带来了很多贡献，取得了一些非常重大的成就。但是，在这件事上，我认为他完全错了。我们不会仅仅通过扩展大型语言模型，然后用更多的数据训练它们，然后进行一些调整以获得一点推理能力和一点图像理解能力，就能达到人类水平的人工智能。

我的意思是，短期内做所有这些事情是有用的，这是毫无疑问的。我们正在努力。这就是你将在某个时候看到的
Llama4。我们不会告诉你具体时间。但显然，这是短期内非常有价值的事情。

但从长远来看，如果你想达到人类水平的智能，我们需要新的范式，也许是我刚才描述的那种，使用 JEPA
架构和目标驱动的架构系统，这些系统真正规划它们的答案和行动。对此，人们可能有不同的意见。

Ilya 和我们的共同导师 Geoff Hinton
认为大型语言模型实际上具有主观体验。我实在不相信是这样。所以，这种想法认为，我们只需要扩展模型，需要在计算基础设施上投入数万亿美元，收集世界上所有的数据，这样人类水平的智能就会从大型语言模型中涌现出来，我根本不相信是这样。

在人工智能的历史上，人们长期以来一直在采用新的范式。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNp4k80trnmte3ialmjNx9nsGIIXyUfNnl1CW3wfGTkYdia8Zd5tou8PZP0iaH0aicr9Ob5uiacWD6icZog/640?wx_fmt=jpeg&from=appmsg)

然后认为问题已经解决了。自从 20 世纪 50 年代以来，这在人工智能历史上反复出现。20 世纪 50 年代最早的经典人工智能系统之一是传奇人物
Newell 和 Simon 的工作，他们都获得了图灵奖，以表彰他们对人工智能的贡献。他们在 1956
年构建了一个人工智能系统，他们非常谦虚地称之为通用问题求解器。他们认为推理可以简化为对解决方案的搜索，基本上就是我描述的过程。你有一个成本函数或目标函数，以及一个可能的配置或解决方案空间。你只需在这个空间中搜索一个满足条件的解决方案，你的问题就解决了。

一个例子是旅行商问题，你有一列城市，你必须找到城市之间最短的路径。可能的轨迹数量是有限的，本质上是 n
的阶乘。你只需搜索它们并找到最佳的路径。如果你想下棋，你可以想象你可以做的所有动作，然后对手可以做的所有动作，接着是你可以在此基础上做的所有动作。这会创建一个可能性树，你遍历它以找到最有可能让你获胜的路径。这就是设计下棋系统的方式。

然而，事实证明，所有可以用这种方式表述的问题本质上都是难以处理的。它们都会导致指数增长，**因此通用问题求解器实际上无法解决任何有趣的问题**
。这种限制导致了人工智能的一个分支，我们现在称之为“优秀的老式人工智能”，主要关注启发式编程或逻辑推理、基于规则的系统。人工智能的另一个分支是基于感知器和神经网络的学习。

人们在发现这些新原理时，相信感知机将引领智能机器的诞生。事实并非如此。通用问题求解器也被认为能够引领智能机器的诞生，但这同样是错误的。人们曾期望通过Prolog和专家系统进行逻辑推理能够创造出一个完整的智能机器产业，但这也不正确。20世纪80年代和90年代，带有反向传播算法的神经网络也被认为预示着智能机器的到来，而这最终也被证明是错误的。虽然这并非完全错误——因为我们现在所做的一切都是对那些理念的延续——但这仅仅是为时过早。

在漫长的发展历程中，许多年轻人都对自己的成果充满热情，宣称：“就是这样！我们解决了这个问题！”相比之下，年长者往往会回应：“等等。我以前见过这种情况。”我年纪大了，但我有一个计划。然而，该领域也有一些人说：“我们永远也无法实现目标。我们尝试了50年，什么都没成功。是的，我们正在取得进展，但这就像玩具一样，我们只会制造应用。我们甚至不应该尝试创造智能机器。”但我们有一个计划。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNp4k80trnmte3ialmjNx9nsPXgvxq6TC7weJ9tlsfvqODflnuK2hqTIQkBn9Pp1lRGFKHNqnias7DQ/640?wx_fmt=jpeg&from=appmsg)

**主持人Gaurav：**
我有一个计划。很好。你知道，我在听你以前在法国某个研究所做的演讲时，你提到过类似“理论路灯效应”的东西。就像过分关注数学的正确性一样，它限制了创新。

你还提到工程优先。就像提出想法就是工程。而理论则会随着时间的推移，以某种方式提高这些想法的效率。

你认为这种对可能被误导的想法的过度热情（我只是借用你的说法），是否也存在这种并非理论上的，而是类似于“路灯效应”的情况？我们花费了过多的时间、精力和金钱（这是真实的），这实际上会减缓我们在真正创新方面应该取得的进展。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNp4k80trnmte3ialmjNx9nsAibM3yLCLvxiayXibibf2akg7r9QcySj8berM4R30I1xRl2CYUdgrOPspQ/640?wx_fmt=jpeg&from=appmsg)

**杨立昆：**
我认为这个领域经历了从一个极端到另一个极端的转变。20世纪80年代和90年代，人们对神经网络的潜在应用确实存在非理性的过度乐观情绪。事实证明，这比我们想象的要困难得多，因为我们需要当时我们所不具备的大型数据集和强大的计算机。我们还需要一种方法让科学家们通过开源代码来分享经验。在20世纪80年代和90年代实施神经网络是一项非常艰巨的任务，人们并没有共享开源代码。没有通用的平台；没有每个人都在使用的Python或通用的操作系统，不同类型的计算机使得代码不可移植。除非你拥有非常昂贵的计算机，否则浮点运算速度非常慢，因此该领域无法取得进展。

缺乏良好的代码交换方式严重阻碍了发展。大学，当然还有工业界，会免费向所有人提供他们的代码的想法，在20世纪90年代是完全不可能的。由于缺乏数据、计算资源以及共享代码和技巧的能力，该领域无法起飞。训练神经网络需要许多隐藏在代码中的技巧，你不能仅仅从论文中复制结果。你必须下载代码才能看到例如学习率的调度是如何实现的。因此，该领域在90年代逐渐衰落，因为很少有人愿意投入资源来构建深度学习神经网络训练系统。

在我博士后期间，我花了一两年时间致力于这项工作。Geoff Hinton
觉得我什么都没做，但我实际上一直在编程。当我来到贝尔实验室时，我们完成了那个项目，这使我们能够开发卷积网络。虽然卷积网络并非一个特别具有革命性的概念——其他人以前也有类似的想法——但实现和使它们发挥作用需要大量的时间来编写软件。一旦我们有了名为SN的软件（后来成为开源软件），它就赋予了我们强大的能力。构建我们自己的工具使我们能够产生其他人无法产生的结果。

我们还在Meta公司通过PyTorch和其他我们开源的工具做了同样的事情，使其他人能够从我们的进步中受益。当时真正缺乏的是沟通、计算和数据。人们对神经网络技术的兴趣在90年代减弱了，人们为不从事神经网络研究找借口。相反，他们选择了更简单的方法，例如逻辑回归、贝叶斯推理、提升方法和像XGBoost这样的分类树。常见的借口是神经网络太复杂了——唯一能够训练它们的人是Yann
LeCun，也许还有另外一两个人。

虽然神经网络取得了令人印象深刻的成果，尤其是在手写识别方面，但它们在小型数据集上的性能有限。它们需要大量的数据进行训练，对于许多应用来说是不切实际的。此外，虽然在像MNIST这样的数据集上取得了良好的结果，但这只是一个问题。许多人认为，使用支持向量机等方法，可以用更低的复杂度获得类似的结果。

人们倾向于驳斥支持神经网络的证据，往往更倾向于理论论证。这些论证声称，任何输入-
输出函数都可以用更简单的模型来逼近，例如一个由固定第一层和训练第二层组成的两层系统。这种方法被认为更简单，具有凸代价函数，可以保证收敛，而无需进行大量的调整。随着人们找到理论上的理由来否定神经网络，并将它们标记为非凸且缺乏有效性证明，这些方法越来越流行。

这种情况**就像你在街上某个地方丢了钥匙，却在路灯下寻找，因为那里有光——尽管知道钥匙在其他地方** 。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNp4k80trnmte3ialmjNx9nsvPFwjrsiaXnmekthZZmnWaQ46VNtHrrSTKawtUSa6RMLPyfOQRDNqYA/640?wx_fmt=jpeg&from=appmsg)

**主持人Gaurav：** 现在我将进入听众提问环节。AI
Foundry的Subhashish想知道，谁将在印度创造公平的等价物？Meta会在印度这样做吗？他们在巴黎做了什么？

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNp4k80trnmte3ialmjNx9nsPweAnLMXcqMR46ID1NB3bDqKbHvVQKW34YNvH67SsTau91BlibZga5w/640?wx_fmt=jpeg&from=appmsg)

**杨立昆：**
好的，有一点令人遗憾的是，自2022年末以来，Meta经历了一个阶段，基本上是规模缩小了一些，消除了在COVID期间发生的过度增长，因为在COVID期间对Meta的服务需求巨大。因此，Meta雇佣了很多人，他们意识到，好吧，那是暂时的；现在我们必须裁员。所以这不是在某个地方开设新研究实验室的好时机，好吗？

然后，Meta非常以硅谷为中心，是一家非常以硅谷为中心的公司。我们在纽约、蒙特利尔、西雅图、巴黎、伦敦、特拉维夫设有实验室，其中最大的是巴黎、纽约和门洛帕克。所以它在某种程度上是分散的。对于研究而言，工程的集中度要低得多。我们只有少数几个地方设有大型工程中心。因此，作为一种理念，该公司不喜欢过分分散或传播，尤其是在裁员的时期。

所以在过去两年里，这个问题根本没有提出。但这在未来可能会改变，并且可能在其他地方开设新的实验室，可能在印度，也可能在其他国家。我们在亚洲没有任何实验室。我们在欧洲和北美洲以外没有任何工程办公室。所以这可能会在某个时候发生。我认为公司定期探索这些可能性，但目前还没有任何可以报道的内容。

好的，那是关于Meta的。现在，我认为印度应该做的是，也许应该稍微改变一下产业文化，这类似于我刚才讲述的关于法国的故事，这样印度那些盈利丰厚的公司就可以建立雄心勃勃的研究实验室，然后为年轻有抱负的科学家提供职业机会。也许这可以鼓励印度的年轻科学家回来在这里找工作。你们中的一些人也这样做过，回来了，也许吧。

但这意味着为印度的科学家和研究人员在学术界之外和产业内部创造职业机会。这需要投资、承担风险的意愿、进行开放式研发和改变文化的意愿。![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNp4k80trnmte3ialmjNx9nstKl2CYWSzovnbIiaTrkp9Zch27z8hECsKuI9wJwAxVm6U9u95A51bMA/640?wx_fmt=jpeg&from=appmsg)

**主持人Gaurav：** 我来看下一个问题，这是Jaskaran Singh
Dubverse提的。您可能在之前的某个演讲中提到过，我们将在5到10年内达到人类水平的AI。所以我想问的是，您认为我们能否在那之前理解神经网络，并提出一个强大的深度学习理论？我不知道，这有点类似于我之前问您的问题。我的意思是，我知道您的答案。

**杨立昆：**
是的，我的意思是，目前在神经网络领域有很多理论研究，试图理解它们是如何工作的，为什么它们有效，以及为什么它们如此有效，即使它们违背了你在统计学教科书中看到的基本原理。每本统计学教科书都会告诉你，在统计估计模型中，你不希望参数过多，因为这会导致过拟合，无法正确外推。这在很大程度上是正确的，但神经网络除外。神经网络即使你给它们大量的参数，它们仍然能够泛化。

从理论的角度来看，这是人们曾经用来论证神经网络永远不会有效的一个借口。事实证明这是完全错误的，理论学家不得不修改理论，并提出了像双重下降和隐式正则化这样的概念。所以，这是需要考虑的一点。

关于之前的问题，我还有一个观点，这与这个问题相关。在科学技术史上，**工程制品的创造往往先于解释其工作原理及其局限性的理论**
。蒸汽机就是一个很好的例子。蒸汽机发明于18世纪后期，但其理论——热力学——直到大约一个世纪后才出现。像卡诺循环和熵这样的概念直到19世纪才出现。

同样，对于飞机来说，虽然空气动力学已经是一个领域，但在20世纪初期并没有完全发展起来。然而，一些实验者想出了如何制造机翼，搭建风洞，并实验飞行器，比如莱特兄弟和克莱蒙·阿德尔。最终，关于机翼空气动力学的全面理论是从他们的实验中发展出来的。

望远镜和透镜的发明早于牛顿建立光学理论。当然，中世纪的阿拉伯世界也有其他思想家理解这些概念，但这些知识在西方基本上是未知的。这种事件顺序在技术进步中非常常见。

在某种程度上，**深度学习被认为有点像炼金术，因为其中很大一部分是经验性的**
。你对某些可能有效的方法有一些直觉，你尝试一下，它不起作用，所以你尝试其他的方法，这些方法可能有点作用。你常常对它如何工作感到惊讶，然后试图弄清楚它为什么有效。因此，这个领域很大一部分仍然是经验性的。然而，也有一些理论，并且需要更多的理论。几十年来，我一直试图吸引物理学家和应用数学家来解决解释深度学习如此有效的问题。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNp4k80trnmte3ialmjNx9ns8W6LQEA6q6azNfiawicN6v86rjjaJg3j4hiaibvMV2Llc7XjZf9rjnq3MQ/640?wx_fmt=jpeg&from=appmsg)

**主持人Gaurav：** 还有一些关于监管的问题。我跳过这些问题，因为Sunil是我的朋友。

**杨立昆：**
好的，对于这个“非问题”，我有一个答案。不要通过监管将开源AI平台扼杀。好的，监管者可能会犯很多错误，这些错误可能会故意或无意地使部署开源AI平台变得有风险，并基本上会扼杀整个领域。它会扼杀这个生态系统。我在印度交谈过的每个人都在某种程度上使用Llama。我们谈论的是数百家初创公司和像Infosys这样的大公司。他们正在使用它，他们正在使用Llama为他们的客户提供服务。如果你使开源变得困难或有风险，那么整个行业第二天就会消亡。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNp4k80trnmte3ialmjNx9nsDtrgPZ4I11ibXakJKA4GtQg5ibmtC9ErKbzJD5N1xpib5fOWo5FogIjWw/640?wx_fmt=jpeg&from=appmsg)

**主持人Gaurav：**
是的，这显然会导致更多的整合。这是关于合成数据的。实际上，我有一个关于合成数据的问题。在我回答这个问题之前，我先问一下。您对合成数据怎么看？有很多初创公司出现，比如精品初创公司。很多资金，再次，都投入到其中，我认为这种情况最近有所改变。所以，我再次很想了解Yann真实的看法，就是关于合成数据。因为我的看法是，合成数据并非真正合成，它是从自然数据中学习得到的。我的意思是，它不是随机的。那么您认为呢？它有用吗？人们应该投资吗？

**杨立昆：**
首先，在某种程度上，所有用于训练LLM的数据都是合成的。它是人类产生的，这表明它本质上是合成的。然而，有一种自然合成的类型，指的是人们出于其他原因而产生的文本，而不是专门用于训练LLM。这包括从公共互联网收集数据以及从出版商处获得一些数据的许可。

一旦数据被收集，就需要进行清理，因为大部分数据都是垃圾。这些经过清理的数据随后成为LLM的预训练数据。在这个预训练阶段之后，LLM往往无法按预期工作。因此，需要进行微调以帮助它有效地回答问题和解决问题。这个微调过程需要半合成数据，这包括雇佣很多人与系统互动，训练它产生正确的答案，并对它生成的响应进行评分。

在某些情况下，这种微调可以半自动完成。例如，如果你有一个代码生成系统，你可以要求它编写一个产生特定结果的函数，并自动检查代码是否有效。这允许系统自行调整并提高其生成有效代码的能力。

此外，还有其他类型的合成数据，其中可以创建简单的模拟世界。在这些世界中，可以发生基本事件，例如Yann走到桌子旁拿起一个玻璃杯。这种类型的数据可以帮助以受控且可预测的方式训练系统。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNp4k80trnmte3ialmjNx9nspd0nOxDgNVb7CyAEibyqWhuibFRItpMJtZBET57T8g35TZVkKMn7N5wA/640?wx_fmt=jpeg&from=appmsg)

然后拿起瓶子。现在你问，桌上有多少个玻璃杯？好的，只有一个了，因为其中一个不在桌上了。这种情况将教会系统，一个物体可以同时存在于两个地方，当你从一个地方拿走一个物体时，如果一开始有n个物体，那么在这个地方还剩下n-1个。所有这些我们认为理所当然的基本事物都需要系统进行训练。

逻辑推理的基本思想必须经过训练。为此，您可以使用合成数据。所以我认为人工智能系统的发展前景光明。然而，如果你想象一个未来，互联网上的大部分数据、大部分文本都是由人工智能系统生成的，并且你继续用其他人工智能系统生成的数据来训练这些人工智能系统，那么系统将会崩溃。

我们对此进行了实验，我们最近发表了一篇来自巴黎的论文，证明这种方法是行不通的。我们必须小心这一点。

**主持人Gaurav：** 我认为，作为一个群体，我们对数据的渴望正在得到满足。因为很快，互联网上大部分数据都将以某种方式由大型语言模型生成。

**杨立昆：**
我们可以利用数据来模拟虚拟世界，让系统学习很多关于世界结构的知识。最终，我们不希望系统仅仅依靠文本进行训练。我们希望利用视频进行训练，也许可以使用我之前提到的JEPA架构。

我经常引用一组数字：如今一个典型的LLM训练使用了20万亿个token。每个token大约是三个字节。因此，总数据量为10的13次方字节。我将把它四舍五入到10的14次方字节。这相当于互联网上所有公开文本的总和。我们任何一个人，都需要花费数十万年才能读完这些文本。这是一个巨大的数据量。

也就是说，一个四岁的孩子已经醒着16000小时了。在这16000小时的清醒时间里，一个孩子或婴儿通过视神经向视觉皮层感知大约每秒2兆字节的数据。我们有200万条视神经纤维，每条每秒传输大约1个字节的数据，所以就是每秒2兆字节。计算一下16000小时的数据量，四年内，一个孩子看到的数据量与最大的LLM（约10的14次方字节）相当。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNp4k80trnmte3ialmjNx9nshILQUK8pFT8nnYhuvoiak3G88PFe7XKe8klTrVyZFHrOMKZRy0nRqqA/640?wx_fmt=jpeg&from=appmsg)

当然，这与高度结构化的抽象数据不同。这是关于世界运作方式的低级数据。但事实上，由于这些数据是冗余的，因此它使得自监督学习更高效。所以我们必须攻克这个难题，让系统能够像婴儿和幼小的动物一样，通过观看视频来学习世界如何运作。我们不会仅仅通过文本训练就能达到人类水平的AI，这就是为什么Ilya……

**主持人Gaurav：**
是的，说到这里，我们知道你认为谁是错的。在当今领先的研究人员或学者中，你最认同谁？所以最后都是以友谊告终，对吧？不是说所有人都是错的。

**杨立昆：**
我的研究生涯一直试图让其他人认同我的观点。但你不能对自尊心很强的科学家这样做。你不能告诉他们做什么研究。你雇用他们是因为他们对研究方向有很好的判断力。所以，将科学家们引导到特定的观点就像牧羊一样。这行不通。你必须使用反向心理学或其他方法。**这非常困难，这就是为什么我不再做管理工作的原因。我是一个个人贡献者。我在Meta拥有最好的工作，因为我是一位高级副总裁。但我没有任何下属。**

**杨立昆：**
这简直是最好的职位了。我拥有一定的合法性和知识影响力。所以，当我说话时，一些人会听我说。不是因为他们必须听，因为我是他们的老板，而是因为他们想听。这是一个更好的位置。

好的，现在。你仍然需要说出一个或几个名字。我喜欢的人大多是对机器人技术感兴趣的人。当你从事机器人技术研究时，你必须考虑规划、推理以及处理现实世界的复杂性——这些问题是LLM完全绕过的。我对机器人技术感兴趣大约20年了，并在此背景下参与了各种项目。

我认为，在机器学习领域带来新的概念性进步的人，是那些从事他们所谓的基于模型的强化学习的人。我不喜欢这个术语；我认为它更多的是关于最优控制与机器学习的结合。有一些人试图使用深度学习并将其应用于机器人技术，例如伯克利的Sergey
Levine、Peter Abiel、斯坦福的Chelsea Finn和麻省理工学院的Russ
Tedrake。还有许多这样的人，我相信该领域应该更关注这个方向。

我在两年半前发表了一篇长篇论文并将其发布到网上。标题是《走向自主机器智能的道路》。在这篇论文中，我解释了我认为人工智能研究在未来十年将走向何方。这就是我们在FAIR遵循的计划，我希望说服其他人来帮忙。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNp4k80trnmte3ialmjNx9nsxsKXVlVBG4rJOIltx7RBlE37PQgATNPjtrh6RBVEBKvibraBeQiaYO4A/640?wx_fmt=jpeg&from=appmsg)

**主持人Gaurav：**
非常感谢。我没有回答所有问题，对于那些没有被回答问题的人，我深表歉意。再次感谢Yann与我们在一起并回答所有问题，感谢你直言不讳。谢谢大家。

**杨立昆：** 谢谢。

 _参考资料: https://www.youtube.com/watch?v=4V_cJX8sVeM，公开发表于2024-11-27_

**关注公众号后设🌟标，掌握第一手AI新动态**

##  往期精选

  1. [黄仁勋专访：OpenAI在大模型混战中达到“逃逸速度”](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2650001718&idx=1&sn=f8129a622e7611702be2cb23e8ce9418&chksm=88ba5831bfcdd127d06ce6492c821074407f805407b4182ca900916521cb5a4717f2a3d71ee8&token=1339625777&lang=zh_CN&scene=21#wechat_redirect)
  2. [李飞飞与Justin深度解读空间智能：数字世界需要三维表征，才能与现实世界融合](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2650000659&idx=1&sn=c71fb5b4ef501424dddd5e8b4dd5860e&chksm=88ba4414bfcdcd023c691a1adf75127a9fd883ceb305ca14cf97f719acaf999d40fa72f84bf3&token=1492077842&lang=zh_CN&scene=21#wechat_redirect)
  3. [PayPal创始人彼得·蒂尔：人类科技停滞源于原子方面的进展远慢于比特](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2650000240&idx=1&sn=26af6013981677b1e14137260857a6f0&chksm=88ba4277bfcdcb615d746615c262927bf51c43c920ed93fa36274ef87c6264d6548c84647121&token=106920805&lang=zh_CN&scene=21#wechat_redirect)
  4. [谷歌联合创始人布林：巨头们打造的“上帝模型”几乎可以理解一切](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2649999870&idx=2&sn=0c714d804a72a52e002743d949e1685e&chksm=88ba40f9bfcdc9ef78749718480265922f4fba539abf6c9d62a6cd681f405dee9283d2429f84&token=106920805&lang=zh_CN&scene=21#wechat_redirect)
  5. [马斯克：AI将使商品和服务的成本趋近于零](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2649999870&idx=1&sn=752f000a117a705e77950c82bfc4a004&chksm=88ba40f9bfcdc9ef5a5afe4a3ae73d5247bd54ed525dbdbedee1fcf74a6c082165e664a5c4d0&token=106920805&lang=zh_CN&poc_token=HDp86Waj18SFm2Y-xnv_Vqd_4J6emFoh10eH48wg&scene=21#wechat_redirect)
  6. [Karpathy最新专访：人形机器人、特斯拉、数据墙与合成数据](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2649999613&idx=1&sn=b8bdda7afe4c3ca08e324ac5bbd5a2bd&chksm=88ba41fabfcdc8ec0e21dbf4c7eb4d33252da70f47e1cfc9f5e113717911c417c2aebb3d6180&token=106920805&lang=zh_CN&scene=21#wechat_redirect)

  

