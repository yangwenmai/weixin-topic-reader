# 核心研发揭秘Cerebras——挑战GPU的巨型AI芯片、2100token/s、晶圆即芯片、良率100%

文章作者: 瓜哥AI新知
发布时间: 2024-12-29 12:46
发布地: 浙江
原文链接: http://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2650006145&idx=1&sn=7767f23795363d92986b21e7f81c0f8a&chksm=88ba6b86bfcde290322500a207d946e52ad3fae790af5a34ede7f0b853cddf8e0b1a80de803d#rd

封面图链接: https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBPIVefV2ibmhPeJ5ic3n0jia14mYMLvTk43GV2U20eubMM3aFQadgNS34MFhZdVIAQVUnu89MHdqd6Ww/300

**👇关注公众号后设🌟标，掌握第一手AI新动态**

本文内容整理自**Joel Hestness** 接受**Dr Waku**
Youtube频道专访，公开发表于2024年12月25日。原始内容参考：https://www.youtube.com/watch?v=qC_lCFTOJU0

![](https://mmbiz.qpic.cn/sz_mmbiz_png/ClW8NejCpBPIVefV2ibmhPeJ5ic3n0jia149yd0TrTicgqLIvpcMPx85IzGDyhMENuGEyoddwicZjA3ics6eSyTAlJOA/640?wx_fmt=png&from=appmsg)

  

## 内容提要: Joel Hestness接受Dr Waku频道专访：Cerebras巨型芯片如何挑战英伟达GPU在AI领域的霸主地位

![](https://mmbiz.qpic.cn/sz_mmbiz_png/ClW8NejCpBPIVefV2ibmhPeJ5ic3n0jia14aLFkLDSJxznDeb31SCib5XLuc5tQn8HicsBtaI83SeFeBRzP8NG5sjTA/640?wx_fmt=png&from=appmsg)

本文主要围绕Cerebras Systems及其新型机器学习处理器展开，核心观点如下：

  1. **Cerebras处理器架构:** Cerebras使用整块晶圆作为单个芯片，而非传统的切割成多个芯片，显著提升性能（相当于16-20个GPU），并简化编程和扩展。其40GB SRAM内存靠近核心，延迟极低，支持权重流式传输到晶圆上进行计算，而非将数据传输到计算单元，从而提高效率。
  2. **大型语言模型训练能力:** Cerebras系统能够在一个设备上运行万亿参数模型（与GPT-4规模相当），并支持横向扩展训练更大规模的模型（目前可达24万亿参数），领先于目前基于GPU的解决方案。他们已经成功在单个设备上运行万亿参数模型，并正在探索专家混合模型。
  3. **高效推理能力:** Cerebras近期发布了推理解决方案，其速度比其他硬件快4倍，比GPU快15-20倍（每秒处理2100个令牌），这得益于其独特的流水线执行方式和领域专用语言(DSL)生成的内核优化。
  4. **应对芯片缺陷的策略:** Cerebras处理器采用了冗余设计，并通过固件映射来屏蔽制造缺陷，从而实现100%良率。失效核心会被自动屏蔽，不影响整体性能。这与传统的芯片制造方式不同，后者通常会丢弃有缺陷的芯片。
  5. **硬件访问方式:** Cerebras提供多种访问方式：直接销售硬件、提供云服务（包括预留实例和按需使用）、白手套服务（协助客户进行机器学习工作）以及推理API服务（支持开源和自定义模型）。
  6. **软件和生态系统:** Cerebras系统支持PyTorch框架，并致力于提高其对Hugging Face模型库的支持程度，方便用户使用熟悉的工具。他们正在开发即时执行功能，并优化各种操作以提升性能。
  7. **系统架构:** Cerebras系统是一个分布式系统，包含三种类型的节点：加速器节点（包含晶圆）、MemoryX节点（存储权重）和SwarmX节点（网络通信）。这种设计更利于处理大型模型和高性能计算任务，例如分子动力学模拟等。
  8. **冷却方案:** Cerebras采用高效的冷却系统，其功率密度与NVIDIA H100 GPU相当，每个设备功耗约为15千瓦。
  9. **未来展望:** Cerebras看好推理时间计算的未来发展，并计划为学术界提供计算资源支持。

## Joel Hestness 简介

赫斯尼斯（Joel Hestness）是一位在机器学习和计算机体系结构领域具有重要贡献的研究科学家。他在Cerebras
Systems担任核心机器学习团队负责人和高级研究科学家。赫斯尼斯的研究重点在于自然语言应用、其规模化、训练动态以及效率特征。他在Cerebras-
GPT等项目中研究了计算效率的规模化定律，并在BTLM等模型中取得了突破性的成果。

## 访谈全文

**主持人：** 大家好。今天我和Joel Hesnes一起，他是Cerebras
Systems核心机器学习团队的团队负责人。Joel，可以介绍一下自己吗？

**乔尔·赫斯尼斯：**
我很好，很高兴来到这里，期待与您聊聊。我是核心机器学习团队的负责人，也是一名研究科学家。我的专业背景是硬件，做过异构系统架构方面的工作。研究生毕业后，我加入了百度研究院，在那里与吴恩达（Andrew
Ng）和许多其他优秀的人一起工作，专注于大型集群上的首批深度学习模型。这些集群由GPU组成。

我们在那儿做的一些工作集中在扩展定律上：当我们改变数据集大小和计算资源时，这些模型的质量如何变化？我们发表了第一篇论文，证明深度学习可以预测地扩展到非常大的规模。随着数据集大小、模型大小和计算资源的增加，模型质量也会提高。这项工作可能是Kaplan扩展定律论文的先驱。我注意到一个趋势，即语言是深度学习的下一个重要领域，因为它是计算密集度最高的领域。我们研究的许多模型，如循环模型，都严重依赖矩阵乘法。基于矩阵乘法的Transformer模型与早期使用卷积方法的深度学习算法不同。

百度研究院的另一个令人兴奋的方面是系统方面，一个小组为百度风投进行尽职调查，以决定投资哪些组织。他们考察了各种硬件公司，让我看到了Cerebras、Groq和Graphcore等公司的创新。我认识到Cerebras的解决方案在大型语言模型中具有特别有效的潜力，我相信这将是该领域的未来方向。在很大程度上，这种情况已经发生，大约五年前半我加入了Cerebras。

随着时间的推移，我在Cerebras从事过各种不同的工作。目前，我的团队主要专注于我们在系统上进行的预训练工作。我们预训练非常大型的模型，例如中东最好的阿拉伯语-
英语多语言模型，它包含300亿个参数。我们现在正与一个名为Core
42的团队合作，训练下一代模型。我的团队的大部分工作都涉及确保我们拥有正确的方案和数据集，确定如何扩展以及保证稳定的训练过程。我们还将我们的发现和方法贡献给Cerebras的其他团队。我与我们的应用机器学习团队密切合作，因为我们每天都与客户互动。他们处理一些预训练工作，以及大量的后期训练工作和适应工作。

**主持人：**
非常酷。我们现在实际上在NeurIPS大会上，Cerebras在那里有一个展位。他们正在向所有机器学习与会者做广告，试图让人们更多地了解他们的系统。

**乔尔·赫斯尼斯：**
是的，Cerebras拥有一款专为机器学习设计的处理器。它是一款全晶圆处理器，所以我们不需要像处理GPU或其他加速器那样将晶圆切割成芯片然后进行封装，而是可以将其作为一个完整的晶圆保留下来。全晶圆的性能相当于大约16到20个GPU，而且由于我们将它作为一个整体保留下来，因此编程和扩展到非常大型的应用程序非常简单。

**主持人：**
我想我们之前在我的频道里讨论过Cerebras。是的，独特之处在于他们将整个晶圆作为一个巨大的芯片来处理，而不是将其分成更小的芯片，这更为常见。所以，看到除了GPU之外，还有不同的加速器用于AI训练和推理，这非常令人兴奋。

所以Joel，你认为Cerebras现在正在发生什么特别让你兴奋的事情？

**乔尔·赫斯尼斯：**
当然，是的。我们在NeurIPS大会上要讨论一些重要的事情。我的团队正在研究的主要课题之一是超大型模型。在过去的几个月里，我们与桑迪亚国家实验室一起展示了一个万亿参数模型在我们硬件上的运行情况。

这其中的有趣之处在于，这是一个能够在我们单个设备上运行的万亿参数模型。对于那些不太熟悉的人来说，如果你想在GPU上训练一个万亿参数模型，你必须将其细分为多个独立的设备，然后将它们缝合在一起才能执行完整的计算。

我们的硬件架构的结构使得我们相对容易地将权重存储在一组单独的服务器中，然后将其流式传输到我们的设备中。这使我们能够在一个设备上运行模型。但是，我认为你实际上并不想在一个设备上将万亿参数模型训练到收敛，因为它需要大量的计算，并且需要非常长的时间。

这就是为什么我们具有横向扩展能力，能够跨多个设备进行数据并行。这使我们能够扩展并训练非常非常大型的模型。我们期待着这方面的下一步进展。

**主持人：**
万亿参数的规模与GPT-4基本相同。或处于那个规模。所以参数很多。如果在GPU上进行，我想你可能需要，呃，GPU有多大？GPU的内存少于100GB。所以你可能至少需要10个GPU。

**乔尔·赫斯尼斯：**
我们实际上已经估算过了，1750亿参数的GPT-3在他们最初进行并行化时大约使用了50个GPU。我们最近听说过的万亿参数模型，仅仅是模型的一个实例就需要大约280个GPU。

**主持人：**
本质上只是为了将其加载到内存中。没错，是的。对于你们来说，你们有一个芯片，Cerebras芯片，并且在这个芯片上拥有足够的显存或通用内存来容纳万亿个参数。其中一个节点实际上有多少内存？

**乔尔·赫斯尼斯：**
是的。晶圆本身具有40GB的SRAM，所以它甚至不是DRAM，更不用说芯片外的内存了。它有点像GPU中的缓存，非常靠近核心，我们可以以极低的延迟访问它们，比如几个周期内。我们可以用各种不同的方式使用这种内存。所以之前，公司刚起步的时候，我们把它看作是一种FPGA，你可以在晶圆上映射出整个算法，比如整个模型，然后通过它流水线化你的激活值。我们仍然支持这种架构，这实际上是我应该回头再谈的另一个令人兴奋的话题。但我们意识到，为了进行大规模训练，我们需要利用系统的吞吐量能力。因此，我们决定改变策略：让我们把权重从晶圆上移开，并将它们流入以进行类似于GPU上进行的时间复用内核式执行。每个内核一次在一个晶圆上运行，然后你将它运行在一个驻留在该内存中的激活值集合上。

**主持人：** 你基本上是将计算转移到数据上，而不是将数据转移到计算上，是吗？或者你这是什么意思？

**乔尔·赫斯尼斯：**
差不多是这样。参数是每次训练步骤都会发生变化的东西，激活值每层都会变化，所以它们是频繁变化的，我们频繁地重复使用它们。这些是我们希望靠近核心处理的。所以我们将权重从外部移入，因为它更新频率较低。

**主持人：** 而激活值一直在变化。

**乔尔·赫斯尼斯：**
是的。所以在我们的流水线执行模式中，当我们那样进行训练时，权重驻留在晶圆上，因此它们占据了大量的存储空间。我们实际上受到限制，模型大小受到限制。然后我们还必须存储用于训练反向传播阶段的激活值。基于此观察，我们发布了一个新版本。就在三个月前，我们发布了我们的推理解决方案，它本质上是使用我们之前的旧流水线执行方式，在晶圆上布局模型并通过它流水线化令牌。因此，我们可以以大约每秒2100个令牌的速度进行Llama
3系列700亿参数模型的推理。这目前比任何其他硬件（如Groq或Sambanova）快大约四倍，并且比当前的GPU快大约15到20倍。

**主持人：**
这速度相当快。是的。所以你在Die上只有有限数量的SRAM，我想。如果模型参数都适合在那里，那么你就可以通过将它们全部放在那里来进行解码。我想这有点接近Groq所做的。但你也有大量的片外内存，可以用作完整的权重集。显然，你可以将其中一些缓存到本地激活中，但我猜完整的权重集将位于片外内存中。就你能使用的片外内存量而言，你是否基本上是无限的？

**乔尔·赫斯尼斯：**
基本上是的。我们最近演示的万亿参数模型使用了我们称之为内存X节点的一组服务器，这些基本上是大型DRAM存储系统。然后我们可以将它们堆叠起来；我们可以跨它们进行并行化，将数据馈送到盒子中。这组支持节点本质上是无限可扩展的。

虽然我们最近刚刚演示了一个万亿参数模型，但实际上在一年前半就已经在内部演示了一个万亿参数模型。如果有人想训练如此大规模的模型，我们现在有能力达到大约24万亿参数。

所以我们开始研究专家混合模型。我们在最近的版本中支持它，并且我们预计它们的规模将继续增长，因为额外的参数可以提高模型的质量。我们预计人们会训练比我们从OpenAI信息中听说或解码的更大的模型。

**主持人：** 所以你们是GPU或ML硬件的VLSI，我想。是的，对我们来说，没有哪个数字太大。这很有趣。在Baidu
Labs，你基本上扮演着风险投资家的角色，审视并思考：嗯，我们应该投资这里吗？我们应该投资那里吗？那么你当时是否认为Cerebras拥有最佳方案？或者你认为所有方法都合理，你最终只是与Cerebras合作？

**乔尔·赫斯尼斯：**
我认为Cerebras是最佳选择。我想我当时可能有一些不受欢迎的观点。许多硬件公司最初的目标是推理，而我认为那不一定是正确的解决方案。首先针对推理会简化问题，但可能会限制未来几代硬件的发展。

相反，Cerebras专注于训练。如果你要进行训练，你还必须考虑推理方面，但你还必须处理其他所有事情：反向传播、优化器步骤和权重更新。我很欣赏他们目标远大，这是值得称道的。他们有很多选择，他们专注于训练为他们提供了坚实的基础。

我们刚刚发布的推理解决方案正是这种方法的一个实例。三年半以来，我们只专注于训练。大约一年前，我们决定重新审视推理作为我们可以提供的潜在解决方案。果然，在过去的一年中，我们能够找到一个强大有效的解决方案。

**主持人：** 非常好。那么在你们现有的文本之上进行操作很容易吗？

**乔尔·赫斯尼斯：**
是的。实际上，这很有趣，Cerebras有两个独立的团队负责堆栈。我们有一个专注于机器学习的堆栈，这是大多数人使用的堆栈，我们投入了大量的精力。我们还有一个高性能计算堆栈。该堆栈专注于能够构建内核，因此您可以运行特定类型的低级计算。

构建该堆栈的团队实际上创建了一种领域专用语言，我们正在努力使其更通用。但是这种领域专用语言对于生成内核非常有用，如果您想这样做，可以将其添加到我们的PyTorch堆栈中。

这是这个推理解决方案的关键驱动力：事实上，我们拥有可以编写内核简化版本的工具，并使其编译。因此，我们能够非常快速地为这些大型语言模型在推理时生成大多数内核。

它也具有很强的适应性。因此，我们正在将其扩展到不同类型的模型，以便能够在硬件上运行这些模型。

**主持人：** 所以这些内核听起来很像CUDA内核的直接比较，对吧？就像你编写一些优化的代码，然后你可以将其切片到你的……

**乔尔·赫斯尼斯：** 是的，抽象级别非常相似。

**主持人：**
是的，很有趣。也许现在是时候过渡到第二部分，即硬件了。我认为Cerebras在你们生产最大的晶圆或最大的单芯片时登上了新闻头条。这些芯片有多大？

**乔尔·赫斯尼斯：** 是的，**芯片是一个完整的300毫米晶圆**
。这是我们可以从中切割出的最大正方形。事实上，角实际上不是正方形的；它们在角落使用晶圆的边缘。本质上，晶圆的设计是一个大型简单的核心网格。所以这是一个非常简单的逻辑架构。

您可以将其视为每个单独的核心都类似于GPU中的CUDA核心。它具有窄向量宽度并执行标量向量乘法。当我们将诸如矩阵乘法之类的运算映射到它时，它本质上只是一个大型的收缩阵列，我们在其中通过另一个张量流水线化权重或激活值，并且我们就是这样进行计算的。

我们CS3代的总核心数为一百万。将其与目前AMD或NVIDIA硬件中数万数量级的核心进行比较。这就是我们获得16到20倍性能差异的地方。

在一个晶圆中，在一个盒子里，我们的性能相当于你可能拥有的两个NVIDIA DGX节点盒子的性能，每个盒子有八个GPU。

**主持人：**
尽可能多地为节点配备GPU，即使这样，你也仍然需要两个才能获得大致较高的性能。我知道，你提到生产晶圆时，它是圆形的。所以你们会在其中放入最大的正方形。这是为了简化操作吗？或者为什么你们不使用……

**乔尔·赫斯尼斯：**
是的，主要为了简化。设计的一个有趣之处在于，它旨在使其看起来像一个逻辑阵列。对于程序员来说，我们希望它是一个非常简单的结构。所以正方形很自然。它并非完全是正方形。二维情况下，它并不完全是正方形，这意味着一个维度上的核心数量与另一个维度上的核心数量不同。

但从逻辑上讲，我们将其设置为向用户呈现为规则的核心网格。我想，你感兴趣的问题之一是可靠性。制造这样的晶圆时，必须处理制造缺陷。在制造过程中，我们确实会有一些核心性能不足或无法工作。

因此，我们必须将其关闭，并绕过它们。我们有一个冗余流程，有助于修复问题，从而获得呈现给程序员的逻辑核心集。

**主持人：**
完美的规则网格。哦，所以即使其中有一些失效的核心，程序员仍然看到的是精确的工作核心网格？是的，没错。这非常有用。这就像一个隐藏了失效区域的SSD。

**乔尔·赫斯尼斯：** 与SSD和DRAM非常相似，它们会加入冗余行并将其映射出来。

**主持人：**
是的。这也让我想起，生产晶圆时，通常会有相当多的缺陷。通常情况下，我不知道，英特尔可能从单个晶圆中获得40个或更多芯片。如果单个芯片中的缺陷过多，他们就会将其丢弃。这就是我们所说的良率；它是指实际可用的芯片数量。许多公司，如果存在单个缺陷，就会将其丢弃。

一些公司试图做得更好。例如，当IBM制造用于PlayStation和Game
Boy的Cell处理器时，他们采用了不同的方法。Cell处理器应该有九个核心执行辅助任务。如果所有九个核心都工作正常，他们会将其放入PlayStation。如果八个或七个核心工作正常，他们会将其放入Game
Boy，因为它不需要那么多核心。如果工作核心更少，他们就会将其丢弃。

但是，当整个晶圆成为一个芯片时，你们的任务就困难得多。你们是否有这样的阈值：缺陷过多，无法使用？或者当必须使用整个晶圆时，你们的实际良率是多少？

**乔尔·赫斯尼斯：**
是的，良率对我们来说非常重要。主要的数学统计挑战在于，缺陷可能是独立的，这意味着一定数量缺陷的概率是，你知道的，1减去缺陷概率的某个次方，这个次方就是核心数量或其他什么。这个核心数量的幂次对我们来说非常困难。

因此，我们确保拥有足够的冗余，以便在统计上能够覆盖我们在标准流程中看到的全部缺陷。从本质上讲，我们实现了晶圆的100%良率。但是，我们确实有一些晶圆最终略微超过了阈值，有时很难展现完整的逻辑网格。

因此，这些部件最终成为我们内部使用的工程组件，我们以不同的方式限制网格。这种方法非常灵活；我们可以更改标准结构（我们称之为标准结构），这是可以在设备上运行的逻辑图像。

**主持人：**
这几乎就像拥有百万个芯片而不是一个芯片。你只需要查看其中有多少个芯片实际工作。然后，你构建一个虚拟层或网络，指明如何访问实际工作的那些芯片。这种映射，虚拟映射是如何实现的？这完全在硬件中实现吗？还是你们会向其刷写一些固件？或者这是如何工作的？

**乔尔·赫斯尼斯：** 是的，它本质上是在固件中实现的。实际上，非常酷的一点是，当我们在系统上进行烧机测试（burn
in）时，当我们启动它们时，我们会将它们放入一个集群中运行。我的团队就在那里运行很多工作负载。有时，你会发现不同核心的可靠性在系统的早期使用中会发生变化，不同的热应力等等。

**主持人：** 你尝试一件事，核心工作正常；你尝试另一件事，核心又不工作了？

**乔尔·赫斯尼斯：**
是的。我的意思是，我们正在将最大的工作负载投入到这些东西中，我们本质上是在对它们进行烧机测试，我们是在尽早对它们进行压力测试，以便我们可以查看在交付给客户之前是否需要修复问题。每当我们看到一些问题时，例如晶圆出现停滞，那么停滞很可能是由于数据包丢失或位翻转导致控制寄存器不正确造成的。我们有工具可以扫描和修复这些问题，然后我们可以将系统重新放入，并映射出某些其他核心。实际上，我们能够多么快速地将它们重新纳入循环并重新运行，这令人惊讶。一旦我们完成了这个过程，烧机测试测试过程，这些往往非常可靠。

**主持人：**
也是。这让我想起，我买了一款非常早期的Ryzen处理器，例如1800X，它们很容易出现段错误，如果运行某些工作负载，你就会得到段错误，就像在用户空间中一样。我认为它只会影响用户空间，但尽管如此，它仍然是一个相当严重的错误。我的处理器受到了影响，所以我不得不运行一些测试，然后寄回，然后得到一个新的芯片，并将其安装进去，那个芯片就没事了。但让我印象深刻的一点是，该系统长期以来一直非常不稳定且存在错误。我认为是内存不稳定。我完全不知道，对吧？所以我觉得，尤其是在CPU中，我的意思是，你们有更简单的核心，但在CPU中，例如一个缺陷可能会以非常奇怪的方式表现出来，对吧？所以，你们是否已经规划出了所有不同的方式，也就是说，如果芯片的这部分失效，我们应该预期会出现这种情况；如果芯片的这部分失效，我们应该预期会出现那种情况。

**乔尔·赫斯尼斯：**
是的，我们有一个庞大的分类法。我们有一个专门关注检测和修复这些问题的团队，因为这些问题非常难以调试。我认为，我会说这相当于系统的收敛问题。例如，如果你试图训练一些非常大的东西，然后模型变得不稳定并在训练中途崩溃，你该如何调试？好吧，在你检查它之前，你是否必须再次花费所有计算资源才能到达同一点？这是一个棘手的问题，硬件也面临同样的挑战，因此，如果你在训练中看到一个NaN，并且梯度范数看起来不错，其他方面也很好，那可能是系统问题，因此我们必须拥有不同故障模式的分类法，然后我们有一堆不同的策略和工具来以不同的方式调试这些问题，你认为……

**主持人：**
你们生产的芯片是否具有防范未来各种故障的能力？例如，我设想一下，如果你们的硬件运行五年后，有两个核心停止工作，那么你们会如何处理？你们会重新进行分析，然后将这些核心下线继续运行吗？或者，如果发生故障，更有可能是一种全有或全无的情况，例如芯片的一些核心电路损坏，导致芯片完全无法启动？

**乔尔·赫斯尼斯：** 是的，这是一个有点棘手的问题，部分原因是Cerebras面世时间不长。因此，我们使用的系统的总量和总时间，还需要观察才能得出结论。

我们很幸运，**我认为我们的CS2代晶圆运行状况良好。我们有两个大型的CS2集群，它们实际上是我们最可靠的系统。它们已经全负荷运行了两年半**
，在这两年半的时间里一直保持着充分的利用率。所以它们的运行情况非常好。

关于未来防护能力，是的，我会说修复晶圆并仍然能够呈现逻辑核心网格的能力非常宝贵。因此，如果某些核心看起来处于边缘状态，我们不会让它继续运行，因为它可能不可靠，而是直接将其映射出去。

所以，我认为这应该有助于延长单个系统的寿命。但是，我认为还需要进一步探索是否存在我们认为会损坏整个系统的严重故障。到目前为止，情况还不错。

**主持人：**
非常好。这真的很酷，因为服务器硬件通常设计为运行10年，对吧？它们的设计标准很高，但如果数据中心有足够的服务器，仍然会经常出现故障。大多数故障都需要更换服务器，这对环境不好，而且也不好，因为你必须不断依靠供应链来提供这些旧零件，或者你进行部分升级，但现在什么都不匹配了等等。所以我几乎认为，这真的很酷，因为你们有可能正在构建一个系统，即使它不再是最快的系统，也能在10多年后继续运行和进行计算。所以我们对此充满希望。

**乔尔·赫斯尼斯：** 是的，就在几年前，AWS中仍然有NVIDIA M80 GPU，它们运行得非常可靠。是的，我们希望我们的产品也能拥有这样的寿命。

**主持人：** 是的，这非常酷。我的意思是，这么大的芯片会产生大量的热量。你提到你们有数据中心，这意味着不止一个这样的芯片。那么你们是如何保持其冷却的呢？

**乔尔·赫斯尼斯：**
这是一个很好的问题。我们的晶圆经过特殊设计。如果你看到晶圆——我希望这里有演示——晶圆上有一些孔，这些孔专门设计用于帮助我们将晶圆与背面的电源交付和正面的冷却系统连接起来。

在正面，与晶圆连接的冷却板实际上连接到我们称之为“发动机块”的部件上，该部件循环水流。在模块内部，我们循环水流以将热量散发到热交换器中。这些热交换器最终可以使用空气，或者可以将水泵送到单独的热交换设施。

也许有趣的是，我认为几年前进行晶圆级研究的人认为，整个晶圆的功率太大了。当时的机架功率密度太高了。但我们设计的很酷的一点是，我们可以处理这种功率密度。它实际上与NVIDIA的H100
GPU的功率密度非常相似。

它们的DGX盒子非常令人印象深刻。所以很酷的是，我们的完整设备大约是15千瓦，在机架中的占用空间与几个NVIDIA的DGX盒子差不多。当你开始将每个700瓦的H100
GPU（一个盒子中8个），然后将几个盒子放在一起时，你会发现功率密度与GPU非常相似。

**主持人：**
我实际上在这次会议上与其他人交谈过，他们说一个装有8个H100的盒子是11千瓦。如果你使用即将推出的NVIDIA芯片B200（Blackwell），它将从11千瓦增加到14千瓦，这有点太多了。所以你过去可以将四个H100节点（每个节点有8个GPU）放入一个机架中，这符合许多标准数据中心的电源选项。但是现在你实际上必须这样做，因为它们正好处于那个极限。所以这很有趣。但我想，如果你们使用的是15千瓦，你们甚至可以在那些电源受限的机架中放入两个。当然，每个这样的机架都比一个装有B200的节点强大得多。

**乔尔·赫斯尼斯：**
是的，所以性能密度最终与H100和B100非常相似。在一个机架内部，我们有完全相同的限制。不同的数据中心对每个机架的电源限制不同，所以我们必须适应这一点。所以在某些情况下，我们在一个机架中放置一个盒子。最近，我们已经能够开始堆叠并在每个机架中放置两个盒子。所以它与NVIDIA面临的约束非常相似。

**主持人：** 是的。我认为主要区别在于，你们将处理晶圆缺陷的大部分复杂性从硬件转移了出去，**在传统方法中，他们必须丢弃大量的芯片**
。我不知道Nvidia的良率是多少，但他们肯定要丢弃一些芯片。你们将这种复杂性转移到了固件/软件中，这很难检测到，因为你们必须寻找所有这些故障。但这非常灵活，对吧？就像你们可以这样说，“我们现在就要把它关闭”，硬件就能继续工作，所以你们的整体良率实际上是100%，这真的很酷。所以，如果你考虑到生产两个Cerebras芯片节点或两个装满Blackwell芯片的节点所使用的资源，我猜想你们能够更好地利用供应链、更好地利用硅片等等。我的意思是，你们承担了更多复杂性，但这带来了很多优势。

**乔尔·赫斯尼斯：** 它通过简化部分供应链来发挥优势。我认为组件更少了。我们可以更容易地逐个组件地处理可靠性问题。因此，组装的可靠性相当或更好。

**主持人：**
也是。好的，让我们继续讨论第三部分，即访问硬件。具体来说，Cerebras的客户如何访问硬件？你们生产芯片，你们直接销售它们吗？你们销售云访问权限吗？它是如何工作的？

**乔尔·赫斯尼斯：** 是的，我们有四种不同的交互方式。我们确实销售硬件。我们现在一直在进行部署。只有少数组织有能力进行这种部署。

**主持人：** 如果可以的话，一个Cerebras节点的大概价格是多少？我可能需要和我们公司的人聊聊。我猜想是几百万美元。

**乔尔·赫斯尼斯：**
我们的目标是价格性能与GPU产品相当。是的，我们销售硬件。我们内部也有自己的云。因此，您可以与我们签订合同以使用我们的云。这有点像预留实例设置。

我们还有几个不同的供应商。如果您需要HIPAA合规性或其他什么，我们有供应商可以帮助您安全存储数据等等。我们提供白手套服务，我们实际上会与客户一起进行一些机器学习工作。目前，这是我们大部分客户所在的地方。

他们带着机器学习专业知识来找我们，要么他们不想处理硬件的复杂性，要么他们根本没有人员。他们没有能力自己做。所以他们来找我们，我们帮助他们在硬件上直接运行。我们可以协助机器学习服务。

最近，我们推出了我提到的这个推理解决方案。我们刚刚发布了一个API，您可以将请求发送到我们的系统。我们的推理基础设施也设置在我们的云中。它的运作方式与OpenAI或Anthropic的API非常相似。

**主持人：**
Groq也有类似的东西，对吧？你只需要发送数据，在云端发送查询，然后获取推理结果。没错，就是这样。对于你们的推理API，你们只使用开源模型吗？或者公司可以提供自己的模型？我想在这种情况下，他们可能需要购买硬件。

**乔尔·赫斯尼斯：**
我们两种方式都做。这还是最新的进展。我们正在与少数不同的客户洽谈。我们现有的开放式API使用的是开源模型。所以我们支持所有LLAMA
3模型。我们已经引入了3.3版本，这个版本是上周刚发布的。我们也正在与一些大型组织洽谈，他们需要更大的处理量和更低的延迟。在这些情况下，我预计我们也会部署自定义模型。我们有能力构建对自定义模型的支持。所以，如果你有与LLAMA不同的特定层或其他内容，我们可以做到。

**主持人：**
很有意思。你们所有的云系统都在美国吗？还是在其他地方也有一些？我知道例如在欧洲，你们需要遵守GDPR和数据治理要求，在加拿大也是如此。那么，对于你们来说，情况如何？

**乔尔·赫斯尼斯：**
是的，为了在某个地区提供云服务，我们需要做的事情我们都做了。我们大部分的云服务都位于美国，但我们在欧洲也有一些不同的云服务可用。我们有一个绿色云，一个基于HIPAA合规性的云。我们还直接与一些国家实验室和大学组织合作，这些组织拥有数据中心。因此，学术界可以申请计算时间资助来访问我们的硬件。我们在世界各地都有这些分布。听起来范围相当广，实际上。这很酷。我们尽量确保能够服务于尽可能多的世界各地地区。是的。同时遵守任何安全要求。

**主持人：**
是的。那么，当客户访问你们的硬件时，他们是否必须使用特殊的编译器或Python包，或者……因为我假设人们希望使用他们熟悉的Python代码，而这正是如今所有机器学习人员喜欢使用的。那么，这如何转换为Cerebus系统？

**乔尔·赫斯尼斯：**
当然，是的。我们的堆栈支持各种框架，PyTorch是最常用的一个。大多数人都在使用我们的PyTorch框架。前端是标准的PyTorch，所以你在Hugging
Face上看到的很多模型可以直接移植到我们的架构中。我们已经设置好，你可以通过pip安装一个针对我们后端的包，所以这有点像用PyTorch针对另一个加速器后端。它只是一个pip安装的包。

**主持人：** 所以你们将需要的任何自定义代码合并到PyTorch中，以便它默认包含在其中了吗？

**乔尔·赫斯尼斯：**
文件夹？我们还没有将其包含在标准PyTorch中。我认为有些事情可能不是PyTorch特有的。例如，PyTorch有它的前端，也就是模型，但它也有内部的中间表示，也就是A10张量库。我们基本上获取围绕A10级别的中间表示，并将其转换为可以降低到我们硬件的东西。目前，我们完全支持将模型编译到硬件上。编译可以提供最佳性能，我们正在开发基础设施，这将使我们能够进行即时执行。所以大多数使用PyTorch的人习惯于能够实际调用每个操作，然后检查内核的结果。我们对此有一些初步的支持，并计划更全面地构建它。

**乔尔·赫斯尼斯：**
堆栈来自框架。我们经历了几个不同的中间表示，对于熟悉编译器的人来说，你将计算的不同方面绑定到硬件的位。所以我们的堆栈在不同的级别处理绑定不同的部分。

**主持人：** 所以你们提前分配，比如这个操作或其他操作将在这个特定的逻辑内核上运行？

**乔尔·赫斯尼斯：**
是的，没错。我明白了。所以是这样的事情。它在一个特定的设备上运行。它以特定的精度运行，等等。所有这些都需要解决。内存是如何布局的？诸如此类的事情。

我还要提到，我们最近一直在测试直接从Hugging Face移植模型。所以基本上我们所做的是建立一个管道，允许我们从Hugging
Face中的模型中提取不同的模块，然后独立测试这些模块的可编译性。我们正在构建一个相当全面的功能来支持Hugging
Face模型。现在我们正在关注大约1400个存储库。所以我很高兴我们将能够支持你在那里几乎拥有的一切。

**主持人：** 这很酷。与ML生态系统交互很难，对吧？因为你必须是，我们实现了CUDA，或者我们重新实现了PyTorch，诸如此类。没错。

**乔尔·赫斯尼斯：**
是的，所以每当我们说我们支持PyTorch时，它都是某个受约束的子集，我们正在积极地构建通向完整的PyTorch集合。但是完整的PyTorch大约有2000或2200个操作。是的。很多这些操作都是针对特定硬件的。它们执行操作融合等操作，而这些操作在我们的硬件上可能不是必需的。所以我们必须仔细决定随着时间的推移要构建哪些内容。

**主持人：**
所以，在芯片内部，听起来你们在不同的内核之间有一种网络，比如一种互连，对吧？所以它们可以互相发送数据包等等。我还认为，如果你有多个节点，你必须拥有某种网络，以太网、InfiniBand，我不知道，它们之间必须能够足够快速地通信。你们在那边有什么特殊的技术，还是只是使用现成的技术？

**乔尔·赫斯尼斯：**
我们目前使用现成的以太网。完整的系统设备架构涉及三种不同类型的节点。所以这是我们的加速器节点，它包含晶圆，但它连接到一组称为MemoryX节点的节点。这些节点用于存储权重。然后是一组我们称为SwarmX的节点，这些节点是执行盒子内MemoryX节点之间以及盒子之间的通信的节点。这些本质上是网络节点。

**主持人：**
好的，所以你们有相当多不同类型的节点。显然有一些带有Cerebras处理器，还有一些只是，有一些存储系统，我想它们有很多内存。然后是网络类型的节点，这些节点可能是标准的路由器和交换机等，以确保一切都可以通信。

**乔尔·赫斯尼斯：** 好的，是的。所以，当我们训练模型时，模型会驻留在 MemoryX 节点中。我们使用权重流式架构，有点像参数服务器，这些
MemoryX 节点会通过我们的 SwarmX 节点将模型发送到各个计算单元（boxes）。我们有一个在 SwarmX
上虚拟化的软件架构，它可以将权重广播到各个计算单元。然后，它还会使用归约树来减少梯度，以便在最后进行优化器步骤。我们还在扩展通信原语，也就是你可以用来在设备之间进行交互的集体通信操作。因此，很快就会有一些有趣的机会能够以不同的方式使用这个基础设施。

**主持人：** 所以，你们不仅拥有 Cerebras
芯片，还拥有一个实际的分布式系统，其中包含各种专门的节点等等。这让人想起会议上许多人一直在讨论的机器学习硬件协同设计。就像你使机器学习算法更专业化一样，例如使用
2 位量化而不是 4 位或 8
位计算等等。你知道，硬件必须改变。而你们拥有的看起来是一个非常好的未来演进的基础，因为它是一个分布式系统。你们已经进行了专门化设计，例如，这是内存所在的位置，这是处理器所在的位置等等，听起来非常酷。

**乔尔·赫斯尼斯：**
当然，我们的目标是实现相当通用的完整功能。我认为我们需要一些时间才能找出在那里有趣的负载。但就目前而言，深度学习是一个很好的目标应用，并且是我们在做的更通用事情的很好的代表。

也许还有一个相关方面。我提到我们还有一个团队专注于高性能计算工作。他们关注的工作负载非常有趣，例如基于规则代码（如模板操作）的分子动力学或偏微分方程求解器，这些求解器需要与邻居通信并进行更新。

这实际上是我们深度学习权重流式矩阵乘法架构最初的灵感来源。但由于晶圆非常大，并且每个晶圆的内存容量很大，这对于模板代码来说非常棒，因为模板代码计算密集，而在具有分布式内存的架构上很难做到这一点。

因此，你必须访问 DRAM HBM 内存才能获取部分模板数据。几周前我们在超级计算大会上宣布的那些应用，其运行速度比当代硬件快 100 倍。

**主持人：** 好吧，感谢你告诉我们所有关于 Cerebras 的信息。这非常有趣。你还有什么想让我们的观众了解的吗？

**乔尔·赫斯尼斯：**
当然，是的。我认为现在是从事深度学习、大规模训练和计算的绝佳时机。我认为推理很快就会变得非常重要。我提到了我们的推理解决方案已经可用。也许我应该补充一点，如果学术界想在我们推理解决方案上进行一些工作，我们可以提供资助。它非常快，所以如果你正在处理具有实时要求的工作负载，或者如果你正在进行推理时间计算，请务必让我们知道。我们可以尝试为你找到一些计算资源。我们认为推理时间计算很快就会变得非常重要。令人兴奋的是，还有一个新的扩展定律，它只是增加了推理时间的计算量，你可以获得更好的质量。这与训练时间的扩展如何相关将非常有趣。这就是我们今天的现状。

**主持人：** 我认为所有这些都存在大量的并行发展，因为像 OpenAI 的 O1
模型基本上利用推理时间计算来实现，这与仅将计算用于训练相比，效率提高了大约 10 万倍。你们也在做同样的事情，但这并不是对该模型的回应，因为你们在 O1
模型存在之前就已经在做了，所以你们知道每个人都在考虑推理时间计算选项。这是一个激动人心的时刻。非常感谢你们的到来，也非常感谢大家的观看。

**关注公众号后设🌟标，掌握第一手AI新动态**

##  往期精选

  1. [黄仁勋专访：OpenAI在大模型混战中达到“逃逸速度”](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2650001718&idx=1&sn=f8129a622e7611702be2cb23e8ce9418&chksm=88ba5831bfcdd127d06ce6492c821074407f805407b4182ca900916521cb5a4717f2a3d71ee8&token=1339625777&lang=zh_CN&scene=21#wechat_redirect)
  2. [李飞飞与Justin深度解读空间智能：数字世界需要三维表征，才能与现实世界融合](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2650000659&idx=1&sn=c71fb5b4ef501424dddd5e8b4dd5860e&chksm=88ba4414bfcdcd023c691a1adf75127a9fd883ceb305ca14cf97f719acaf999d40fa72f84bf3&token=1492077842&lang=zh_CN&scene=21#wechat_redirect)
  3. [PayPal创始人彼得·蒂尔：人类科技停滞源于原子方面的进展远慢于比特](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2650000240&idx=1&sn=26af6013981677b1e14137260857a6f0&chksm=88ba4277bfcdcb615d746615c262927bf51c43c920ed93fa36274ef87c6264d6548c84647121&token=106920805&lang=zh_CN&scene=21#wechat_redirect)
  4. [谷歌联合创始人布林：巨头们打造的“上帝模型”几乎可以理解一切](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2649999870&idx=2&sn=0c714d804a72a52e002743d949e1685e&chksm=88ba40f9bfcdc9ef78749718480265922f4fba539abf6c9d62a6cd681f405dee9283d2429f84&token=106920805&lang=zh_CN&scene=21#wechat_redirect)
  5. [马斯克：AI将使商品和服务的成本趋近于零](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2649999870&idx=1&sn=752f000a117a705e77950c82bfc4a004&chksm=88ba40f9bfcdc9ef5a5afe4a3ae73d5247bd54ed525dbdbedee1fcf74a6c082165e664a5c4d0&token=106920805&lang=zh_CN&poc_token=HDp86Waj18SFm2Y-xnv_Vqd_4J6emFoh10eH48wg&scene=21#wechat_redirect)
  6. [Karpathy最新专访：人形机器人、特斯拉、数据墙与合成数据](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2649999613&idx=1&sn=b8bdda7afe4c3ca08e324ac5bbd5a2bd&chksm=88ba41fabfcdc8ec0e21dbf4c7eb4d33252da70f47e1cfc9f5e113717911c417c2aebb3d6180&token=106920805&lang=zh_CN&scene=21#wechat_redirect)

  

