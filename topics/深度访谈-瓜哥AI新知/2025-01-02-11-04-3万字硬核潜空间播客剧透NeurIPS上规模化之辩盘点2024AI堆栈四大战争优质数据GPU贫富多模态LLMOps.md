# 3万字硬核｜潜空间播客剧透NeurIPS上规模化之辩、盘点2024AI堆栈四大战争：优质数据/GPU贫富/多模态/LLMOps

文章作者: 瓜哥AI新知
发布时间: 2025-01-02 11:04
发布地: 浙江
原文链接: http://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2650006320&idx=1&sn=724aef4b6aa66222373060dc2f12a8b7&chksm=88ba6a37bfcde3212446b8d25eb4220c9cea70d61fa25e4b0fd0d9761fd7aff7ff694f611106#rd

封面图链接: https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgB4nosNNdhOB6kbZCAicibDEtyiaUDmgib2Hf8pehpickVul0dJibe8V9TmrVA/300

**👇关注公众号后设🌟标，掌握第一手AI新动态**

**  
**

本文内容整理自**Latant Space 2024年度回顾**
，公开发表于2025年01月01日。原始内容参考：https://www.youtube.com/watch?v=26PVzD707a8

## Latent Space2024年度AI回顾：规模化辩论、AI四大战争及智能体崛起

这段访谈主要讨论了2024年AI领域的主要进展和趋势，总结如下：

**一、AI 工程师的崛起与AI 工程领域的演变：**

  * AI 工程师这一职业迅速崛起，其发展速度超出了预期，反映在播客收听率的增长上，也与整个AI行业的蓬勃发展同步。
  * AI 工程师的定义尚不明确，但这反而促进了有益的讨论和领域的不断细化，其边界仍在探索中。
  * AI 工程师的角色是将最新的研究成果转化为生产应用，需要兼顾研究和工程技能。

**二、2024年AI领域的主要技术进展：**

  * **大型语言模型（LLM）的竞争格局变化：** OpenAI的市场份额下降，Anthropic和Google Gemini崛起，形成三足鼎立之势。Gemini凭借免费层级策略迅速抢占市场。
  * **模型规模的瓶颈与推理范式转变：** 大型预训练模型的扩展遇到瓶颈，转向关注推理时计算（ITC），计算最优训练和推理成为研究重点。
  * **小型模型的崛起：** 大型实验室开发的小型模型在性能和成本方面具有竞争力，与开源社区开发的小模型形成差异化竞争。
  * **多模态模型发展迅速：** 视觉、语音、文本等多模态模型快速发展，Gemini 2.0等模型在多模态能力上取得突破。图像生成模型（如Sora、Veo2、Pika 2.0）和视频生成模型发展迅速，视频与音频同步成为新的前沿方向。
  * **代理（Agent）技术发展：** 代理技术成为关注焦点，但仍面临诸多挑战，例如环境理解、机构知识提取等。
  * **合成数据的重要性提升：** 合成数据在模型训练和评估中发挥越来越重要的作用，但其有效性仍存在争议。
  * **LLM运维（LLMOps）和监管运维（RegOps）发展：** LangChain和Llama Index等LLMOps工具增长迅速，表明该领域的需求日益增长。
  * **代码解释器和代码生成工具发展：** 代码解释器和代码生成工具得到改进，并被集成到各种产品中，例如ChatGPT Canvas。
  * **记忆功能的改进与挑战：** LLM的记忆功能仍在发展中，目前多为显式记忆，缺乏隐式记忆和偏好提取能力。

**三、其他重要观察：**

  * **数据质量之战：** 真实数据与合成数据之间的争议持续存在，合成数据在模型训练中的应用不断扩大。
  * **GPU竞争格局：** GPU超级富豪继续占据优势，而GPU中产阶级和贫乏阶层面临挑战，但一些GPU贫乏的公司通过云服务或模型包装等方式取得成功。
  * **基准测试的演变：** 新的基准测试不断涌现，反映了AI领域研究方向的变化。
  * **AI安全问题日益突出：** 大型实验室面临安全风险，数据泄露和刺探活动等问题需要重视。
  * **AI的商业化和投资：** AI领域的融资活动活跃，大型公司和初创公司积极寻求商业化途径。
  * **2025年的预测：** 智能体技术将迎来快速发展，并将在生产环境中得到广泛应用；AI将改变不同职业的技能下限。

## 内容节选

**主持人 Alessio：** 大家好。欢迎来到 Latent Space 播客。我是 Alessio，Decibel Partners
的合伙人兼首席技术官，今天我的联合主持人SWYX 也加入了，这已经是我们第 100 次合作了。耶！

看起来，即使我们不做太多事，我们仍然在不断增长，只是因为有更多的 AI 工程师了。你预料到这种增长了吗？还是你认为 AI 工程师的发展会需要更长的时间？

**主持人SWYX：** 你知道，今天每个人都在谈论它。是啊，我们已经成功的标志是，Gartner 现在把它放在炒作周期的顶峰。所以 Gartner 称
AI
工程达到了顶峰。我没想到会达到这种程度。当我提出这个概念时，我知道我是正确的，因为我为此做了两个月的工作。但我不知道，你知道，它会发生得这么快。当然，我也有可能错了。但我认为大多数人都接受了这个概念。

Hacker News 讨厌它，这是一个好兆头。但是，已经有足够多的人定义了它。你知道，GitHub 在推出 GitHub Models（也就是
Hugging Face 的克隆版）时，在最显眼的位置用大字写着 AI
工程师。所以我认为它已经作为一个有意义且有用的定义出现了。我认为人们正在试图弄清楚它的边界在哪里。

我认为这正是六月份世界博览会幕后发生的那场“闹剧”的原因。因为我认为人们对 ML 工程的终点和 AI
工程的起点存在很多怀疑或疑问。这是一场有益的辩论。在某种意义上，我其实也预料到了这一点。

所以我故意没有给出明确的定义，因为大多数成功的定义都是必然不明确的。有不同的观点实际上是有益的。你不需要明确一切。

**主持人 Alessio：** 明白了。是的，我当时在 AWS ReInvent 大会，参加 AI 工程相关讲座的队伍，也就是应用 AI
等等的，排队的人数有几百人。我认为这在某种程度上成就了人们，这就像你之前说的那样。就像，嘿，你其实不需要博士学位，直接用模型就行了。然后，也许我们可以谈谈你作为工程师可能会有的盲点，就像我们在
Substack 上发布的关于 ALEA 的文章中讨论的那样。但总而言之，这两年真是太疯狂了。

**主持人SWYX：** 是的，你知道，我试图把会议看作是像 NeurIPS 那样的会议，我认为那里有 16,000、17,000 人。我们在那里举办的
Latent Space 现场活动有 950 人报名。我认为 AI 世界，或者说 ML 世界仍然以研究为主。这本该如此，因为 ML 仍处于研究阶段。

但是，随着我们将整个领域推向生产，我认为这个比例会倒过来，变得更加以工程为主。所以，至少我认为工程应该达到相同的水平，即使它永远不会像研究那样有声望。它永远是低地位的，因为归根结底，你是在操作
API 或其他东西。但你是在封装 GPT。要做好这些事情，需要越来越多的技术和技巧。

我认为这就是我们播客、会议，以及我所做的一切似乎都在关注的焦点。我想我们会在这里讨论适用的趋势。这是一种非常奇怪的混合，既要掌握最新的研究成果，又不是研究人员，然后将研究成果投入生产。

所以人们总是问我，你为什么要关注 NeurIPS？这是一个 ML
研究会议。我说，好吧，是的，我们不会理解所有内容或重现每一篇论文。但这里发现的东西会在某个时候进入生产，你希望如此。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgBTuDCTNLsTNm5rNiaZ8sn9oDLSiaQeo8QVwR6hrAgrYCpMm5ZTpEXeH7w/640?wx_fmt=jpeg&from=appmsg)

实际上，当我与研究人员交谈时，他们非常兴奋，因为他们会说，哦，你们实际上关心这个东西如何投入生产。这正是他们真正想要的。之前的成功衡量标准只是同行评审，对吧？比如在他们的学术评审会议上获得七分和八分。引用率是一个指标，但金钱是更好的指标。对吧。

**主持人 Alessio：** 是的，大约有 2200 人观看了直播，或者类似的数量。是的，直播有 2200 人观看。我尽力主持，但现场的
Jonathan 和 Dylan 比 YouTube 聊天室里更激烈。

**主持人SWYX：** 我创建 Latent Space Live 的初衷，实际上也是为了解决我认为学术会议存在的一些缺陷。这不仅针对
NeurIPS，也针对
ICML、ICLR、NeurIPS。基本上，它们都非常倾向于博士生的就业市场，对吧？就像字面上来说，每个人都去那里宣传他们的研究和技能，以获得工作。然后，所有的公司都会去那里招聘他们。我认为这对个人研究人员来说很棒，但对于想收集信息的人来说，并不好，因为你必须理解字里行间的意思，并且需要引入大量的背景知识，才能理解每一篇论文。

缺少的是我最终做的事情，也就是逐个领域地回顾年度最佳成果并调查该领域。例如，NeurIPS 有一个立场论文轨道，ICML
增加了一个基准和数据集轨道。这些都是解决这个问题的方法。此外，每个会议都有一个最后一天的研讨会，提供更广泛的概述，但他们并不是专门被要求这样做的。组织会议的关键在于找到优秀的演讲者，并给他们正确的提示，他们就会做得很好。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgBxbEHibmVEpwxULeYZ1u0VMQ4HGa0rzSj4SNB1EPakYIE4SUcHJ3u64Q/640?wx_fmt=jpeg&from=appmsg)

我认为 Sarah 在初创公司专题方面做得非常出色。我不能一一列举，但我们做了 2024 年在初创公司、视觉、开放模型、后
Transformer、合成数据、小型模型和智能体方面的最佳成果。此外，我们还与 Nathan Lambert
进行了一次关于推理的快速会议。最后一个备受期待的环节是辩论。这非常尴尬，我非常感谢 John Frankel，他挺身而出挑战 Dylan。Dylan
支持扩大规模，我认为 AI 领域的每个人都普遍支持扩大规模。因此，有必要有人公开断言我们已经撞墙了。

**这场辩论与 Ilya 第二天在台上断言预训练已经撞墙，数据也已经撞墙的说法不谋而合。有趣的是，Jonathan 最终赢得了辩论，然后 Ilya
支持了这一说法；Noam Brown 在最后一天进一步强化了这一说法。最初的共识似乎是我们还没有完成扩展，相信“苦涩的教训”。然而，在会议的连续四天中，像
LSTM 的创始人 Sepp Hochreiter 和 Juergen Schmidhuber
这样的人物也回应了预训练已经撞墙，或者说我们遇到了另一种墙的观点。**

多位专家支持了我们在大型预训练模型的现状中已经触及某种墙的观点。我们需要一种新的方法。人们将这种新方法称为推理时计算，我认为集体的术语确实已经变成了推理时。这是有道理的，因为称其为测试时间带有预训练的偏见，暗示运行推理的唯一原因是为了测试你的模型，但这并非完全如此。我同意
**OpenAI 和更广泛的社区似乎正在采用 ITC 而不是 TTC 这一术语，因为我们现在关注的是推理，包括计算最优化的各个方面。**

我实际上采访了一位评论 Chinchilla 论文的作者。Chinchilla
论文讨论了计算最优训练，但它没有说明它是关于预训练的计算最优训练。一旦你开始关注推理计算最优训练，你就会遇到不同的缩放定律，这是我们去年不了解的定律。

**主持人 Alessio：**
我在想，因为约翰，他也是站在“注意力机制即一切”这一边的。就像他和萨沙争论时那样。所以我很好奇，他一方面不相信扩展规模，另一方面又认为Transformer（架构）可行。我想知道他现在是否仍然……

**主持人SWYX：**
是的，所以，所以，他显然认为一切都是有细微差别的，你知道的，我告诉他这次辩论要扮演一个角色。对。所以他实际上是这样做的。是的。他仍然认为我们可以进一步扩大规模。呃，他只是为了这次辩论而扮演了一个非常投入的角色。所以更要赞扬他，他扮演了一个他自己并不相信的立场，而且还赢得了辩论。

**主持人 Alessio：** 回到正题，迪伦。你能不能快速过一下这些东西，就讲一下重点吗？

**主持人SWYX：**
好的，我们不能过一遍每个人的幻灯片，但我提取了一些我们打算讨论的内容。我们会发布剩下的内容。是的，我们会在这个频道发布2024年这些领域最佳内容。希望大家能从我们演讲者所做的工作中受益。但我认为这些幻灯片很好，而且我一直在寻找人们的年终回顾总结。

这个领域已经发展了很多。我认为2023年LMSYS上的最大ELO值是1200。现在，所有人的ELO值至少都有1275。这包括Gemini、ChatGPT、Grok、O1
AI的超大模型，当然还有Anthropic。这是一场非常非常激烈的竞争。有很多前沿实验室都在竞争，但存在一个明确的零级前沿，然后是一级。就像，我希望我能拥有其他一切。零级竞争非常激烈，实际上现在是Gemini、Anthropic和OpenAI之间的三强争霸。

我想说，人们仍然对XAI抱有一丝希望。由于某些原因，因为他们的API推出非常缓慢，XAI不包括在这些指标中。所以实际上很难把它放进去。作为一个也做图表的人，XAI一直被忽视，因为他们不善于与进行基准测试的人合作。这算是关于为什么XAI总是被忽略的一个小花絮。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgBianicwSdjWiaBpbUZTiaYlyJbggDj5PbcjbxZRQ2rJcCLjNicN9tfJ6qj1A/640?wx_fmt=jpeg&from=appmsg)

另一个是市场份额。这些是莎拉的幻灯片，我们把它放在屏幕上了。它已经从OpenAI一家独大变成了百家争鸣的局面。我们有一些来自Ramp的数据和估计。基本上，是什么呢？GPT
3.5和GPT-4占据了95%的生产流量。我认为如果你把这个和我们在LangChain那一集问哈里森·蔡斯的问题联系起来，那就对了。

然后在今年年中，大约在3月份，Claude 3发布了，而Claude
3.5的Sonnet大约是在6月份。你可以看到市场份额开始非常非常迅速地向Anthropic转移。最近的竞争者是Gemini，如果我向下滚动一点，这甚至是一个更近的数据集。Ram的数据集到2024年9月结束。Gemini基本上在低端市场发起了一场价格战，**Gemini
Flash基本上对个人用户免费。**

我认为人们不了解免费层级。它每天有大约10亿个令牌。除非你试图滥用它，否则你实际上无法用完Gemini的免费层级。他们真的很想让你使用它。他们知道他们排在第三或第四位，这取决于你的计算方式。所以他们先从较低的层级入手，然后可能会在以后进入较高的层级。但是，是的，**根据Open
Router的数据，Gemini Flash现在占其Open Router请求的50%**。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgB2YwxHE7nq005ffA4ozyuHv9K4lHM0hVuYJGSiaib0KajlDYlhuUY9uEA/640?wx_fmt=jpeg&from=appmsg)

显然，这些是小请求。这些是小的、廉价的请求，从数学角度来看，它们会更多。聪明的人显然仍然会使用OpenAI。但这确实是市场的一个非常非常大的转变。基本上，**在2023年到2024年期间，OpenAI的市场份额从95%降到了大约50%到75%之间**
。

**主持人 Alessio：**
是的，我真的很好奇RAMP是如何归因到模型的，如果它是API的话。因为我认为这都是……信用卡消费。好吧，但是信用卡上没有显示。也许当他们做费用报销时，他们会上传PDF。但是，是的，我认为Gemini的情况是合理的。我认为这是我2024年的主要收获之一，那就是最好的小模型公司是大型实验室，这并不是我会认为开源的长尾会是小型模型的情况。

**主持人SWYX：**
是的。我们这里谈论的是不同大小的小模型，对吧？比如，Gemini的小模型是8B，对吧？我们不知道小模型的尺寸是多少，但可能是两位数，也可能是个位数，但很可能是两位数。开源社区已经把重点放在了1到3B的大小上，可能是0.5B。那是Moon
Dream。如果你认为这很小，那就太好了。现在我们对小型模型有一个范围是合理的，比如1到5B。我甚至把它放在高端。所以这也包括Gemini的Gemma，但也包括Apple
Foundation模型，我认为Apple Foundation是3B。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgBCoYf2JGOM7XXMwGe1d352nJBVcN5PQS6SN8DmCAgcZ8EVf8cnicDm5Q/640?wx_fmt=jpeg&from=appmsg)

**主持人 Alessio：**
是的。不，这太好了。我的意思是，在开始的时候，小只意味着便宜。我认为今天，小实际上是一个更加微妙的讨论，人们以前并没有真正进行过。

**主持人SWYX：** 我们可以继续。这是我不太同意莎拉的一个幻灯片。她指的是Scale
Seal排行榜。我认为我在欧洲交谈过的研究人员对此持肯定态度，因为基本上你需要私有测试集来防止污染。

而且Scale是今年为数不多（可能三四个）真正努力制作可信的私有测试集排行榜的人之一。LLAMA
405b的表现比Gemini和GPT-40好，我认为这很好。我认为有一个如此庞大的开源模型在这些指标上表现良好是件好事。

但是任何在生产中使用405b的人都会告诉你，如果你向下滚动一点到人工分析数据，你会发现它的推理速度非常慢，而且非常昂贵。它甚至不能装在一台H100的节点上。Cerebras会很乐意告诉你他们可以在他们超大型芯片上服务4或5B，但如果你需要对它进行任何定制，你仍然会受到限制。

那么4或5B真的那么重要吗？我认为大多数人基本上说他们只使用4或5B作为教师模型来提炼成某种东西。即使是Meta也在这样做。所以当Llama
3.3发布时，他们只发布了70B，因为他们使用4或5B来提炼70B。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgBxDGziadHb2Jia4lenmGdQxEZjghSEl9cicbVcjxun1gVHOlCmGMaMShlw/640?wx_fmt=jpeg&from=appmsg)

我不知道开源是否跟得上。我认为开源工业综合体非常热衷于告诉你差距正在缩小。我不太同意。我认为差距正在扩大。我认为有很多非常聪明的人在努力缩小这个差距，他们应该这样做。我真的希望他们成功。

![](https://mmbiz.qpic.cn/sz_mmbiz_png/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgBB11fKuwxwrMUibobGIRkQ3KZNxQGt997GVhc8Okona8xZBhMAuE7qvA/640?wx_fmt=png&from=appmsg)

但是你不能使用接近100的饱和图表。你看，开源和闭源之间的距离正在缩小。当然，它会缩小，因为你接近100了。这很愚蠢。但是在重要的指标上，开源是否在缩小差距？可能在01的时候不会。

这真的取决于开源人士能否搞清楚他们是否可以匹敌。

**主持人 Alessio：**
我认为推理时间的计算对开源不利，因为你知道，Zuck（扎克伯格）可以在训练时捐赠算力，但他不能在推理时捐赠算力。所以要跟上这个节奏真的很难。这是一个大的商业模式转变。所以我不知道这对
GPU
云意味着什么。我不知道这对超大规模云计算公司意味着什么。但显然，大型实验室有很大的优势，因为你投入的计算资源并不是一个静态的产物。你仍然在做这个，但你投入了大量的计算机信息。

**主持人SWYX：** 是的。我的意思是，**Llama 4 将会是面向推理的** 。我们和 Thomas Shalom
聊过了。感谢他促成了那一集的播出。真的很好，时机也很好。实际上，我在 NeurIPS 上和 AI Meta 的负责人取得了联系，我们将协调一些事情。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgBp9J3ibJLicUxMVTQ0030png83DQoZMRV0N3q3YeqaQuyq8WsQaqBRdwA/640?wx_fmt=jpeg&from=appmsg)

**主持人 Alessio：** 是的，我们的朋友 Clara She 刚刚加入来领导商业代理方面的工作。我相信我们会在新年请她来参加节目。

**主持人SWYX：** 是的。关于商业模式转变，我的评论是，这非常有趣。显然，众所周知，OpenAI 希望通过融资获得超过 66
亿美元的资金。他们的目标是筹集更高的金额，但没有成功。

这基本上意味着，我们没有得到
GPT-5，这反而很方便，因为GPT-5会涉及到更大的预训练过程。相反，**我们正在将固定成本转化为可变成本，对吧？这种成本转移有效地将费用转嫁给了客户**
。在这种模式下更容易获取利润，因为你可以将成本直接归因于使用量；例如，如果你使用得更多，你就会支付更多的成本，我会在其中加入利润。

这种方法可以更好地控制增长利润，并使支出与推理成本相匹配。非常有趣的是，**这种推理范式的变化恰好与预训练的融资环境实际上正在枯竭同时发生。我觉得风投们可能非常了解研究，所以他们会注意到这一点。**

**主持人 Alessio：** 这真的很有意思。是的。我回顾了我们去年的年度总结，当时最重要的事情是 Mixtral
的价格战，你知道吗？而我认为现在几乎无路可走了。比如，Gemini Flash
基本上是免费提供的。所以我认为这对于实验室来说是一个更好的创收方式，并将一部分成本转嫁给用户。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgBibZjIwWYYGgvnIKRh7ouag5Ko3Q6l1XicG8UDmHm1xx2ibLfTsLLzpG3Q/640?wx_fmt=jpeg&from=appmsg)

**主持人SWYX：** 将一部分计算资源转移给客户。是的，我认为你会继续这样下去。

**主持人 Alessio：** 是的，完全同意。我的意思是，明年我要做的第一件事就是注册 Devin，注册 Pro
ChatGPT。只是想尝试一下。我只是想看看每月在人工智能上花费 1000 美元是什么样的体验。

**主持人SWYX：**
是的。我认为如果你的工作至少是人工智能内容创作者、风险投资人或需要掌握最新动态的人，那么你每个月应该已经在这些东西上花费大约一千美元了。而且，显然，花钱容易，使用难。是的，你必须真正使用。好的一点是，实际上
Google 现在让你免费做很多事情。比如，他们刚刚推出的 Deep Research 使用了大量的推理，而且在预览期间是免费的。

**主持人 Alessio：** 是的。所以你应该使用它。他们需要把这个放在林迪（Lindy）上。我最近一直在使用林迪。自从有了 Flow
之后，我就构建了很多东西，因为我喜欢这个新东西。它很不错。我做了一个电话助手。是的，他们刚刚推出了新的语音功能。是的。我认为一旦他们有了高级语音模式的功能，目前它仍然像是语音转文本。你可以感觉到这一点，但它对于预订之类的操作很有用。所以我有一个会议准备工具。这是一个，

**主持人SWYX：**
不错。好的。我觉得我们已经谈了很多内容。是的。你知道，我认为我们会在单独的一集中回顾每个演讲。我不想在这上面花太多时间，但足以说明每个领域都有很大的进展。

我们讨论了视觉。基本上，这都是观众投票选择他们想要的内容。然后我邀请了我能找到的每个领域最好的演讲者，特别是代理领域。我在维也纳 ICA Bell
上与之交谈过的 Graham，目前仍然是第一名。

在 SWEBench 上保持领先地位非常困难。Open Hand 目前在 SWEBench Full
上仍然排名第一，这是最难的一个。他对代理有很多很好的见解，我会为人们重点介绍。

每个人都说 2025 年是代理之年，就像他们去年说的那样。但他对代理中需要解决的八个前沿问题有自己的想法。所以我也将重点介绍他的演讲。

**主持人 Alessio：**
是的，第六个问题，即代理如何更多地了解环境，对我们来说也一直非常有趣，值得思考。因为，是的，你如何将一个代理放入一个企业中，而企业中的大多数东西从未公开过？你知道，很多工具，比如代码库之类的东西。

所以只是索引和 RAG？嗯，是的，但更像是你无法真正 RAG
那些没有文档记录的东西。但人们基于他们一直以来的做法了解它们，你知道吗？所以我认为这几乎像是...
哦，机构知识。是的，一个枯燥的词有点像业务流程提取。这就像，你如何真正理解这些事情是如何完成的？我明白了。

我认为今天，大多数人正在构建的代理擅长遵循指令，但不如从你那里提取指令那么好。是的。所以我认为这将是一个重要的突破。

酷。我认为它很漂亮，我的意思是，我们会把它链接在相关内容中。但我认为主要重点是，你如何使用机器学习来优化系统，而不是仅仅专注于使用机器学习来做其他事情？

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgB6wKw11DDWIYRgTkP0uay09J9hia0MUU7pbQkEzMM6ficXoAWKic1anYvg/640?wx_fmt=jpeg&from=appmsg)

是的，我认为投机解码，我们之前在播客中请到了来自 RWKV 的 Eugene。他在这方面做了很多工作。

**主持人SWYX：** 我会说这是常态。我有点不舒服的是它的成本，因为它每次调用都会使用更多的
GPU。但是，由于每个人都非常渴望快速推理，那么，是的，这很有道理。

**主持人 Alessio：** 的确如此。是的，但我们会把相关的链接发出来，当然还有 Jeff 的。

**主持人SWYX：** 是的，所以 Jeff 的演讲更多的是，它不是专注于 Gemini。我认为人们从我的推文中产生了错误的印象。更多的是关于
Google 如何处理机器学习，并使用机器学习来设计系统，然后系统反馈到机器学习中。

我认为这与 Lubna
关于合成数据的演讲相关联，基本上是人类和人工智能在人工智能研究或人工智能生产中自举的故事。所以她的演讲是关于合成数据的，比如合成数据在 2024
年或在预训练、后训练和评估方面增长了多少。

我认为 Jeff 随后也将其扩展到了芯片，扩展到了芯片设计。他花了很多时间谈论
AlphaChip，而我们大多数听众都说，我们不是搞硬件的，伙计。你们很棒。TPU 很棒。

**主持人 Alessio：** 然后是 Ilya 的演讲？是的。然后我们有一篇与之相关的文章，关于 Ilya 所看到的内容。我不知道我们是否称其为文章。

**主持人SWYX：**
我们现在要讨论这个吗？对我来说，这就像是给潜在空间支持者的一个额外福利，因为我觉得他们一直没什么收获。是的，然后我想要一种更高频率的方式来写类似我一个下午写完的那种东西。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgB0gy2QUico8ibrVFkrxxjsjr1UwEtqYkZsNXSkPQl6K0PEKAHeUXHPQJA/640?wx_fmt=jpeg&from=appmsg)

我觉得我们现在基本上知道伊利亚看到了什么。自从那次“闪烁”事件已经一年了，我们知道他在2014年14岁时看到了什么。我们知道他在2024年看到了什么；我们认为我们知道他在2024年看到了什么。他给出了一些暗示，然后我们对他在2023年看到的东西也有模糊的迹象。

哦，还有2016年。因为和埃隆的诉讼，OpenAI正在公布来自萨姆的邮件，比如他发给希沃恩·齐利斯之类的个人的短信。所以我们有伊利亚的邮件说，这是我们在OpenAI看到的情况，这就是为什么我们需要扩大GPU规模。

我认为在2016年写下这些是非常有先见之明的。基本上，他的见解是，他和格雷格在仍然抱有怀疑的情况下推动了OpenAI的规模扩大。他们就像是...

**主持人 Alessio：**
不，就像我们看到了这里的道路。是的，这很有趣，是的，他们甚至提到，你知道，我们只能在1v1的Dota上训练。我们需要在5v5上训练。

**主持人SWYX：**
是的，至少对我来说，我可以代表我自己说，我没有看到从Dota到我们今天所处位置的道路。我觉得即使你问他们，他们不一定会画出一条直线。

**主持人 Alessio：**
是的，我绝对没有。但我认为这几乎是强化学习的全部想法。我们在内森的播客中也讨论过这个问题。就像，**通过强化学习，你可以在特定的事情上做得非常好，但是你不能真正地进行泛化**
。我认为语言模型正好相反，你要把所有的数据都扔给它们并扩大规模，但是之后你真的需要在一个特定的任务上推动它们。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgBibPN7ibxhWUibVicCGpHHjhibxE7xicibqT3wM7zQLKlK4PkurbyBsNXj6eXA/640?wx_fmt=jpeg&from=appmsg)

我们还会谈论OpenAI的强化微调公告，以及所有这些。但是，是的，我认为规模就是你所需要的全部。这大概就是伊利亚会被记住的原因。为了澄清人们喜欢在推特上发的“预训练结束了”这句话：谈话的重点是每个人都在扩大这些芯片和计算规模，但是第二个要素，也就是数据，并没有以相同的速度增长。这并不一定意味着预训练结束了；更多的是说，让我们走到今天这一步的东西，不会让我们走到那里。

在他的邮件中，他预测了每两年增长10倍左右，我想也许现在你可以再次将芯片增加10倍，但问题仍然是，是否每年增长10倍。我不太清楚；摩尔定律大约是2倍，所以它比那个快得多。我喜欢人工智能的化石燃料的比喻，因为它与一些小的背景token有关。

**OpenAI的强化微调基本上就像不是在数据上进行微调，而是在奖励模型上进行微调。它就像是以任务为导向而不是以数据为导向**
。我认为人们有任务要做，但他们并没有真正的数据。我很想看看这会如何改变微调的人数，因为这是人们会遇到的问题。这就像是，“哦，你可以微调LLaMA”，然后问题是，“好的，我在哪里可以获得微调所需的数据？”

所以我们朝着这个方向前进是很棒的。我真的很喜欢他展示大脑质量和身体质量关系的图表。基本上，哺乳动物的大脑和身体大小呈线性增长，而人类有点打破了这个斜率。这几乎就像也许哺乳动物的斜率代表预训练的斜率，而训练后的斜率就像人类的进化方式。

**主持人SWYX：** 是的，我想知道，我的意思是，我们会在10年后知道，但是我想知道伊利亚的SSI的y轴是什么。我们会尝试让他们来参加。

**主持人 Alessio：**
伊利亚，如果你在听，欢迎你来。是的，然后他谈到了接下来会发生什么，比如，智能体、合成数据、推理、计算机，所有这些就像是...我不认为他在这里透露了任何有价值的信息。还有其他新的代表、亮点，或者...

**主持人SWYX：**
我认为相对来说，有更多的工作。哦，对了，我需要宣传一下，我的朋友Yi制作了这个很不错的，是的，这真的很好。关于2024年所有必读的论文，她称之为必读论文。我在NeurIPS上展示了一些，结果都被拿光了。就像每个人都拿走了，因为人们非常需要一些指导和每篇论文的可视化呈现。所以我认为我们能得到这个真是太好了。我们应该做一本关于潜在空间的书吗？

**主持人 Alessio：** 每年一本。我考虑过。每年一本。咖啡桌书。

**主持人SWYX：**
有一个有趣的，还有一个更普遍的。所以有趣的是这篇关于智能体串通的论文。这是一篇关于隐写术的论文。这是人工智能智能体之间的秘密串通，通过隐写术进行多智能体欺骗。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgBkXPiaO5qXysyJB6MepPx1QliajB5YjzMcic7MV4JMxYIvyR2jhxyCgL2A/640?wx_fmt=jpeg&from=appmsg)

我试图去NeurIPS是为了找到这类论文，因为真正的原因是，今年NeurIPS有一个抽签系统。很多人实际上甚至去了都不买票，因为他们只是去参加场外活动。然后那些去参加的人最终会挤在最受欢迎的论文周围，这些论文你早已知道，并且在你去纽约之前就已经读过了。所以你去那里唯一的理由是和论文作者交谈。但是那里还有大约1万篇其他论文，你知道，这只是人们一年中所做的工作。他们由于这样或那样的原因而没有获得关注。

而这篇论文就是其中之一。它在最后面。这是一篇DeepMind的论文，**实际上侧重于通过隐藏在他们生成的文本中的消息来进行人工智能智能体之间的串通。这就是隐写术**
。一个非常简单的例子是每个单词的第一个字母。如果你把它挑出来，然后解码它，你会看到与你最初读到的不同的消息。

但是我一直强调的是，对于LLM，我们从左到右阅读。LLM可以上下左右，甚至以随机的字符顺序阅读。这对它们来说和对我们来说是一样的。因此，如果我们真的要得到有自我驱动力、隐藏的LLM，它们试图合作来接管地球，这将是它们的方式。它们会将消息散布在我们生成的文本中。他为此开发了一个缩放定律。

他标记了，我现在正在屏幕上展示，这种现象的出现。基本上，例如，对于密码编码，GPT-2、LLAMA-2、Mixtral、GPT-3.5显示为零能力，然后GPT-4突然出现。这正是人们所寻找的杰森·韦（Jason
Wei）类型涌现属性。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgBw1YKVBo4veFvmX5g3RiaU7xoNsUVSWk2n4AbQhhnQJbficd1WRsJxhDw/640?wx_fmt=jpeg&from=appmsg)

我认为这篇论文脱颖而出的原因还在于他为隐写术串通开发了一个基准。他还专注于“谢林点”串通，其特点是非常低的协调性。为了就解码或编码格式达成一致，你确实需要在某种程度上达成一致，但谢林点意味着低或几乎没有协调。

例如，如果我给你的唯一消息是“在纽约见我”，而你不知道在哪里或什么时候，你可能会在中央车站见我。中央车站是一个谢林点，而且大概会在白天。在某种程度上，隐写术的谢林点就像我们谈到的常见的解码方法。

在未来，当我们担心对齐问题时，这将变得很有趣。今天并不有趣，但有趣的是DeepMind已经在考虑这个问题了。

**主持人 Alessio：** 有意思。我觉得欧洲最难的事情之一就是长尾效应。

**主持人SWYX：**
非常长尾。我找到一个定价专家。我打算在播客上介绍他。基本上，这位来自英伟达的专家计算出了语言模型的最佳定价。这基本上是一篇在NeurIPS上的计量经济学论文，而其他所有人都在讨论GPU。

那是一个挺有趣的。我看到的更广泛的焦点是，**NeurIPS上的模型论文已经有点过时了。现在基本上没人再展示模型了。全是关于数据集的。这成了所有研究生都在研究的内容**
。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgBhibQtsxzS74FjFbXw09BjJibV1GibRtqralK9js7avukhRWBgV7ZyxZag/640?wx_fmt=jpeg&from=appmsg)

所以，当时有一个数据集的专场。然后我到处看了看。我就想，你不需要数据集专场，因为每篇论文都是关于数据集的。数据集和基准测试其实是同一事物的两个方面。所以，是的，如果你是一个研究生，你有一块GPU板，你基本上就研究那个。

然后那些大型模型让人们四处走动，挑选他们喜欢的模型，然后在他们的模型中使用它。我觉得这就是它发展的模式。去年，你有像侯天这样的人，他研究了Lava，即在Llama的基础上添加视觉功能。显然，XAI聘请了他，他将视觉功能添加到了Grok中。他是Vision
Grok的人。

今年，我不认为有任何类似的。

**主持人 Alessio：** 最受欢迎的口头报告是什么？去年，我认为Mixed
Monarch是参与人数最多的。是啊。我需要查一下。是啊，我的意思是，如果没什么想法，某种程度上这也是一种答案。但我觉得去年大家对进一步发展模型、不同的架构以及所有这些都非常感兴趣。

**主持人SWYX：**
我会说，我觉得今年的口头报告选择不太好。要么就是我个人看待论文的方式发生了变化，才导致这种感觉。所以，我估计今年最好的两篇数据集论文是DataComp和FineWeb。这两篇实际上是工业界都在使用的论文，但没有被选为口头报告。

我认为DCLM获得了关注，而FineWeb甚至没有得到它应得的关注。所以，只是选择不同而已。一个确实引起很多讨论和争议的事情是调度的作用。这是Meta的无调度优化器论文，由Aaron
DeFazio撰写。

今年在机器学习社区，有很多关于洗发水、肥皂以及其他优化学习率的洗浴用品的讨论。我问过的大型实验室的大多数人说这很可爱，但它并不重要。我不知道，但这确实是一个被讨论并变得非常流行的东西。

**主持人 Alessio：** 用四个词来概括一下人工智能，也许可以快速地概括一下。你想从哪里开始？数据？

![](https://mmbiz.qpic.cn/sz_mmbiz_png/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgBlFgCZj6D68CNfUkJNTM9icOrSbEZUdpQXicGrj5AZw6O6Ly4xlZyQclw/640?wx_fmt=png&from=appmsg)

**主持人SWYX：**
是的。为了提醒大家，这是我们今年早期做的四篇最糟糕的文章之一的总结。左边的交战方是：记者、作家、艺术家，以及基本上任何拥有知识产权的人，比如《纽约时报》、Stack
Overflow、Reddit、Getty、Sarah
Silverman和乔治·R·R·马丁。我认为今年我们可以把斯嘉丽·约翰逊加入到这一阵营中。所以，基本上任何起诉的都在睁眼。

我其实想弄一个所有诉讼的快照。我相信一些律师可以做到。这就是数据质量之战。在右边，我们有合成数据的人。我们讨论了Luminous
Talk，它真正展示了今年合成数据的进展。Scale
AI和合成数据社区之间发生了一些争执，**因为Scale发表了一篇论文，声称合成数据不起作用。不出所料，Scale是非合成数据的主要供应商**
。只有无限制的注释的数据（cage-free annotated data）才有用。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgBCYF7vMsYKsV8Oibr9WibDh16kqTJgdxoqj9xOwnNg5icbibTZiaXhVEyHDw/640?wx_fmt=jpeg&from=appmsg)

我认为那里有一些争论，但我不认为这有多大争议了，至少合成数据，出于Lumina演讲中概述的原因，是有道理的。我不知道你对此有什么看法。

**主持人 Alessio：**
我觉得，再说回到强化微调，我认为这会稍微改变人们对此的看法。现在，**人们主要使用合成数据进行蒸馏，以及从大型模型微调出较小的模型。**

我不太清楚前沿实验室如何使用它，除了苹果也做的改述和网络相关的事情。但，是的，我认为它会很有用。这是否会引导我们走向下一个重大进展，可能还有待确定，你知道的。

我认为人们喜欢谈论数据，合成数据是人们可以积极参与的事情，所以与更抽象的优化器相比，他们对它更有看法。

**主持人SWYX：**
我认为合成数据背后有一个道理。所以今年，我们在一个论文俱乐部中讨论了星系列论文。这包括星、Q星和V星。它基本上可以帮助你合成推理步骤，或者至少从验证器中提取推理步骤。如果你看看OpenAI发布的RFT
API，或者他们宣布的API，他们要求你提交评分器，或者他们从预设的评分器列表中选择。本质上，这感觉就像是为他们创建有效的合成数据以微调推理路径的一种方式。我认为这是另一个它开始变得有意义的角度。

非常有趣的是，行业之间的大部分数据质量之战，比如音乐行业、报纸出版行业或教科书行业，都发生在预训练时代。相比之下，在新的推理时代，似乎没有人对推理有任何问题。特别是因为它都是以数学和科学为导向，涉及非常合理的评分者。我认为下一个更有趣的步骤是推理机制如何推广到STEM之外的领域。

我们已经使用O1来获取人工智能新闻一段时间了。我想说，在总结、创意写作和指令遵循方面，O1被低估了。我们最终取消了O1，但是在开头的歌曲里，我开始使用O1。在写歌词方面，它表现非常好。Noam展示的一个O1
Pro演示涉及写一整段或三段不使用字母A的段落。这展示了它在字符级操作和计数以及指令遵循方面的能力。因此，当我让它押韵并创作歌曲歌词时，它比之前的模型表现更好，这并不奇怪。因此，**我认为O1在创意写作方面被低估了。**

**主持人 Alessio：**
你认为他们在法庭上会持有什么样的理由呢？当他们不给你展示O1的思维轨迹，但他们又想提起诉讼，比如他们因使用其他出版商的数据而被起诉，但另一方面，他们会说，嗯，你不应该使用我的数据来训练你的模型。所以我很好奇想看看这会怎么发展。

**主持人SWYX：**
是啊，我的意思是，我希望有尽可能多的方法来惩罚人，而不用把他们告上法庭。所以任何被发现泄露思维链的人都将被禁止继续使用API。这没关系，没什么大不了的。我觉得这根本不是个问题，因为思维链隐藏得很好。你需要非常非常努力才能让它泄露出来。

而且即使思维链泄露了，你也不知道它是不是真的。所以这方面没什么好担心的。更大的问题实际上是它背后没有那么多的知识产权——我们在开发者日谈到的余弦相似度，可以微调
4o 来打败 O1。Claude Sonnet 到目前为止在编码任务上已经击败了
O1，至少在没有成为推理模型的情况下，没有预览版的情况下就做到了，Gemini Pro 或 Gemini 2.0 也是如此。

所以，推理有多重要？他们推测已经实现的这种专有训练数据有多大的护城河？因为即使是 DeepSeek 也能够做到这一点，而且他们只有两个月的通知时间来做
R1。所以，到底有多少护城河实际上并不清楚。

显然，如果你和 Strawberry
团队谈，他们会说，是啊，我们过去两年都在做这个。所以我们不知道。这会很有趣，因为会有很多人说他们有推理时的计算能力，但实际上没有，因为他们只是有花哨的思维链。

然后还有其他人实际上拥有非常好的思维链，你不会看到他们和 OpenAI 在同一水平上，因为 OpenAI
在建立他们团队的神话方面投入了很多，这是有道理的。真正的答案介于两者之间。

**主持人 Alessio：** 是啊，我认为这有点像是正在发展的主要数据争夺故事。GPU 贫乏与 GPU
富裕。你觉得我们现在处于什么位置？我认为，再次回到小型模型的问题，曾经有一段时间，GPU
贫乏的人有点像反叛派系，致力于开发这些开放、小型且廉价的模型。我认为今天人们已经不太关心 GPU 了。你也可以从 GPU
的价格中看到这一点。你知道，那个市场有点暴跌，因为人们不想成为，他们想摆脱
GPU，他们甚至不想贫乏，他们只是想，你知道，完全没有它们。是啊。你如何看待这场战争的发展？

**主持人SWYX：** 你可以告诉我这个，**但我觉得人们对 GPU 富裕的初创公司的兴趣正在发生变化。以前融资 6000 万美元，然后给 NVIDIA
5000 万美元的融资计划似乎已经消失了。现在没人再推销这个了** 。这绝对是去年这个时候四五家初创公司的确切计划。所以，是的，GPU
富裕的初创公司已经从舞台上消失了。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgBomhbTpQzibNx1U7dFxRDlDqTUDUYSUg0adYBJJVWcQ4MBNq4YDXG3OA/640?wx_fmt=jpeg&from=appmsg)

然而，我认为 GPU 超级富豪，GPU
超高净值个人仍然很强大。我们有利奥波德关于万亿美元集群的文章，虽然我们还没有达到那个水平，但我们有多个实验室正在取得重大进展。例如，XAI 因在大约 12
天内启动了 10 万个 GPU 集群而受到黄仁勋的称赞。同样，Meta 和 OpenAI 以及其他实验室也在取得重大进展。GPU
超级富豪继续向前推进，因为这些资源是必不可少的已经成为一种信仰，即使确切的应用尚不明确。

从 2020 年到 2023 年的时间线来看，已经从扩展大型模型转向探索更多以研究为导向的途径。2020 年，我们有 GPT-3，它导致了参数从 1250
亿到 1.8 万亿的显著扩展，发展到 GPT-4。然而，就所有人而言，我们似乎已经撞墙了：Claude 和 Opus 3.5 迫在眉睫，而且没有
GPT-4.5 或 Gemini 2 的迹象。这似乎是我所说的“2 万亿参数墙”。超越这一点，例如达到 10
万亿参数，由于相关的训练成本和数据要求，这并不被认为是好主意。业界似乎不愿意为微小的改进支付更高的费用，这导致推理范式的转变。

当我们转向更多以研究为导向的项目时，对通用计算资源的需求也更大。这与之前范式的生产部署的增加相吻合。因此，GPU 富豪蓬勃发展是有道理的。最近，我们采访了
Together 和 Fireworks，Replicate 也在我们的名单上。虽然我们还没有关注
Skill，但亚马逊已经成为这个领域中一个有趣的参与者。特别是在他们在 reInvent
大会上表现出色之后，他们已经成为一个重要的基础模型实验室。David 在那里创建模型所做的努力值得关注，并表明了形势的变化。

**主持人 Alessio：** 是啊，我的意思是，这就是预付合同的力量。我认为像很多 AWS
客户一样，他们会签订大量预留实例合同，现在他们必须使用他们的钱。这就是为什么这么多初创公司通过 AWS 市场被收购的原因。他们有点把它们捆绑在一起。

**主持人SWYX：** 并优先定价。好的，所以也许 GPU 超级富豪做得很好，GPU 中产阶级却不行。

**主持人 Alessio：** 然后是 GPU 贫乏。我的意思是，我觉得每个人都应该富裕起来，不应该真的存在。即使是 GPU 贫乏，真的有意义吗？如果你
GPU 贫乏，你应该直接使用云。是的。你知道，我认为一旦我们弄清楚这些模型的大小和形状，可能会有一个未来，那时像小盒子这样的东西会实现，你可以在家里 GPU
贫乏。但我觉得今天的问题是，为什么要如此努力地让这些模型在非常小的集群上运行，而在云上运行却如此便宜……是的。

**主持人SWYX：** 我觉得大多数人认为这很酷。人们认为这是扩大规模的垫脚石。所以他们渴望有一天成为 GPU
富豪，并且他们正在研究新的方法。比如新研究，可能他们今年做的最深奥的技术是 distro
或其他什么名字。人们对异构计算和分布式计算非常感兴趣。我通常历史上都不太重视这一点，但它可能即将变得相关。

我不知道。你知道，SF Compute
今年推出了他们的计算市场。但真的有人在用吗？它就像一堆小的集群，不同类型的计算。如果你能让它变得有用，那么它将对更广泛的社区非常有益。但这可能仍然不是前沿模型的来源。这只是一种为人们解锁的第二层计算。这很好。

但是，是的，我的意思是，我认为今年会有更多设备上的计算。我现在手机上有 Apple 智能。除了总结我的通知之外，它什么都不做。但还不错——它是多模态的。

然后 Gemini Nano 将在 Chrome 中推出。它们仍然被标记为功能标志，但如果你使用 alpha
版，现在就可以尝试。我认为我们正在获得这些东西的 GPU 贫乏版本，而且我认为这很有用。Windows 也是如此，在每个 Windows 部门推出 RWKB
非常酷。

我认为我从未将之纳入这场 GPU 贫乏战争的最后一件事，我现在应该这样做，那就是许多初创公司是 GPU 贫乏，但仍然可以很好地扩展，作为基础模型实验室或
GPU 云的包装。GPU 云的一个例子就是 Suno。Suno 被评为今年增长最快的初创公司之一。最新的公开数字是今年 ARR 从零到 2000 万美元。

另一个例子是 Bolt，它是一个直接的 Claude 包装器。现在他们宣布了 2000 万美元的 ARR，这比我们在标题中写的 800
万美元又上了一个台阶。令人难以置信的是，所有这些 GPU 贫乏的公司都在找到成功之道，而 GPU 富裕的公司也在找到成功之道。

**我看到的唯一失败可以用我所说的 GPU
微笑曲线来描述，即两端表现良好。你要么靠近机器，并且是机器上的第一名，要么靠近客户，并且是客户方面的第一名。然而，中间的公司，也就是拐点，表现不佳**
。我认为 Character 是所有人中表现最好的。你在这里有一条笔记，我们显然说过 Character 的估值是 100%。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgBDq1ljDxia7xnve0Z2HiaFw1EVBqXbgXrTPxXkAk8DUVu5AjchuXepicPQ/640?wx_fmt=jpeg&from=appmsg)

**主持人 Alessio：** 我说过这话吗？是的。你说谷歌应该花10亿美元直接买下他们。这数字太疯狂了。

**主持人SWYX：** 我是说，就好像，是啊，你为了什么付钱？我不知道竞价战是什么样的，也许起拍价就是10亿美元。

**主持人 Alessio：**
我是说，不管怎样，这对所有参与者来说都是好事。多模态之战，在这场战役中，我们最初的版本里没有文本转视频的功能，而现在这是最热门的功能。是啊，我会说它是图像的一个子集，但的确是。好吧，但我认为当时人们并没有真正做这个。现在，Veo2昨天刚发布。Sora是上周发布的。你试过Sora吗？我还没试过Sora。

**主持人SWYX：** 我觉得它现在应该可以公开使用了。你可以去sora.com试试。

**主持人 Alessio：** 是的，他们遭遇了中断，我认为这也起了作用。一些小事罢了。你今天发的另一个模型是什么？在Replicate
Video或者One Live上的那个？

**主持人SWYX：**
是的，名字非常普通，但它来自Minimax，我觉得是一家中国实验室。中国实验室在视频模型方面做得出奇地好。。是啊，中国人喜欢视频。我能说什么呢？他们有很多视频训练数据。好吧，我不知道。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgBXH2cLicEsbMWsLqU4bR19N9XtfzJu5YCNnr28NibichjyQiaIt0TdOC2uA/640?wx_fmt=jpeg&from=appmsg)

**主持人 Alessio：** 我觉得没有太多其他的东西了。我觉得，比如，在图像方面，

**主持人SWYX：** 我觉得它仍然是开放的。是的，我的意思是，11
Labs现在是独角兽。所以，多模态之战到底是什么？**多模态之战是，你专注于单一模态，对吧？还是你有一个能处理所有模态的上帝模型，**
对吧？所以这绝对还在进行中，比如11 Labs现在是独角兽。Pika Labs做得不错；他们最近发布了Pika
2.0。HeyGen，我觉得它的年度经常性收入达到了1亿美元。Assembly，我不清楚，但他们到处都有广告牌，所以我认为他们做得非常好。这些都是专门的模型和专门的初创公司，尤其是在产品方面。

然后是那些做一体化的大型实验室。在这里，我特别要提到Gemini
2，它具有原生的图像输出功能。你看过演示吗？没有吗？是的，很难跟上。他们真的是上周发布的。要感谢Paige Bailey，她在发布当天来到Latent
Space活动上进行演示。她没有做准备；她只是说，我来给你们展示一下。他们有语音功能，有图像输入，当然，他们可以共同生成等等。

但是OpenAI和Meta都有的新功能，虽然他们还没有推出，是图像输出。你可以输入一张汽车的图片，并要求对汽车进行细微的修改，他们可以完全按照你的要求生成修改后的图片。不需要像稳定扩散或ComfyUI那样进行遮罩和绘制之类的操作。这是小模型才有的废话。大模型的人会说，哈，我们用Transformer搞定一切。这就是多模态之战，也就是你押注上帝模型，还是像个傻瓜一样把一堆小模型串联起来。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgBiauGLJ3y4hprJMl7KBpHbM01EU4G3bUemuxbh1NH6DPIHDTe665gGLQ/640?wx_fmt=jpeg&from=appmsg)

**主持人 Alessio：**
是的，我不知道，伙计。是的，那会很有趣。我的意思是，显然，我用MidJourney来做我们所有的缩略图。不过，他们在产品方面做了很多。我想说他们发布了一个新的MidJourney编辑器之类的东西。他们做了很多，因为我觉得，是的，这个模型——有点像，也许你知道，人们说Black
Forest模型在逐像素的基础上比MidJourney更好。

但我觉得当你把它们放在一起看时，你有没有在Black Forest上尝试过相同的提示？是的，但问题是，你知道，在Black
Forest上，它只生成一张图片，然后你必须重新生成。你没有我做的所有这些UI功能。

有问题吗？
不，但这是一个时间问题，你知道吗？就像在MidJourney上，调用API四次。不，但是那样就没有变化了。就像MidJourney的好处是你只要进入那里，你就可以开始创作。有很多东西让它变得非常非常简单。

我觉得人们低估了这一点——这不是一个技术问题，因为我正在为MidJourney付费。所以，这是一个Black
Forest的技术问题，因为我没有为他们付费，对吗？你知道的。

**主持人SWYX：** 所以这是一个用户体验问题，对吧？你明白，至少我们认为Black
Forrest应该能够做到所有这些事情。我还要强调一下，ReCraft已经在人工分析所做的图像领域中脱颖而出。它显然已经取代了Flux的位置。这还是真的吗？人工分析现在是一家公司。我想我在今年早期的AI新闻中强调过他们，他们已经推出了一系列的竞技场。他们正试图挑战LM
Arena，Anastasios和他的团队，他们有一个图像竞技场。哦，是的，ReCraft V3现在击败了Flux
1.1，这非常令人惊讶，因为Flux和Black Forest Labs是老牌的稳定扩散团队，在管理问题后离开了stability。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgBlDvib20bg5eiaumDn4MgWFLC7AT26gCdibo2OYW3odXvUKH1cG64x3rsQ/640?wx_fmt=jpeg&from=appmsg)

ReCraft从无到有，成为顶级的图像模型。非常非常奇怪。我还想强调一下，Grok现在推出了Aurora，这在Grok和Black Forest
Labs之间呈现出有趣的动态。Grok的图像最初是与Black Forest
Labs合作推出的，作为一种简单的包装，然后Grok说，不，我们要自己做。他们已经自己做了。我不知道；没有任何关于它的API或基准测试。他们只是宣布了。所以，是的，这就是多模态之战。我想说，到目前为止，小型模型和专用模型的人正在获胜，因为他们只专注于他们的任务，而大型模型的人总是在追赶。

当我看到Gemini
2的图像编辑演示时，我可以输入一张图像并直接提出要求，我想，这就是人工智能应该有的样子——而不是一堆复杂的步骤。它真的是很厉害。**我认为今年我们还没有看到的一个前沿是视频与音频的同步**
。显然，视频表现非常好，并将继续增长，特别是随着今天Sora
Turbo的发布。在某个时候，我们将获得完整的Sora，或者至少好莱坞实验室会获得完整的Sora。**我交谈过的研究人员已经在讨论视频与音频同步作为下一个前沿。然而，视频可能还需要大约五年的时间才能真正达到这一点**
。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgBbGduXdfHQKYv7XGiandibSnFONXkHibVhWL76ASeyfvibHzicUOoHtvtGmg/640?wx_fmt=jpeg&from=appmsg)

我想说，与OpenAI相比，Gemini的方法似乎更加成熟。如果你看看我发布的ICML回顾，到目前为止还没有人听过，它只有七个小时长。为什么人们不听呢？它就像把所有东西都放在一起了。DeepMind正在研究Genie；他们还发布了Genie
2和Video Poet。他们在世界建模方面可能比OpenAI有四年的优势，因为OpenAI去年才在聘请了GoPeebles之后开始研究Diffusion
Transformers。因此，我想说DeepMind在这里展示他们的能力方面具有一些优势。

Veo2之所以好，一是因为他们挑选了他们的视频，所以显然看起来比Sora更好。然而，我相信当Veo2完全推出时，它会做得非常好，因为他们在视频方面做了多年的背景工作。去年在纽约网站上，我已经在采访他们的一些视频人员。我忘记了他们的模型名称，但对于该主题的忠实粉丝来说，他们可以去2023年纽约网站查看那篇论文。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgBQOvTZXic98eFbu6RNx02TebZIg6sKCOFjGr9iadqKeoPVCibTub8pc8hA/640?wx_fmt=jpeg&from=appmsg)

**主持人 Alessio：** 最后但同样重要的是，LLMOps（大语言模型运维）或者说regops（监管运维）。

**主持人SWYX：** 我把最新的图表放在了“智囊团”那一期节目里。我想把这些文章从节目注释中分离出来。顺便说一下，我以前这样做，是因为我想在Hacker
News上展示。我希望这个播客能在Hacker News上出现。所以我总是把一篇文章放在里面，因为Hacker
News的人喜欢阅读，不喜欢听。所以节目文章，我也不知道，我们现在只是把它们分开来做了。你说 Langchain 和 Llama Index 还在增长。

**主持人 Alessio：**
是的，我看了PyPI的统计数据，你知道，我不在乎那些星标。在PyPI上，你可以看到……你想分享一下你的屏幕吗？好的。我更喜欢看实际的下载量，而不是GitHub上的星标。

所以如果你看一下，你会发现LangChain仍在增长。这些是过去六个月的数据。Llama
Index也在增长。我基本上看到的是，第一，显然这些东西都有商业产品。

所以有人购买并坚持使用它们，而不是在各种东西之间跳来跳去。相比之下，例如，CrewAI的增长就没那么明显。虽然星标在增长，如果你在GitHub上看，星标是在增长，但使用量在过去六个月里基本持平。

**主持人SWYX：**
他们有没有做过某种重组，比如把软件包拆分开，现在变成了一堆软件包？有时候会发生这种情况，你知道吗？我没看到这种情况。我能看到两种情况都发生。**CrewAI的声势很大，但使用的人不多**
。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgBpxrHJoNTT2ra36SpCLuGWQU9uZFDbURRwRa9qQnrRibnJwuQQcZCJEw/640?wx_fmt=jpeg&from=appmsg)

**主持人 Alessio：** 总之，对我来说，这就像是分裂。我的意思是，跟AutoGPT类似，现在仍然有使用AutoGPT的等待名单。

**主持人SWYX：** 是的，他们仍然很活跃。他们发布了一些东西。

**主持人 Alessio：**
但我认为那是另一个例子。它是GitHub历史上增长最快的项目。但我觉得当你可能计算星标的价值和炒作的价值时，我认为你在AI领域会经常看到这种情况，那就是大量的星标，大量的兴趣，而且这种增长速度在开源领域过去是没有的，以前没人急着去启动一个NoSQL数据库，只是为了构建一个数据库。

**主持人SWYX：**
是的。我认为这里有趣的一点是，在AI领域，你好像可以先为承诺买单。然后，在交付方面，人们很有耐心。我认为这种耐心随着时间的推移已经减少了。这里有一个例子就是Devin，对吧，今年，3月份的时候有很多承诺，然后花了九个月才达到GA（正式发布）。但我认为人们现在仍然对Devin感兴趣。Devin的产品已经有了一些改进，甚至你也将成为付费客户。所以我认为像Devin这样的产品会成功。我不知道是不是Devin本身。

AutoGPT有一个有趣的第二层，我认为这里面存在一些AI特有的动态。过度承诺和交付不足适用于任何初创公司。但具体到AI，存在一种通用的承诺，即我可以做任何事情。所以AutoGPT最初的问题是赚钱，比如增加我的净资产。我认为这意味着来自很多不同的人的广泛兴趣，他们试图在这个项目上做各种不同的事情。所以这就集中了很多星标。然后，显然，因为它做得太多了，或者不够专注，所以它无法部署。这就是我对为什么兴趣与使用率如此之低的解释。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgB2ib2UwrmcEPqMqb4bGLar58FiaOPaQiaXvCld9ublmtz9K1DxG2ZpJMqA/640?wx_fmt=jpeg&from=appmsg)

第二个显然是纯粹的执行问题。团队需要有愿景并执行。在去年的AI工程师峰会之后，一半的核心团队离开了。这就是我的解释。这种通用性的承诺基本上只适用于ChatGPT，对吧？也许今年还包括Notebook
LM。把任何东西放进去，它基本上都是正确的。而对于其他所有人来说，感觉就是，我们会帮你完成代码。我们会帮助你进行PR审核。都是一些小的事情。

**主持人 Alessio：**
是的。好的，代码解释。我们已经谈论过很多次了。我们在这个播客上软发布了E2B的融资消息。CodeSandbox上周被Together
AI收购了，他们现在也将把它作为API提供。越来越多的活动，这很棒。

是的，然后在前两期节目中，我们和Bolt谈论了他们一直在做的Web容器相关的东西。我认为代码解释领域可能存在一种频谱，就像有专门的SDK。也有像，嗯，世界上的各种模型，就像，嘿，我们有一个沙箱。现在你可以直接运行命令并协调所有这些。

我认为这是其中之一，我的意思是，E2B的增长一直非常疯狂，只是因为，我的意思是，每个人都需要运行代码。对吧。而且我认为现在所有的产品，每个人都在升级，比如，好吧，仅仅聊天是不够的。

所以Perplexity，它是E2B的客户，他们会为金融以及其他各种领域制作这些漂亮的图表。这说明产品正在成熟，而且我认为这正变得越来越像一个“迫在眉睫”的问题，可以说。所以，是的，很高兴看到更多，而且这个在我们最初写“四大战争”时并没有引起注意。

**主持人SWYX：**
是的，我认为主要是因为我当时试图把它限制在RAG和运维方面。但我认为现在Frontier在核心工具集方面已经扩展了，核心工具集应该包括代码解释，就像每个代理都需要的工具。Graham在他的代理状态演讲中也提到了这一点，这对我来说很有趣，因为每个人都发现了相同的集合。所以基本上就像每个人都需要网络浏览，每个人都需要代码解释，然后每个人都需要某种形式的记忆或计划或其他类似的东西。随着时间的推移，我们会发现更多，但我认为这是我们目前为止发现的。

我还要提到Morph
Labs发布了一个时间旅行VM。我认为这些东西的状态性需要被严格锁定。你不能只是启动一个VM，在上面运行代码，然后杀死它。因为有时你可能需要时间旅行回去，比如回溯或分叉，来探索代理开发中树搜索方法的不同路径。我要指出的是，较新的实现，新的实现，是人们在代理进行大规模扩展式代码执行时将需要的新兴前沿。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgB3Gy9gTeribVl7fqzHlNiad7gRuFFgeUUJVF8BTTVCSLADBktWApiaaGFA/640?wx_fmt=jpeg&from=appmsg)

此外，我认为ChatGPT
Canvas，在他们宣布的12天发货活动中发布的产品，已经出人意料地取代了代码解释器。代码解释器是去年的东西，而现在Canvas也可以编写代码，而且可以做比代码解释器以前做的事情更多。所以现在它还没有被取代；当您创建新的自定义GPT时，会有一个Canvas和代码解释器的切换框。你知道，我之前的论点是，自定义GPT是您投资的路线图，因为它满足了每个人的需求。

现在有一个新的框叫Canvas，每个人都可以访问，而且基本上，你没有理由使用代码解释器而不是Canvas。**Canvas已经加入了差异模式，Anthropic、OpenAI和Fireworks现在都已经推出了这种模式。我认为这将成为明年的常态，因为每个人都需要某种形式的差异模式代码解释器功能**
。AIDR在这方面也很早就进行了尝试，AIDR的基准测试就是基于差异的。

另外记忆产品，你认为它是不真实的。

**主持人 Alessio：**
是啊，我只是觉得现在的多数记忆产品，就像是总结和提取。是啊，它们非常不成熟。是啊，没有隐式记忆，你知道的。它不是你写下的内容的显式记忆。它没有隐式提取，比如，哦，你对这个说了不，你对这个说了十次不。

**主持人SWYX：** 当你说记忆产品时，你的意思是那些提供记忆即服务的初创公司吗？

**主持人 Alessio：** 是啊，或者，你知道，Lindy
也有类似记忆的功能，你知道，它根据我说的话来记住，**所以它不太像是创建我偏好的实际记忆，更多的是关于我明确说过的内容，**
嗯，我正在试图弄清楚在哪个层面解决这个问题，你知道，比如这些记忆产品，像MGPDS这样的公司，能否创造一种更好的方法来隐式提取偏好，或者是否可以做得很好，你知道，我想这就是为什么我不认为记忆不真实。我只是觉得，今天的方法实际上不是记忆，或者说不是一个系统需要拥有的。

**主持人SWYX：**
是的，我其实同意这一点。但我会认为它是不成熟，而不是不需要。就像，这显然是我们迟早会想要的东西。所以现在开发它的人，你知道，在这方面做得不是很好。我肯定会预测明年会更好，后年会比明年更好。

我确实认为上次我们和 Harrison 一起做 Shunyu 的播客时，我过于关注 LangMem 作为一个独立的产品。他现在已经把它整合到
LandGraph 中作为一种记忆服务，使用相同的 API。我认为每个人都需要某种形式的记忆，而且这现在已经与普通的 RAG
向量数据库区分开来，成为一种单独的需求。你需要一个记忆层，无论它是否在向量数据库之上，这取决于你。一个记忆数据库和一个向量数据库是两种不同的东西。

就像我必须为此辩解太多次了，以至于我在Latent
Space仪表板中有一篇草稿文章，基本上说明了记忆和知识之间的区别是什么？对我来说，很清楚：知识是关于你周围的世界。有你拥有的知识，也就是 RAG
语料库，比如你的公司文档之类的，然后有外部知识，也就是你在谷歌上搜索的东西。

所以，你使用像 Exa
之类的东西，然后是记忆，也就是我随着时间的推移与你的互动。两者都可以用向量数据库或知识图谱来表示；这真的不重要。时间在记忆中特别重要，因为你需要一个衰减函数，然后你还需要一个复习函数。很多人将其实现为睡眠。当你睡觉时，你实际上会处理白天的记忆，并产生新的见解，然后将这些见解保留下来，并在未来将其带入上下文中。

所以，我觉得这正在被开发。LandGraph 有一个这样的版本，ZEP 是另一个基于 Neo4j 的知识图谱的版本，它也有一个这样的版本。MemGPT
曾经有这个，但我觉得自从被 QuietCapital 投资后，已经扩展成更像是一种通用的 LLMOS
类型的初创公司，我觉得现在有很多这样的公司。大家都投入进去了。

**主持人 Alessio：** 你认为这是一个 LLMOS 产品还是应该是一个消费品？

**主持人SWYX：**
我认为它是一个构建模块。我认为每个消费产品最终都会想要一个管理其请求和运营工具等的网关。代码解释器对于在幕后执行代码是必要的，即使代码本身没有暴露出来。

实施长期记忆很重要。作为消费者，假设你是一个新的点计算机，它已经启动了自己的小代理，或者如果你来自
friend.com，你迟早会想投资记忆。也许不是今天，也许你可以用一百万个 token 的上下文延迟一段时间。

然而，在某个时候，你需要通过压缩和选择性检索来管理你的记忆。这就提出了如何有效地应对这一挑战的问题。你可能会发现你需要重新发明整个内存堆栈，而这些公司已经为此工作了一年了。

**主持人 Alessio：**
是啊，对我来说，它更像是我想带上记忆。它几乎就像是我的记忆，对吧？那么，为什么要选择性地选择我所带的记忆呢？是啊，为什么我每次使用一个新产品时，它都需要重新学习关于我的一切？好吧，你想要便携式记忆。是啊，它像一个协议吗？比如，它是如何工作的？

**主持人SWYX：** 说到协议，Anthropix
模型上下文协议他们发布的这个有300行代码的记忆实现。非常简单，对所有记忆初创公司来说都是非常糟糕的消息。但这已经足够了。是的，能够拥有一个便携式的记忆，让你发送给其他人，那会很好。

简单的答案是，在一段时间内不会有标准化，因为每个人都会尝试自己的东西。我认为 Anthropix 在 MCP
上的成功表明，基本上只有大型实验室才能做到这一点。因为没有其他人有足够的影响力来做到这一点。那么事情就会是这样发展。

就像，除非你有一些很傻的东西，比如，好的，一种标准化形式基本上来自 Georgi Gorgianov 的 Llama
CPP，对吧？那是完全开源的，完全自下而上的。那是因为那里有大量的工作需要完成，然后人们才从那里开始发展。

另一种标准化形式是 Comfy Anonymous 的
ComfyUI。所以可以完成那种标准化。基本上需要有人为角色扮演社区创建这个，因为那些人拥有最长的记忆。

就我所知，目前角色扮演社区已经研究了 Sully Tavern 和
Cobalt。他们只分享角色卡，并且有四五种不同的标准化版本的角色卡，但没有人拥有可导出的记忆。如果有人首先开发出一种成为标准的记忆，那就会是那些人。

**主持人 Alessio：** 酷。很高兴看到人们构建的东西。基准测试。

**主持人SWYX：**
我们最喜欢的主题之一。是啊，是啊。所以基本上，我只是想简单地提一下。我认为在年终回顾中，提醒大家我们过去的状态是很有用的。我们谈到了 LMSys ELO
中，每个人都取得了进步，而且竞争非常激烈。看看 OpenAI 今天发布的直播，他们介绍了具有结构化输出和一切功能的 O1
API，我注意到他们讨论的基准与我们去年这个时候考虑的基准完全不同。

去年这个时候，我们仍然在谈论 MMLU 和一点 GSM-8K。有一些基准基本上在 Hugging Face Open Models 排行榜的 V1
版本中，对吧？我们和 Clementine 谈论了她为升级到 V2 所做的决定。我还想说，LMSYS，现在是 LM
Arena，今年已经成为大型前沿实验室之间竞争的主要战场。此外，我们还看到了 SWEBench、LiveBench、MMLU Pro 和 AIME
的出现，特别是 AIME。

有趣的是，看看2020年到2021年、2022年、2023年，然后到2024年，今年被引用最多的基准是什么。你可以看到哪些已经被饱和和解决，以及人们现在关心的是什么。目前，人们对前沿数学编码非常感兴趣。实际上有一个名为“前沿数学”的基准，我曾在
NeurIPS 上花了一些时间讨论。其他基准包括 AIME、Livebench、MMLU Pro 和 SWEBench。

回顾去年，还有一个名为 GPQA 的基准，我在这里将其列为顶级基准之一。可悲的是，欧洲的 GPQA 被宣布死亡，但人们仍在谈论 GPQA
Diamond。GPQA 的名称代表 Google Proof Question
Answering（谷歌证明问题解答），它旨在在一段时间内抵抗饱和。Noam Brown 表示 GPQA 已死，因此我们现在关注
SWEBench、LiveBench、MMLU Pro 和 AIME。

即使是 SWEBench，我们也不关心 SWEBench 本身；我们关注 SWEBench 验证和 SWEBench 多模态。此外，我们对 Andy
Kaminsky 的新 Kowinski 奖感兴趣，Andy Kaminsky 是我们昨天谈论过的人，他基于 C-Bench 度量标准启动了类似的 AGI
尝试，这可以说是更有用的。OpenAI 也有 MLE Bench，它跟踪 ML
研究和自举。这可能是与前沿实验室最相关的关键指标，因为它表明研究人员何时可以自动化自己的工作。如果我们最终达到那个点，这可能是加速曲线中的一个关键环节。

**主持人 Alessio：** 是啊，有道理。我是说，我挺好奇的。我觉得迪伦在辩论会上说，SWEBench 80% 像是他对明年年底的期望。

**主持人SWYX：**
表明模型还在不断改进。请记住，我们年初的时候只有13%。是的，没错。所以现在我们已经超过50%了。“开放之手”也在这个水平附近。是的，80%听起来不错。Kominsky
提出的是90。

**主持人 Alessio：** 然后当我们达到100%时，开源就会赶上。

**主持人SWYX：**
哦，是的。奇迹般地缩小闭源和开源之间的差距。所以基本上，我对人们的建议是关注基准语言的缓慢演变。那些不在前沿的实验室会继续用去年的基准来衡量自己。与此同时，那些真正处于前沿的实验室会告诉你一些你从未听说过的基准，然后你会想，“哦，有新的领域可以探索了。”这就是我给出的快速提示。

我可能不会过多地强调这一点，但我也想说，也许Vio引入了一些新的视频基准。本质上，每一个新的前沿能力都会引入新的基准。我们还简要讨论了Ruler作为新的......去年，我们谈论了大海捞针，而Ruler基本上是一个多维的大海捞针。我们会在描述中链接所有剧集。这是对我们所有剧集的回顾，我都在脑子里——这是我在DevDate演讲中展示的其中一张幻灯片。

现在，**我们要从基准转向能力了**
。我认为我有一个有用的分类方法，我一直在推广，我很想听听你们的反馈或修改意见。我喜欢“思维点”模型，它把能力分为成熟、新兴、前沿和利基。成熟的能力是指那些你可以在生产中可靠使用的能力；它们已经解决了，每个人都有。例如，已解决的问题包括常识、MMLU和长上下文——每个人都有128K。今天，Owen宣布了200K，这非常昂贵；我不确定它的定价。

有些解决的问题包括RAG，它有大约18种不同的类型，但大部分都得到了解决。在转录方面，我认为你应该尽可能多地使用Whisper。代码生成也有些解决，但我需要区分单行自动补全和多文件生成。

在新兴方面，工具使用仍然被认为是新兴的，但它可能已经更成熟了，因为他们今年才推出了结构化输出。至于明年，我认为新兴是没问题的。视觉语言模型现在已经广泛可用；每个人都有视觉能力，包括O1。这很明显。视觉能力的一个子集是PDF解析，我认为社区对Colpali和Colquen正在进行的工作感到非常兴奋。你们对此有什么看法？

**主持人 Alessio：** 视觉的临界点。

**主持人SWYX：**
我觉得基本上就是现在。这可能就两个月左右的历史。英伟达，世界上最有价值的公司。而且，我认为这是在六月份。他们在第三季度的收益方面也让很多人感到意外。我认为我在AI新闻中强调的引言是，Blackwell
是公司历史上最畅销的系列。而且他们已经卖光了，我的意思是，显然它们总是卖光了，但是他发表这个声明，我认为这，这又表明了从H系列到B系列的过渡。会进展顺利。

**主持人 Alessio：** 是的，我的意思是，如果你只是买了英伟达的股票，并试玩了你的BT游戏，那就太...是的，太疯狂了。

**主持人SWYX：**
英伟达还是比特币？我觉得是英伟达。问题是，人们问我，不投资英伟达的原因是什么？我认为真的只是，他们已经致力于此。他们从两年周期改为了一年周期，对吧？所以只要一步失误就会导致延迟。你知道，过去曾经出现过延迟。当延迟发生时，通常都是很好的买入机会。

所以我们的计划基本上是涵盖我们认为的新兴能力、前沿能力和利基能力。新兴的包括工具使用、视觉语言模型（你刚才听到的）、实时转录（我将在我们即将推出的一期节目中介绍），以及你可以在Whisper
Web GPU中尝试的B系列，这非常棒。我认为语音分割能力也在成熟，但仍然很难正确地完成。就像我们为了使潜在空间转录正确输出，做了很多工作。

我想，也许，你知道，Dwarkesh最近一直在谈论他如何使用Gemini 2.0
Flash来做这件事。我认为这可能是一个不错的尝试，一个不错的方法。特别是如果涉及串音，那可能会非常好。但是可能还有其他原因可以使用正常的语音分割模型，特别是PyAnote。关于文本和图像，我们已经谈论了很多，所以我就跳过了。

然后我们讲到前沿，我认为它基本上在地平线上，但还没有准备好广泛使用。给人们展示一下很有趣，但我们还没有真正弄清楚如何从长推理、实时中断式的实时API语音模式、设备端模型以及所有其他模态中赚取大量资金。

然后是利基模型，利基能力。我总是说基础模型被严重低估了。人们总是喜欢与基础模型对话。我们越来越少地访问它们。这很有可能。我认为，你知道，Sam
Altman在2025年询问他应该发布什么或者人们希望他开源什么。人们真的想要GPT-3基础模型。

嗯，我们可能会得到它；我们可能会得到它。这只是出于历史兴趣。嗯，但你知道，在这一点上，对他来说这肯定不再是一个重要的知识产权了，所以我们拭目以待。我认为OpenAI有更多的事情需要担心，而不是发布基础模型，但是这对社区来说会是一件非常好的事情。状态空间模型，我也要说，今年看到了很多炒作。尽管在Linspace
Live上的后Transformer讨论非常火爆，观看次数也很多，但感觉今年有所下降。我不知道为什么。似乎空间模型和RWKV正在扩展。所以Cartesia，我认为做得非常好。我们把它们用于很多东西，特别是小型演讲和我们的一些播客克隆。我认为它们也是11
Labs的真正挑战者。

我当然会在Windows上运行RWKB。我仍然会说，这些是我们长期以来一直在讨论的未来问题。我的意思是，从技术上讲，我们生活在比去年晚一年的未来，而我们仍然在说与去年完全相同的事情。所以改变了什么？我不知道。我确实认为，当我们讨论NeurIPS论文时，我们将要介绍的XLSTM论文值得一看。我认为他们非常清楚地了解他们想要如何修复LSTM。

所以，我们还想谈谈今年的一些主要主题。我们想按月进行回顾，所以我将把你带回录音，我们仍然保留着录音的音频。其中一个主要主题是底部推理竞赛。我们去年这个时候开始了这场竞赛，当时是2023年的混合审判价格战，混合审判的价格在短短几周内从每token
1.80美元降至1.27美元。很多人也对今年的价格战和价格情报曲线感兴趣。我开始追踪它，我认为大概是在2024年3月Haiku发布时。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgBp9utEHgzBMF6GvGR1Nk5F4PxZLOoMomEEhBQQrDSJfwBqFptdu9zBw/640?wx_fmt=jpeg&from=appmsg)

如果你在YouTube上观看这段视频，这就是我最初绘制的图表，每个人都落入了LM的ELO与模型定价的相当紧凑的范围内。你可以为更高的智能支付更多的钱，而获得较低的智能则会更便宜，但大致来说是相关的——这是一条趋势线。我在7月份再次更新了它，发现一切都向右移动了。对于相同数量的ELO，比如2023年的GPT-4大约是1175
ELO，你曾经需要花费大约每百万token 40美元才能获得。现在你可以花费大约50美分获得Claude3
Haiku。这在一年左右的时间里提高了两个数量级。

更重要的是，我认为你可以看到最近发布的Claude3 Opus，它在今年3月发布，现在基本上完全被Gemini 1.5
Pro所取代，它每百万的价格更便宜，为5美元，而且更智能，ELO值略高于1250。从3月前沿到7月前沿的转变大约代表了每个ISO ELO改进一个数量级。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgB9wS0wzhZ2D6CAUzkEOhvlPrSPHNqTMM7745dibEdrj80qJKzYmBKbHw/640?wx_fmt=jpeg&from=appmsg)

你现在在7月份开始看到的是4o Mini和DeepSeek V2作为7月前沿的异常值出现，而7月的前沿曾经由4.0 Llama 405、Gemini 1.5
Flash。这些东西打破了前沿，如果你在一个月后更新它，你会看到更多的项目开始出现在8月的前沿，Gemini 1.5
Flash的出现，价格大幅下降，同时保持了大致相同的ELO。

我们可以看到九月份的更新，它确实澄清了人们对定价曲线的理解，即它们是真实的，而不是互联网上某人随意绘制的。Gemini 1.5 降低了价格，并在 ELO
定价图表方面与其他所有竞争者保持了一致。到了九月份，我们有了 O1 预览版及其定价以及 ELO，这标志着前沿包含了低端的 O1 预览版、GPT-4o、O1
Mini 和 Gemini Flash。当时，Gemini 1.5 Pro 并不在前沿之列，但在他们降价一半之后，突然加入了前沿。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgBerFuU7VzI89OqBcFCNKSm2CYicjYZ7gCaWHVWsria470YrjkemvbibeNA/640?wx_fmt=jpeg&from=appmsg)

这形成了一条非常具有预测性的线，我认为这很有意思。在十一月份，我们有了 3.5 Haiku New，显然，我们也有了 Sonnet。Sonnet
没有出现在这张图表上，但 3.5 Haiku 的价格是 Haiku 3
的四倍。虽然人们合理地假设这不仅仅是价格上涨，而是更大模型的反映，但我们缺乏透明度，只能得出自己的结论。

总结一下 ELO 定价图表的更新情况，如果你查看我的公开 LLM 定价图表，最近的添加是 Amazon
Nova。我们在播客中简要讨论了这一点，他们引入了一个有效前沿，包括 Amazon Basics LM，其中包括 Nova Pro、Nova Light 和
Nova Micro，智能水平从 1200 到 1300 不等。要超过 1300，你必须为更高层级的模型付费。然而，某些模型，如
Flash，不包含在这里，可能基于更高层级。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBNLFwXqFyEMvhIAjT0fJqgBy108vZ6zGvK1zzpCUc4ibaFDSmzAsvAaUjD8U0YCN8Bl6NcxORKbT1w/640?wx_fmt=jpeg&from=appmsg)

保持这张图表更新始终是一个挑战，但对于 2024 年来说，基于早期的每百万 token 40 到 50 美元左右的定价，现在随着 Amazon Nova
的出现，每M个 token 大约只需 7.5
美分。这是一年内几个数量级的改进，几乎是三个数量级。我之前说过，智能成本正在以每年一个数量级的速度下降，这已经比摩尔定律还要快了。然而，今年出现了更快的下降速度，我认为没有足够的人在讨论这个问题。

此外，还有关于明年随着 H200 成为商品和 Blackwells 的出现，还会有多少降价空间的猜测。这很难预测，并且除了 GPU
之外的许多因素也会影响这些趋势。这个概述总结了主题发展，然后我们过渡到年度概述，我们将回顾今年的 AI 新闻发布，并选择我们最喜欢的。我让我们的新研究助理
Will 协助进行研究，但你也可以查看 AI 新闻以获取当天的所有头条新闻。

我们还计划了一个 AI 回顾，我将从我们的录音中简要地过渡回去。

所以在 1 月份，我们迎来了 Perplexity 的今年第一轮融资。对我来说，值得注意的是 Jeff Bezos 支持了它。Jeff
不会投资很多公司，但当他投资时，他会支持大型项目。他当年支持了谷歌，现在他又支持了新的谷歌，这有点酷。Perplexity 现在价值 90
亿美元，我认为他们今年进行了四轮融资。

Will 还指出 Sam 很快就要谈论 GPT-5 了。当时他还在全球峰会类型的活动之一，达沃斯。当时，关于 GPT-4
仍然存在一些不确定性。人们仍在反思去年的开发者日，在那之后三个月，对 GPT
的信心似乎正在减弱。我觉得这种情绪还没有完全恢复。我听说有人说还有工作正在进行中，我们不应该放弃这些模型，它们实际上现在被低估了。

在市场挑战的背景下，任何市场都很困难。考虑到所有其他取得的进展，评估形势可能非常具有挑战性。例如，ChatGPT 也在 2
月份发布了记忆功能，我们对此进行了一些讨论。我们还遇到了 Gemini
的多元化争议，我们通常不会在播客中讨论这个问题，因为我们专注于技术方面。然而，我们开始看到今年上下文窗口大小的扩展。Gemini 推出了 100 万个
token，我相信也有关于 200 万个 token 的讨论。

我们与 Gradients 做了一个播客，讨论如何为 100 万个 token 微调模型。这不仅仅是声明你的 token
上下文；你还必须有效地使用它。我们越来越看到人们对不仅仅是标尺的方法感兴趣，标尺就像大海捞针一样。我们现在正在研究
muser，它涉及在长上下文中进行推理，而不仅仅是从中检索信息。

我还想强调一下 magic.dev，它在去年凭借其预告的 1 亿 token 模型引起了巨大轰动。尽管它尚未发布，但他们制造了一些关于它的热议，因此我们将在
3 月份尝试邀请他们参加播客。此外，Claude 3
也发布了，这对当前的话题来说非常重要。这标志着我们之前讨论过的市场份额开始发生转变。最初，大多数生产流量都在 OpenAI 上，但现在 Anthropic
引入了一个可靠的前沿模型系列，人们可以过渡到该模型。显然，Sonnet 已经成为一个主力模型，类似于 4o 对 OpenAI 的作用。

Devin 在 3
月份发布，这是一个非常非常大的发布。这可能是技术领域，也许是这个十年中执行最成功的公关活动之一。然而，关于他们发布的视频中具体哪些内容是真实的，存在很多反弹。在那之后，他们花了九个月的时间才将其发布到
GA。现在，你可以每月花费 500
美元购买它，并形成自己的观点。有些人很满意，而另一些人则不太满意。很难兑现他们所做的承诺，虽然其中一些承诺确实得到了兑现，但值得注意的是，许多批评仍然存在。

我认为关于 Devin 的主要警告是，许多想法都可以被复制。这突显了所谓的“GPT
包装器”的持续威胁，这些包装器通过一个单一功能实现产品市场匹配，但很快就会被众多竞争对手复制。因此，竞争归结为品牌、更好的产品和卓越的工程技术——Devin
在这些方面表现出色。当我们展望 4 月份时，我们也与 Yurio 和 Suno 进行了交谈。虽然我们特别关注 Suno，但我也获得了 Yurio 的
beta 访问权限。在我们的播客中，我们尝试了 AI 音乐生成，我非常喜欢。我们的一些朋友在他们的汽车里播放了我们的 Suno 开场歌曲，我喜欢使用 O1
和 Suno 等工具来创作歌词。

然而，尽管有这些积极的体验，一些听众还是选择跳过歌曲。我不确定确切的百分比，但跳过歌曲的 10% 的人促使我们决定删除开场歌曲。我们还看到了 Llama 3
的发布，许多人对此感到兴奋，认为这是一个良好的前沿开源模型。Llama 3 发布了其 8B 和 7B 变体，而 400B 版本稍后发布。然后在 5
月份，GPT 4o 发布，展示了模型效率，并有力地证明了 4o 的能力。这也是关于 Omni 模型的讨论真正开始产生共鸣的地方。

以前，4o 和 4 Turbo 侧重于文本，而不是天生面向视觉或语音。虽然它们包括视觉功能，但原生的语音组件开发程度较低。然后出现了 Sky
语音，每个人都立即爱上了它，尽管它在公开发布之前就被删除了。似乎这是一个自找的麻烦，源于 Sam Altman
的一条三字母推文引发的一系列不幸事件，导致了关于配音演员的不必要的法律问题，她只是像斯嘉丽·约翰逊。我与之交谈过的许多人都对失去的斯嘉丽·约翰逊的声音感到怀旧，这就是共识所在。

在六月份，苹果在 WWC 上发布的 Apple Intelligence 引起了广泛的讨论。我们大多数人都体验过手机上的更新，特别是如果你拥有
iPhone。我会将这种体验归类为不错；然而，它并不是许多人预期的游戏规则改变者。人们期望由于 iPhone 的大规模升级，该公告将导致苹果股票上涨
20%，但这种情况并没有发生。尽管如此，这一事件可能代表了迄今为止最大规模的 Transformer 模型部署，继谷歌为搜索引入 BERT 之后。

人们正在积极使用新功能，这些功能利用在设备上本地运行的 30
亿参数基础模型，该模型具有热插拔层。苹果确实为我们提供了重大进步，尽管它们不是最透明的公司。值得称赞的是，他们分享了比以往苹果技术发布更多的信息，这受到了研究界的赞赏。

Nvidia 仍然是一个令人感兴趣的话题。我最近参加了台湾的 Computex
贸易展，在那里我观察到了一些奇怪的时刻，包括一个可能反映行业趋势的不寻常的签名活动。虽然有些人猜测 Nvidia
可能已经达到顶峰，但行业的发展势头表明情况并非如此。

当我们回到我们的节目录音时，我想提到的是，尽管一些技术困难导致我们丢失了部分录音，但我们正在重新录制，以供您参考。此外，我们正在努力结束关于 2024
年的讨论，所以让我们继续讨论 6 月、7 月、8 月、9 月的新闻，并总结关于 2024 年下半年的见解。

伊利亚从幕后走到台前，看到了一份条款清单，筹集了十亿美元。丹·格罗斯似乎现在已经成为这家公司的全职
CEO，这很有意思。我以为他会一辈子做投资人，但现在他却在运营公司。他只做过很短一段时间的投资人。两年。关于伊利亚，我们还能说些什么呢？我的意思是，我认为那种只推出一款产品，并且直奔超级智能的想法，似乎是一个非常好的聚焦任务。但是，这基本上与特斯拉和
OpenAI 的做法背道而驰，它们都推出了中间产品，以实现愿景。

**主持人 Alessio：** 嗯，我认为问题在于，像 OpenAI
现在需要更多的钱，因为他们需要支持这些产品。我猜他们可能认为，有了十亿美元，我们就可以实现目标。

**主持人SWYX：** 是的，但是，你从哪里获取数据呢？这就是问题所在。我认为我们也可以把这个看作是 OpenAI
安全部门离职的一个总体主题的一部分。是的。公平地说，简·莱克也离开了，基本上整个超级对齐团队都离开了。

**主持人 Alessio：** 是的，然后出现了一些像 ChatGPT Canvas之类的东西，

**主持人SWYX：** 类似的东西也出来了。我认为它更偏向于代码。是的。除了 OpenAI 之外，还没有人做出Canvas的克隆版。有趣的是，我认为负责
Artifacts 和 Canvas 的同一个人，卡琳娜，在这次之后正式离开了 Anthropic，加入了 OpenAI，这是一种罕见的反向跳槽。

**主持人 Alessio：** 是的，然后我们在六月份举办了人工智能工程师世界博览会。有超过 2000 人参加，不包括我们。我很想参加下一次。

**主持人SWYX：** 如果我们能买到票就好了。是的，但我认为这是一个非常好的演示。我们现在已经有了它，并部署给所有人使用。

Gemini 实际上有点抢先一步发布了 GA
版本，这有点意思。我认为只要你对隐私设置感到舒适，每个人基本上都应该始终开启这个功能，因为这样就相当于有第二个人在看着你。

而且，如果到明年这个时候，我愿意打赌，我会在我的机器上运行这个功能。你知道，我认为那种始终在线、你可以与之交谈，并且能够看到你所看到的东西的助手，至少是软件体验应该发展的方向之一。

然后，在现实生活中实现这一点还需要几年时间，在屏幕之外。但是对于屏幕体验来说，我认为它基本上已经存在了，只是分布不均。我们刚刚看到了在六月份演示的这个功能的
GA 版本。

**主持人 Alessio：** 然后七月份是 Llama
3.1，我们为此做了一整期播客。但这很棒。七月和八月有点平静。是的，结构化输出。我们也为此做了一整期播客。然后到了九月，我们有了 01。是的。草莓，又名
Q-Star。我们和草莓玻璃杯举办了一个很棒的派对。

**主持人SWYX：** 是的。我认为它被严重低估了。这基本上是草莓的第一次内部演示，可以说是在 2023 年 11 月。

从 11 月到 9 月之间，进行了所有的红队测试等等。老实说，发布速度非常快。我不知道人们是否给予 OpenAI 足够的赞誉，因为这一切都在 ChatGPT
中可用，并且不久之后也在 API 中可用。我想可能是在同一天；我已经不记得确切的顺序了。

但这是一款非常非常快地向全世界推出的前沿模型。然后我们很快就习惯了它，很快就说它不令人印象深刻，因为人们仍然在使用 Sonnet
或其他模型。但它仍然非常好。

现在我们有了 O1 Pro 和 O1 Full。就今年最大的发布而言，我认为就是它了。

**主持人 Alessio：** 是的，完全正确。我认为它现在打开了一个关于推理时间计算的潘多拉魔盒，然后

**主持人SWYX：** 以及所有这些。是的。这很有趣，因为这本可以被其他人早点完成。是的。确切地说，这是一个公开的秘密。自从他们聘请 GNOME
以来，他们一直在研究它，但其他人都没有做。是的。另一个发现，我认为伊利亚实际上在 2021 年研究过一个名为 GPT-0
的早期版本。想法完全一样，但失败了，不管那是什么意思。

**主持人 Alessio：** 我的意思是，还有语音模式。语音模式。是的。是的。我想现在大多数人都已经尝试过了。

**主持人SWYX：** 因为它已经全面推出了。是的，我觉得你的妻子也很喜欢它。是的。

**主持人 Alessio：** 是的，她一直在和它说话。好的。十月份的 Canvas。好的。另一个重大发布，计算机使用。你用过多少？老实说，不多。

**主持人SWYX：** 我用很多。你主要用它做什么？起草任何东西。我认为人们没有看到这一切的走向。**OpenAI 真的在和谷歌以及所有东西竞争。**

Canvas 就像 Google Docs。它是一个完整的文档编辑环境，侧边有一个自动助手，可以说比 Google Docs
更好。至少对于某些编辑用例来说，它比侧边有 Gemini 的 Google Docs 拥有更好的 AI 集成。

因此，OpenAI 正在挑战谷歌和 Google Docs。它还在搜索领域展开竞争。他们推出了自己的小 Chrome
扩展程序，作为默认搜索工具。我认为他们正在以一种非常巧妙的方式，逐步地解决谷歌的问题，这种方式是对工作流程的补充。人们应该开始按预期使用它，因为这是对未来的展望。

也许他们不会成功，但至少他们正在尝试。我认为谷歌已经太久没有竞争了，任何尝试的人都至少会受到赞赏。谢谢。

**主持人 Alessio：** 会受到我的关注。是的。然后，是的，计算机使用也出来了。是的，是的，这是忙碌的一年，

**主持人SWYX：** 忙碌的几个月。我会说，计算机使用是今年 Hacker News
上最受好评的演示之一。但是相比之下，我并没有看到人们使用它那么多。是的。这就是你感受到成熟能力和新兴能力之间差异的方式。也许这就是为什么视觉是新兴的。因为我发布了计算机使用，你今天没有使用它，但你使用了其他一切成熟类别中的东西。这主要是因为它不够精确，或者太慢，或者太贵。而这些将是主要的批评。

**主持人 Alessio：** 是的，这有道理。这也是因为对让它失控的整体不安。我认为很多人在乎。十一月。R1。这有点像开源版本的一。

**主持人SWYX：** 这是一个惊喜。没人知道它会来。每个人都知道 F1，我们在 Fireworks
总部进行了预览。然后我想其他一些实验室也做了。但我认为 R1 和 QWQ，Quill。

**主持人 Alessio：** 还有什么要强调的吗？我认为 Stripe 代理工具包，我的意思是，这是一件小事，但就像人们说代理不真实一样。当你有像
Stripe 这样的公司开始构建东西来支持它时，它今天可能不是真实的，但显然他们不必这样做，因为他们不是一家 AI
公司。但他们这样做的事实表明，一方面有需求，另一方面表明他们有信心。

**主持人SWYX：** 这对我来说是一个更广泛的论点，我正在探索，我们是否需要为代理（agents）开发特殊的 SDK？为什么人类使用的普通 SDK
不能做同样的事情？Stripe 代理工具包恰好是 Stripe SDK 的一个包装器。这没问题，它只是一个不错的 DX（开发者体验）层。但我仍然不清楚。

之前有人问过我对这个问题的看法，我想我在播客中也说过，你主要需要的是单独的角色划分，这样你就不会假设是人类在做这些事情。这允许你更快地锁定一些东西，并且可以识别出是代理代表你行事，还是实际上是你本人。这是你需要的东西。

我因为丢了笔记本电脑，导致我的 11 Labs 密钥被泄露了，我注意到了一大堆 API 调用。我当时就在想，哦，那是我自己还是其他人？结果发现是我提交到
GitHub 上但没有清除的密钥。因此，我认为 API 使用的来源应该归因于代理，我们应该为那个世界构建（系统）。

除此之外，我认为，如果我们假设每个工具都需要为代理重新发明，那么 SDK 代表了技术和 AI 的失败。

**主持人 Alessio：** 我在某些方面同意。我认为在其他方面，我们也并非总是把事情说得非常清楚。人们在设计 API
时，通常会使用很多默认设置。但我认为，如果你在一个使用 API 的人或代理拥有几乎无限的记忆和上下文的世界中重新设计它们，你可能会做不同的事情。

但我不知道。对我来说，最有趣的方面是 REST 和 GraphQL。在代理的世界里，这几乎更有趣，因为代理可以提出很多不同的查询方式。以前，我一直认为
GraphQL 有点没必要，因为你知道你需要什么；你只需为它构建 REST 端点。所以，是的，我很好奇看看还会有什么变化。

然后就是搜索大战。我认为像 Search GPT、Perplexity 和 Dropbox Dash 这样的工具就证明了这一点。我们请 Drew
上了播客，然后又加入了 Python Year Summit。Dropbox 拥有 Google Drive 集成的事实感觉非常重要。如果你五年前告诉别人
Dropbox 并不真正在乎你的文件，你知道，这简直无法理解。所以，是的，我很好奇看看它会走向何方。

**主持人SWYX：** 酷。整个领域。这使我们进入了 12
月。仍在发展中。我很好奇“开放式交付”的最后一天会是什么时候。我认为每个人都在期待那里会出现一些重大事件。我认为到目前为止，今年是非常多事的一年。肯定增长了很多。实际上
Will 问我们是否做了预测。我不认为我们做了，但是……没有真的做。

**主持人 Alessio：** 我想我们确实谈论了代理。是的。我不知道我们是否说过这是“代理之年”，嗯，两个月，在能力方面取得了巨大的进步。

**主持人SWYX：** 是的，我的意思是，我认为 Satya 最近也经常说这个词。我的意思是，Sam 已经说了一段时间了。所以，DeepMind 在发布
Gemini 2.0 时，他们发布了 DeepResearch，还有 Project Mariner，这是一个浏览器代理，是他们的计算机使用类型的东西，以及
Jules，这是他们的代码代理。我认为这基本上与 OpenAI 明年发布的 Codename Operator
相辅相成，后者是他们的代理产品。如果它真的可以取代一名初级员工，那它要收费 2000 美元也是合情合理的。

**主持人 Alessio：** 是的，我认为这就是我的全部想法。我写了一篇帖子，它一直在我的 Twitter
上，所以你可以很容易地找到它，内容是关于工作中的技能下限和技能上限。

我认为技能下限将会发生显著变化，我相信 2025 年将是 AI 设定职位技能下限的第一年。你知道，我认为过去并非如此，但我现在看到它正在发生。例如，如果
Devin 在客户支持部门工作，这个职位就要求他比 AI 代理更出色，否则经济上就不可行。

我认为同样的事情也会发生在软件工程领域。目前，该领域的技能下限非常低；许多从事软件工程的人员的技能水平并不特别高。我很好奇在下一年的总结中会发现哪些其他工作会经历类似的变化。

**主持人SWYX：**
我每次参加新晋代表会议，都会与研究人员进行一些交流，然后我会重点介绍该小组的最佳预测。然后我们将进入年终回顾，即前五名的播客。所以最好的预测是，其中一个主要实验室会抓到一个无间道。这反映了一种特定的意识；你知道，每当你看到在旧金山的派对上，男女比例为一百比一，突然有一个女孩对你非常感兴趣，那可能不是因为你的外貌。

这些实验室里保存着很多国家层面的秘密，而且安全性不高。如果有的话，情境意识论文确实提高了人们对这个问题的认识。我认为它的方向是正确的，即使不完全准确。我们应该开始非常关注这个问题。OpenAI
今年聘请了一位首席信息安全官（CISO），我认为安全领域总体上正在变得越来越重要。

哦，我想起来了，在休息之前，我本来想说一下 Apple 的基础模型。他们发布了 Apple Secure Cloud，Cloud
Compute。我认为我们也对投资专注于为所有人提供安全云 LLM
推理的领域感兴趣。我们今天拥有的东西不够安全，因为它属于普通的安全性，而这实际上是一个国家层面的问题。

**主持人 Alessio：** 感谢大家的收听。新年再见。拜拜。

**关注公众号后设🌟标，掌握第一手AI新动态**

##  往期精选

  1. [黄仁勋专访：OpenAI在大模型混战中达到“逃逸速度”](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2650001718&idx=1&sn=f8129a622e7611702be2cb23e8ce9418&chksm=88ba5831bfcdd127d06ce6492c821074407f805407b4182ca900916521cb5a4717f2a3d71ee8&token=1339625777&lang=zh_CN&scene=21#wechat_redirect)
  2. [李飞飞与Justin深度解读空间智能：数字世界需要三维表征，才能与现实世界融合](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2650000659&idx=1&sn=c71fb5b4ef501424dddd5e8b4dd5860e&chksm=88ba4414bfcdcd023c691a1adf75127a9fd883ceb305ca14cf97f719acaf999d40fa72f84bf3&token=1492077842&lang=zh_CN&scene=21#wechat_redirect)
  3. [PayPal创始人彼得·蒂尔：人类科技停滞源于原子方面的进展远慢于比特](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2650000240&idx=1&sn=26af6013981677b1e14137260857a6f0&chksm=88ba4277bfcdcb615d746615c262927bf51c43c920ed93fa36274ef87c6264d6548c84647121&token=106920805&lang=zh_CN&scene=21#wechat_redirect)
  4. [谷歌联合创始人布林：巨头们打造的“上帝模型”几乎可以理解一切](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2649999870&idx=2&sn=0c714d804a72a52e002743d949e1685e&chksm=88ba40f9bfcdc9ef78749718480265922f4fba539abf6c9d62a6cd681f405dee9283d2429f84&token=106920805&lang=zh_CN&scene=21#wechat_redirect)
  5. [马斯克：AI将使商品和服务的成本趋近于零](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2649999870&idx=1&sn=752f000a117a705e77950c82bfc4a004&chksm=88ba40f9bfcdc9ef5a5afe4a3ae73d5247bd54ed525dbdbedee1fcf74a6c082165e664a5c4d0&token=106920805&lang=zh_CN&poc_token=HDp86Waj18SFm2Y-xnv_Vqd_4J6emFoh10eH48wg&scene=21#wechat_redirect)
  6. [Karpathy最新专访：人形机器人、特斯拉、数据墙与合成数据](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2649999613&idx=1&sn=b8bdda7afe4c3ca08e324ac5bbd5a2bd&chksm=88ba41fabfcdc8ec0e21dbf4c7eb4d33252da70f47e1cfc9f5e113717911c417c2aebb3d6180&token=106920805&lang=zh_CN&scene=21#wechat_redirect)

  

