# Keras之父肖莱：实现AGI需要融合“直觉”与“推理”、O1原理是在思维链空间中进行搜索以生成自然语言程序

文章作者: 瓜哥AI新知
发布时间: 2025-01-09 15:32
发布地: 浙江
原文链接: http://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2650006628&idx=1&sn=addee21dae9b829473a82e41e71e4b53&chksm=88ba6d63bfcde47541052b6c719dbd533e625ef976f468692aaebe41f5aaa2cd8160e0fe1ee2#rd

封面图链接: https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBOswZEtzgq2kSyM0wVFaKvBgOJIRxEdZcia7biccAKDVlOt6KSWldL0QfY7oGROVXIb2MtZeJFbic2CQ/300

**👇关注公众号后设🌟标，掌握第一手AI新动态**

本文内容整理自**François Chollet** 在**Machine Learning Street Talk**
的访谈，公开发表于2025年01月09日。原始内容参考：https://www.youtube.com/watch?v=w9WE1aOPjHc

## 内容提要: 弗朗索瓦·肖莱谈测试时计算、O1、AGI、Agent

  * **计算与准确性呈对数关系:** 完成任务所需的计算量与准确性之间存在对数关系。 更大量的计算可以带来准确性的提升，但提升幅度会逐渐减小。 这在蛮力搜索和测试时搜索中都适用。
  * **人类认知的高效性:** 人类大脑在解决复杂问题时，能源效率远高于当前AI模型。 这暗示着，提升AI模型的能源效率是通往通用人工智能的关键。
  * **DSL 的局限性与终身学习:** 虽然DSL在特定领域有优势，但它限制了模型的学习能力。 理想的系统应该能够从数据中学习并构建可重用的函数和模块，从而提升效率。
  * **意识作为自我一致性机制:** 意识可能是一种自我一致性机制，确保迭代的模糊模式匹配不会产生幻觉。 系统二的推理需要意识来维持结果的一致性。
  * **调和符号主义和连接主义:** 人类认知是直觉（连接主义）和推理（符号主义）的混合。 需要结合连续的向量空间表示和离散的符号表示才能取得进展。
  * **新颖问题解决与抽象:** 解决新问题促使模型提取可重用的构建块，并形成更高层次的抽象概念，从而提升效率。
  * **François Chollet 离开 Google 专注程序合成:** 他将与朋友一起创办一家新公司，专注于深度学习引导的程序合成，并招募团队成员。
  * **ARC-AGI 及其后续基准:** ARC-AGI 不是最终目标，而是研究工具和指南针，引导研究方向。 未来需要新的基准来持续推动人工智能的发展。
  * **编程的民主化与自然语言指令:** 未来的编程将通过自然语言指令实现民主化，任何人都可以根据领域知识自动化流程。 系统将处理复杂性，并与用户协作以澄清模糊之处。
  * **LLM Agent 的可靠性问题:** LLM Agent 的可靠性受制于 LLM 本身的不可靠性。 串联多个猜测会增加出错的概率，因此需要提高LLM的可靠性和自主性。
  * **O1 的工作机制推测:** O1 可能通过在思维链空间中进行搜索来生成自然语言程序，并在测试时进行适应。 这代表了对经典深度学习范式的突破。
  * **ARC-2 人类测试:** ARC-2 将进行人类测试，以评估人类解决问题的难度和能力，并探索智力中的 g 因素和专业化之间的关系。 测试结果显示存在更聪明的人，但在任务类型上专业化程度较低。

## François Chollet简介

弗朗索瓦·肖莱（François
Chollet）是谷歌的一位人工智能研究员，也是Keras深度学习库的创作者。他以其在深度学习领域的贡献，特别是对易用性和可访问性的关注而闻名。肖莱认为，深度学习不应该只局限于少数专家手中，而是应该成为更广泛的开发者群体可以使用的工具。

ARC-AGI（Abstract and Reasoning Corpus for Artificial General
Intelligence）是由深度学习领域知名人物弗朗索瓦·肖莱（François Chollet）倡导的AI测试基准。
以衡量人工智能在未知任务上技能习得的效率。

## 对话节选

### 计算和准确性的对数关系

**主持人：**
从哲学上讲，你认为完成一项任务所需的计算量之间总是存在相应的关系吗？我的意思是，当我们使用语言和认知工具时，我们可能不认为自己在使用计算，但宇宙可能已经花费了大量计算才使这些东西出现。所以在某种程度上，我们真的有可能压缩我们使用的计算量吗？

**弗朗索瓦·肖莱：**
我认为是可能的。我认为人类的计算效率非常高。你可以从以下事实中看到这一点，例如，你在排序任务；例如，你可以对整个私有测试集进行排序，并且基本上只消耗少量卡路里。

你可能会说，好吧，这是因为，你知道，我们只是大脑的每次操作使用的能量非常少。但事实并非如此。例如，如果你比较晶体管和神经元，你会发现神经元比晶体管的耗能高得多。

只是恰好我们设法使用相对少量的神经去极化来解决极其困难的问题。与当前的人工智能相比，我们的能源效率非常高，当我们达到类似的财务效率水平时，我们将拥有通用智能。

**主持人：** 你如何看待使用像 Python 这样的图灵完备的编程语言，而不是在这些方法中使用 DSL？

> ★
>
> DSL，即特定领域语言（Domain-Specific Language），是一种专注于某个特定领域问题的编程语言或标记语言。 它与通用编程语言（如
> Java、Python）不同，通用语言可以应用于各种问题，而 DSL
> 只针对特定领域，例如：网页设计（HTML、XML）、数据库查询（SQL）、正则表达式等等。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBOswZEtzgq2kSyM0wVFaKvB9tgWbz2YoUMYUo7RHWobkibKSoT6C8ysfibUriae7Om8xAltxcZGWv8hQ/640?wx_fmt=jpeg&from=appmsg)

### 为未来任务学习新的构建模块

**弗朗索瓦·肖莱：**
嗯，我认为使用像DSL这样的东西，比如用在ARC上，从根本上来说是有限制的。无论你做什么，无论你使用什么基础语言，你都应该能够从你拥有的数据中学习你正在应用的函数。事实上，你应该能够把这作为一个终身过程。

每次你发现一个新的任务并解决它时，在这个过程中，你都会产生有用的抽象概念，也许这些抽象概念与你过去遇到的问题有关。所以你会想把它变成可重用的函数、可重用的构建模块，并存储它们，这样下次遇到类似问题时，你就可以重新应用相同的构建模块，节省计算资源，用更少的步骤解决同样困难的问题。

所以无论你做什么，你都想学习你将要使用的语言。当然，这可能意味着学习DSL。这也可能意味着使用像Python这样的东西，但在其中，越来越多地编写高阶函数、类和其他合理的构建模块来丰富你的语言。

### 意识作为迭代推理中的自我一致性机制

**主持人：**
我想知道你是否稍微软化了你的立场。所以你在推特上说，模糊模式匹配在迭代足够多次时，很有可能渐近地变成推理。甚至人类也可能基本上是以这种方式进行推理的，但这并不意味着这是进行推理的最佳方式。这是你的立场发生了转变吗？

**弗朗索瓦·肖莱：** 和什么立场相比？我不认为这是转变。

**主持人：**
我想，这首先是在说，我们有可能以这种方式思考。因为我本以为，根据你对事物的某种“拼写键”观点，你会认为我们进行的是高层次的推理，而不是模糊匹配。

**弗朗索瓦·肖莱：**
我认为人类大脑中基本的认知单元实际上是模糊模式认知。然后，当你做一些更类似于推理或计划的事情时，**当你基本上进行系统二处理，即缓慢、逻辑、一步一步的处理时，你真正做的是以结构化的形式迭代地应用你的直觉**
。顺便说一句，这正是深度学习引导的程序合成的意义所在，这是我自2017年以来一直倡导的方法。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBOswZEtzgq2kSyM0wVFaKvBcP9QmPic4LRibsfMxnnazYgtcDx7C99pgCc7b8MoYhhmbqybcW5qBYkg/640?wx_fmt=jpeg&from=appmsg)

当你进行深度学习引导的程序合成时你在做什么？你正在构建一个程序，基本上是操作符的图，但你正在一步步地构建它。在每一步，当你选择在图中编辑什么、添加什么或做出什么分支决策时，你都在应用你的直觉；你正在应用一个深度规划模型提供的猜测。所以你正在迭代地猜测，以创建这种高度结构化、符号化、离散的产物，也就是这个程序。

当你运行这个程序时，系统二的处理，我认为，基本上就是我们人类实现系统二的方式。我们正在迭代地猜测并应用模糊模式认知来构建一个实际上具有符号性质的产物。例如，假设你在下棋。当你在脑海中计算时，你正在一步步地展开一些动作，但你只会对棋盘上的一些动作这样做。你如何选择考虑哪些动作？你正在应用模式认知。

当你模拟未来的一个动作时，你不会模拟整个棋盘；你会专注于特定的区域。同样，那是模式识别。有时，任何形式的模式识别本质上都是一种猜测，所以它在某种程度上可能是错误的。当你在下棋时进行计算时，你对棋盘未来状态的一些直觉可能是错误的，然后你走了一步棋，意识到“哦，糟糕”。我认为这基本上就是人类如何实现系统二的。

### 调和符号主义和连接主义的观点

这并不是我的立场转变；我思考这些想法已经很长时间了。事实上，这是我目前最喜欢的关于如何解释意识的理论的基础。这个想法是，**对于像系统二这样的东西从迭代的模糊模式认知中产生，这种迭代序列需要高度自我一致。你添加的每一项都需要经过双重检查，以确保它与之前的内容相匹配。**

如果你只是在没有任何保护措施的情况下迭代地进行模式匹配，你基本上就是在产生幻觉；你正在做梦。顺便说一句，这正是当你处于梦境状态时发生的事情。你只是反复地凭直觉感知接下来会发生什么，而不考虑与过去的连贯性或自我一致性。我认为这就是为什么大脑中任何形式的刻意逻辑处理都需要涉及意识的原因。

意识充当这种自我一致性检查。它迫使你的直觉的下一次迭代，即这个模式认知过程，与之前的所有内容保持一致。**实现这种一致性的唯一方法是通过这些来回循环，将过去带入现在，并将你对未来的预测带入现在。你在当下有这样一个焦点，这就是你所关注的。这个焦点本质上就是你的意识。**

因此，**意识是将迭代模式的认知转化为类似真正自我一致的推理的过程。**

**主持人：**
是的，所以我想问题是你没有改变你的立场。这只是人们理解你的立场的一个例子。所以我认为人们将系统二分化，你知道，就像符号主义者认为一切都是这个离散的世界。然后另一种选择，你知道，连接主义的方法是，一切都是模糊的魔法。

**弗朗索瓦·肖莱：**
我从来都不是纯粹的符号主义阵营。如果你回顾我早期关于程序合成的著作，我当时说我们需要深度学习引导的程序合成。我们需要直觉和模式与认知以及离散的一步步推理和搜索的融合，将它们融入到单一的数据结构中。

在过去的八年左右的时间里，我一再强调，**人类的认知实际上是直觉和推理的混合体，如果你只依赖其中一种，你不会有太大的进展。你需要由向量空间、一般的深度规划模型提供的连续性抽象，以及由图和离散搜索提供的更离散的符号化抽象**
。

**主持人：** 那么，为什么你的反对者认为你是符号主义者，而你显然不是？

**弗朗索瓦·肖莱：**
嗯，我不确定。我研究深度学习很长时间了，基本上从2013年开始。我大约在2014年开始大力推广深度学习。那时候，这个领域还很小。特别是在混沌理论方面，我认为我为普及深度学习做了很多工作，使其尽可能地让更多人更容易接触到。我一直都是一个深度学习爱好者，对吧？当我开始思考深度学习的局限性时，我并不是想着用完全不同的东西来取代深度学习。我当时的想法是用符号元素来增强深度学习。

### 系统二的推理需要意识和一致性

**主持人：** 你在关于意识的那条推特中也评论过。你认为所有的系统二处理都涉及意识。请进一步解释你是什么意思。

**弗朗索瓦·肖莱：**
任何形式的明确的、一步一步的推理都需要涉及意识。反过来说，如果存在任何你无意识运行的认知过程，它就不会有这种强大的自我一致性保证。它会更像一个梦，一种幻觉。

基本上，这个想法是，如果你只是迭代地猜测，除非你拥有这种强大的自我一致性保证，否则你最终会漂移；你最终会发散。意识是自我一致性的护栏，这也是为什么你不能拥有一个没有意识的系统。

**主持人：** 你对推理的定义是什么？

**弗朗索瓦·肖莱：**
我并没有对推理的单一明确的定义。我认为这是一个含义很丰富的术语，你可以用它来指代很多不同的东西。但至少有两种我看到这个术语被使用的方式，而且它们实际上非常不同。

例如，如果你只是在记忆一个程序，然后应用这个程序，你可以说这是一种推理形式。比方说，在学校里你正在学习乘法的算法。当你学习这个算法时，然后当你参加测试时，你实际上是在应用这个算法。这算是推理吗？我认为是的，这是一种推理形式。**这是大型语言模型和深度学习模型特别擅长的那种推理。你是在记忆一种模式，在那时，你是在提取一种模式并重新应用它。**

**但是另一种推理形式是当你面对你从未见过的事物时，你必须将你拥有的认知构建块——你的知识等等——重新组合成一个全新的模型，并且即时完成。这也是推理，但它是一种非常不同的推理，它内化了非常不同的能力。**

所以我认为关于深度学习模型，特别是大型语言模型，重要的问题不是它们是否能推理。在某种意义上，它们总是在进行推理。更重要的问题是：它们能否适应新事物？有很多不同的系统可以简单地记住人类提供的程序，然后复制它们。更有趣的是它们是否能够即时提出它们自己的程序和它们自己的抽象概念。

### 新颖问题解决、抽象和可重用性

**主持人：** 那么，一个系统提出它自己的抽象概念意味着什么？

**弗朗索瓦·肖莱：**
要开始提出抽象概念，首先，你需要解决新的问题。解决一个新的问题意味着你从一些知识库，一些构建块开始。然后你面临一个新的任务；你将它们重新组合成一个任务模型，你应用这个模型，它奏效了。

在这个过程中，当你解决许多问题时，你会开始注意到你拥有的构建块的某些重组模式经常发生。当你开始注意到这一点时，这意味着你可以提取它们并将它们抽象化，将它们重构为更可重用的形式。

然后你可以将这种可重用的形式添加回你拥有的构建块集合中。所以下次你遇到类似的问题时，你将能够用更少的步骤解决它，消耗更少的能量，因为你可以访问适合这个问题的更高层次的抽象概念。

**主持人：** 有没有办法衡量推理的强度？

**弗朗索瓦·肖莱：**
所以，同样，你需要首先精确地定义你所说的推理是什么意思。我认为你可以精确地定义，例如，你可以精确地定义泛化能力，这基本上是你能够适应的新事物的数量。

**主持人：**
是的，这很有趣，因为我想你把它定义为性能，就像我的模型有多好。另一种描述方式是，让我们假设我们将推理纯粹地视为遍历演绎闭包，即将我们已经拥有的知识以新的配置组合在一起，然后我们在解决方案空间中实现飞跃，因为我们找到了一个非常非常有效的新的模型。是否有衡量模型类型本身的内在方法，而不是其泛化能力？

**弗朗索瓦·肖莱：** 不，我认为你真的必须观察模型做了什么。你不能仅仅检查模型就告诉你它在推理方面有多强。

**主持人：** 所以没有内在的衡量标准来说这是好的推理。

**弗朗索瓦·肖莱：**
例如，给定一个问题的两个模型，哪个模型更好？你能不能只看看它们就说出哪个更好？我认为这很大程度上取决于目标。你不能真正评估一个模型，比如对一个事物的模拟，如果你没有想要用它来做什么。但是如果你有一个目标，那么你只需查看实现该目标所需的因果因素。然后最好的模型可能是保留这些因果因素的最简单的模型。

### François离开谷歌专注于程序合成

**主持人：** François，你接下来打算做什么？

**弗朗索瓦·肖莱：**
嗯，我几周前刚离开谷歌。所以我要和一个朋友一起创办一家新公司，一个新的研究实验室。我目前还不能透露太多，但我们将致力于程序合成，特别是深度学习引导的程序合成，我们目前正在组建团队。

**主持人：** 太棒了。你们在招人加入吗？

**弗朗索瓦·肖莱：** 是的。

**主持人：** 多告诉我一些。你允许告诉我们什么？

**弗朗索瓦·肖莱：**
所以，是的，我的意思是，我可以告诉你的是，你知道，我一直在谈论一些问题，比如实现AGI的最佳途径，关于深度学习引导的程序合成，关于围绕ARC-
AGI的方法，关于ARC-
AGI之后的下一个基准可能是什么。所以，我一直在思考这个问题，就像在谷歌工作时的副业一样，我在谷歌的全职工作是开发Keras，对吧？现在我觉得是时候全身心地专注于研究问题了，所以让研究不再是一个副业，而是主要的事情。

**主持人：** 那么重点是创建新的基准，还是超越基准？

**弗朗索瓦·肖莱：** 我认为两者都有。我非常相信你需要将解决方案与问题共同进化的观点。这实际上也是最初创建ARC-
AGI的动机，是为了有一个正确的挑战，迫使你专注于正确的问题，即用人工智能实现强泛化的主要瓶颈。所以我不认为ARC-AGI是最后一个基准。当然，会有ARC-
AGI的V2版本，但那也不是最后一个基准。我认为我们总是需要新的基准，探索对人工智能来说很难但对人类来说很容易的新事物。

**主持人：** 如果你自己做自己的基准测试，这在某种程度上算作弊吗？

**弗朗索瓦·肖莱：** 我不这么认为。基准是作为研究工具而存在的。再次强调，像ARC-
AGI这样的东西，并不是真的作为一个二元指标告诉你，哦，我们有没有AGI？它真的被认为是一个研究工具。这是一个挑战，迫使你致力于正确的问题，引导你关注正确的问题，并帮助你取得进展。所以你可以说ARC-
AGI基本上是指向AGI的指南针。它不像是一个对AGI的测试。

**主持人：** 并且存在各种可能的解决方案。例如，因为你知道私有测试中的内容，你可以直接把答案放进去。

**弗朗索瓦·肖莱：** 然后...当然，但是我不会以任何方式参加ARK Prize。我的意思是很明显不会。对吧。反正我都在运营ARK
Prize了，我为什么要参加呢，你知道吗？

**主持人：** 但在泛化这个光谱上，有登月计划，也就是追求极端的泛化，或者比它低一个档次的。你们的目标是什么？

**弗朗索瓦·肖莱：** 我想构建通用人工智能（AGI）。我想构建具有人类水平能力的东西。

**主持人：** 那意味着什么？你认为用那种AGI可以实现什么？

**弗朗索瓦·肖莱：**
嗯，最明显的就是解决编程问题。如果你解决了通用人工智能，那么你就可以直接向计算机描述你想要什么，然后计算机就会为你构建它。如果它真的是通用人工智能，那么它就会扩展到与人类软件工程师所能达到的代码复杂程度相同的水平。而且，它可能也不会止步于此。

**主持人：** 你认为，当我们开始达到那种水平时，人类在软件工程中的作用是什么？

**弗朗索瓦·肖莱：**
嗯，我会观察。我认为，**当这项技术准备就绪时，我们将开始创建全新的工具和全新的界面来使用它。我们离那还很远。我们正在谈论的是一些尚未完全存在的东西。我认为前沿模型，甚至包括O1，都没有达到那个水平**
。

### 编程民主化与自然语言指令

**主持人：** 现在程序员是非常技术性的。你认为编程在未来可能会民主化吗？

**弗朗索瓦·肖莱：**
我认为会，是的。**我认为在未来，任何人应该都能够基于他们自己对所面临问题的领域专业知识，开发自己的自动化流程。每个人都应该能够编程，但不是真正意义上的编写代码，而是向计算机描述他们想要自动化什么以及如何自动化，然后计算机就会自动完成。**

**主持人：**
软件中的一个重要问题是在不同规模上应对复杂性。这当然是你整个职业生涯一直在做的事情。你认为我们是否会一直面临这个问题，它将始终处于这种难以置信的复杂性的边界上？

**弗朗索瓦·肖莱：** 你是什么意思？

**主持人：** 嗯，就像，即使我们把层级向上民主化一两个步骤，我们不还是会始终构建出非常复杂的软件吗？

**弗朗索瓦·肖莱：** 是的，很有可能。但我们的想法是，我们将能够把这种复杂性卸载到一个外部的复杂性处理AI上。

**主持人：** 所以我们将进入一个我们不再理解正在运行的代码的未来。

**弗朗索瓦·肖莱：** **在任何方面都不理解** 。 绝对的。 你知道，我认为在很大程度上，这已经是真的了。
如果你看看任何大型的代码库，实际上没有哪个软件工程师完全理解它。
由于我们对我们正在做的事情的理解是零碎的，我们总是受到限制，只要我们对系统的高级目标和约束有很好的把握，这就没问题。

**主持人：**
那么，应该在哪里找到能动性的来源呢？因为你知道，你描述的是一个盲人摸象式的挑战，其中大量的开发人员对系统的很小一部分有自己的看法。但是，当我们有了AGI版本时，这种情况会发生怎样的改变？

**弗朗索瓦·肖莱：** 总的来说，我认为从输入-
输出对进行编程将是未来一种普遍的编程范式，这将对所有人开放，因为你不需要编写任何代码。你只是指定你希望程序做什么，然后计算机就会自己编程。

顺便说一下，如果你所表达的意思有任何含糊之处，而且总会有含糊之处，特别是当指令是由非技术用户提供时，你也不必担心，因为计算机会要求你澄清。

如果你说，好吧，我基本上根据你告诉我的创建了最合理的程序，但是这里那里有一些含糊之处。那么，这个输入呢？目前我得到了这个输出。看起来对吗？你想改变它吗？

当你迭代地改变它时，你正在与计算机协作创建这个正确的程序。

**主持人：** 所以这些未来的系统将把程序合成作为核心组件，一个显式的组件。但是人类将如何与它交互呢？我们仍然会用自然语言、手势、图像等等来描述吗？

**弗朗索瓦·肖莱：**
它可以是自然语言。它也可以只是在你的屏幕上绘制界面元素。它可以是，你知道，你总是可以尝试生成一个正在生成的高级程序表示，该表示处于非技术用户可以可视化和理解的水平。例如，我可以展示一个数据流图。

**主持人：**
非常酷。那么，大规模的LLM的一个特点是，在某种意义上，它并不那么困难，因为它们只需要像CDN一样，复制所有的权重，然后把它们移动到各处。你描述的东西非常复杂。它可能有点类似于一个全球分布式数据库，其中的技能程序在所有不同的节点上移动。我们需要为此构建一种全新的架构吗？

**弗朗索瓦·肖莱：**
是的，**我认为我们需要一种全新的架构来实现终身分布式学习，其中基本上有许多相同的AI实例，并行地为不同的人解决许多不同的问题，并寻找问题之间的共性以及解决方案之间的共性。每当他们发现足够的共性时，他们就会将这些共性抽象成一个新的构建块，该构建块将返回到系统中，并使该系统更强大、更智能。**

**主持人：** 我想我现在明白了。所以你正在构建的是一个基于我们找到ARC问题良好解决方案的全球分布式ARC结构。

**弗朗索瓦·肖莱：** 嗯，我不能确切地告诉你我们在构建什么，但它会很酷。

### 基于搜索的思维链与标准前向传递

**主持人：** 是的，听起来很酷。你认为像OpenAI这样的人将如何开始将不仅是测试时的推理，而且还将你的一些想法切实地纳入到他们的系统中？

**弗朗索瓦·肖莱：**
例如，前沿模型如何整合程序合成？嗯，你知道，我认为像O1这样的模型已经在精确地做这件事了。**当你观察O1可能正在做什么时，它正在编写自己的自然语言程序，描述它应该做什么，并且它正在执行这个程序本身。它编写这个程序的方式是一个非常复杂的搜索过程。所以这已经是自然语言空间中的程序合成了。**

你可以通过其他方式利用程序合成。你可以在潜在空间中进行程序合成，可能类似于Clermont-
Bunet和他的朋友们正在做的事情。你也可以生成实际的程序；有时，你可能希望使用实际的编程语言而不是自然语言。

我认为我们肯定看到了一种趋势，即越来越多地利用测试期的计算能力，而且这种趋势将会加速。这是一个根本性的趋势.

**主持人：** 这不是主动微调，这在架构上有点困难，不是吗？因为我的模型总是在被微调。所以我可以想象他们可能会构建一些类似 Docker
的东西，有一个基础层，然后是我的微调层，再是另一个微调层，它变得非常碎片化。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBOswZEtzgq2kSyM0wVFaKvB1icm7CJUBq17m9m4PhOoWBJ8fspPicib7PDo2jtqlP6qPcQcwkWnRuesw/640?wx_fmt=jpeg&from=appmsg)

**弗朗索瓦·肖莱：**
在实际的前沿模型中应用测试时训练的难点，不在于基础设施。当然，目前的服务基础设施绝对不是为每个任务进行微调而设置的。但是你可以重新设计它来实现。主要的瓶颈实际上是任务格式。只有在有非常明确的输入和目标时，才能进行测试时训练。你基本上需要输入-
输出对。对于大多数提示，你没有这些。你当然在ARC中有，但对于大多数提示，你没有。

### o1 的自然语言程序生成和测试时计算缩放

**主持人：** 非常酷。你对O1的工作原理有什么理论吗？

**弗朗索瓦·肖莱：** 嗯，我们只能推测。我不太确定它究竟是如何工作的。但似乎正在发生的事情是，它正在可能的思维链的空间中运行一个搜索过程。

**尝试评估树中哪些分支效果更好，如果当前分支没有进展，可能会回溯和编辑，最终得到一个非常长、不复杂，并且可能接近最优的思维链。这个链条基本上代表了一个自然语言程序，描述了模型应该做什么。**

**在创建这个程序的过程中，模型正在适应新颖性。我认为像 O1
这样的东西在这些系统可以实现的泛化能力方面是一个真正的突破。我们已经远远超出了经典的深度学习范式。**

**主持人：** 所以一种观点，我认为你也同意，是说在推理时存在某种主动控制器，所以它实际上是在一个隔离的环境中进行多次轨迹探索。

**弗朗索瓦·肖莱：** 它在进行搜索。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBOswZEtzgq2kSyM0wVFaKvBzuGoEkmHiboictCtHfWpBvDTec5O3cWyVpAGWInDPd9Lfa6clgeicSAwA/640?wx_fmt=jpeg&from=appmsg)

**主持人：** 好的，因为有些人认为，它在训练时有过程监督等等，但在推理时，它只是一条前向路径。

**弗朗索瓦·肖莱：** 不，这绝对不可能，因为测试时消耗的计算量。它非常明显地在测试时进行搜索。

**我认为它在训练时被训练来重现当前问题的最佳可用思维链。有点像 AlphaZero
风格的训练。但它也在测试时在思维链空间中进行搜索。这从明显的迹象中可以看出来，即它消耗的计算量，比如 token 的数量和延迟。**

**主持人：**
还有其他什么明显的迹象表明这种情况正在发生吗？例如，它可能探索了某个特定区域，然后在整合后，该区域现在消失了。所以当你与模型交谈时，它几乎就像忘记了它思考的部分。

**弗朗索瓦·肖莱：** 是的，老实说，这有点太具体了。你知道，我没有任何关于 O1在做什么的内部信息，所以我只能推测。

### 更深层次的搜索带来的对数收益

**主持人：** 好的。所以像 Noam Brown 这样的人，他们对测试时这种新的缩放规律非常乐观。当然，我喜欢 O1
Pro。我认为它真的非常好，在质量上是一个很大的改进。你对此怎么看？

**弗朗索瓦·肖莱：**
当然。所以测试时缩放规律基本上是这个观察结果：如果你扩大更多的计算量，如果你搜索得更深入，你会看到准确性相应地提高。而且这种关系是对数的。

因此，准确性随着计算量的增加而对数地提高。虽然这并不是什么新鲜事，就像任何时候你进行测试时搜索，你都会看到这种关系。

例如，如果你正在进行蛮力程序搜索，你会发现你解决问题的能力随着计算量的增加而对数地提高。如果你有更多的计算量，你就可以在可能的程序空间中搜索得更深入，并且以对数方式找到更多的解决方案。

因此，任何时候你进行任何类型的测试时搜索，你都会看到这种关系。

**主持人：** 你目前首选的模型是什么？你用它来做什么？

**弗朗索瓦·肖莱：** 在大多数情况下，我使用 Gemini Advanced。而且我刚刚开始使用新的 Gemini Flash，最新版本。所以我付费使用
Gemini Advanced。我也在使用 Claude 3.5。我认为它非常适合编程。所以这是我正在使用的两个。

**主持人：** 你使用 LLM 的编程工作流程是什么？

**弗朗索瓦·肖莱：** 我在编程时并不经常使用 LLM。但通常，如果我当前面临的问题我认为可能很适合 LLM，我就会直接在浏览器中打开
LLM，并要求它提供一个可以执行 X、Y、Z 的函数。

虽然通常第一次尝试并不能真正成功，但在经过一些调试和引导后，我认为这可以节省大量时间。

**主持人：** 你在编程时看到了哪些类型的故障模式？

**弗朗索瓦·肖莱：** 你是说使用 LLM 吗？嗯，所以故障模式会因你使用的模型而有所不同，对吧？我认为一般来说，Claude 3.5
是最好的。是的。但有时，你可能会得到一些毫无理由存在的代码，比如没有使用的变量，或者代码基于的假设与传入的数据不符。所以很明显，没有代码是根据统计可能性生成的。它没有努力使自己保持一致、正确，并事先尝试执行它，等等。所以我认为那里实际上有很大的改进空间。你可以想象未来基于
LLM 的软件开发助手，它们可以在你提示它们时实际完成所有这些事情，它们实际为你编写代码，然后尝试在实际展示给你之前自己调试它。

### LLM 作为猜测机器和 Agent 的可靠性问题

**主持人：** 你对目前非常流行的 LLM Agent 系统有什么看法？

**弗朗索瓦·肖莱：** 是的，Agent 已经流行很长一段时间了。人们大约在一年半前左右就开始谈论 Agent 是未来，但到目前为止，Agent
并没有真正兴起。

所以 Agent 有一些... 所以这里根本的问题是 LLM 并不是很可靠。如果你看 LLM 的一次运行结果，那基本上是... 你可以将 LLM
看作一个猜测机器。它做出的猜测显然比随机猜测好得多。它们是非常有用的猜测。它们在方向上是准确的，但它们有一定的出错概率。

**当你查看 Agent
工作流程时，你正在串联许多这些猜测。随着你像这样串联更多的猜测，你最终不会达到你想要的目标的可能性会急剧增加。这是很大的瓶颈。Agent
只是不可靠。它们没有足够的自主性。**

人们说，随着模型变得更好，这个问题会得到解决。我认为这是一个经验问题。我正在等待看到 Agent 工作流程何时真正开始工作。我不认为我们今天已经到了那一步。

**主持人：**
我在这件事上的立场稍微软化了一点。我同意，由于歧义问题，它们被误导了。当把这些串联起来时，它们会被严重误导。但是，手头有更多的计算能力还是有一定好处的。所以我采访了那些写了AI科学家论文的人。当然，如果你用Claude
Sonnet
3.5，然后说，帮我生成一篇完整的论文，那结果会平淡无奇，简直就是个草稿。**而他们所做的是将问题分解成许多代理工作者，就像谷歌地图的比喻一样，不断地放大、放大、再放大，反复多次。这样一来，效果就显著提高了。**

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBOswZEtzgq2kSyM0wVFaKvBKkGDibWbLmg98LEQz8xo7G75UYE2OIVU2Ud8LicefAQdC0uLowH2kV7w/640?wx_fmt=jpeg&from=appmsg)

###  ARC-2 人类测试与 g 因素的相关性

**弗朗索瓦·肖莱：** 是的，这很有道理。我认为这基本上印证了我们之前讨论过的观点，即系统 2 就像带有强约束的迭代系统
1。而这些约束非常重要。在这种情况下，上层结构是由人类程序员提供的。人类程序员将问题分解为正确的子问题，并以正确的方式协调整个过程。然后，每个子问题实际上都可以通过猜测并产生足够好的猜测来解决。

**主持人：** 太棒了。ARC-2 要来了。

**弗朗索瓦·肖莱：**
可能在明年初，我们目前正在完成人类测试。正如我提到的，每个谜题都将由我们知道可以解决的一群人来解决，并且我们有一些数据来判断对于普通人来说它有多难。我们的目标是拥有三组难度经过校准的题目。因此，如果你在公开评估中获得分数，并且没有对该数据集过度拟合，那么你可以非常确信你在其他两组数据集中也会获得非常相似的分数。

**主持人：** 当你们进行人类测试时，你们在关于智力的衡量标准中写道，关于智力的一个学派认为存在一个 G
因素。而另一个学派认为智力是非常专业化的。你们的实验是否揭示了，一组人类在所有任务中表现都相当出色？或者你们看到了巨大的专业化差异？

**弗朗索瓦·肖莱：** 不，绝对是存在一些人更聪明，他们更擅长解决难题。你在人类测试数据中确实看到了这一点。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/ClW8NejCpBOswZEtzgq2kSyM0wVFaKvBjicbuSulTEQiahMhsz8AKiby1qMhyiayViczsn8e9qtoyYwJia0OBkTAZh5w/640?wx_fmt=jpeg&from=appmsg)

**主持人：** 那么在长尾部分呢？你是否看到任务类型上的专业化，还是说它相当平坦？

**弗朗索瓦·肖莱：** 我认为它相当平坦。我的意思是，你要么擅长，要么不擅长。

**主持人：** 有意思。Francois Chollet，非常荣幸能邀请你来到节目。非常感谢。

**弗朗索瓦·肖莱：** 非常感谢邀请我。很高兴能来到这里。

**关注公众号后设🌟标，掌握第一手AI新动态**

##  往期精选

  1. [黄仁勋专访：OpenAI在大模型混战中达到“逃逸速度”](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2650001718&idx=1&sn=f8129a622e7611702be2cb23e8ce9418&chksm=88ba5831bfcdd127d06ce6492c821074407f805407b4182ca900916521cb5a4717f2a3d71ee8&token=1339625777&lang=zh_CN&scene=21#wechat_redirect)
  2. [李飞飞与Justin深度解读空间智能：数字世界需要三维表征，才能与现实世界融合](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2650000659&idx=1&sn=c71fb5b4ef501424dddd5e8b4dd5860e&chksm=88ba4414bfcdcd023c691a1adf75127a9fd883ceb305ca14cf97f719acaf999d40fa72f84bf3&token=1492077842&lang=zh_CN&scene=21#wechat_redirect)
  3. [PayPal创始人彼得·蒂尔：人类科技停滞源于原子方面的进展远慢于比特](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2650000240&idx=1&sn=26af6013981677b1e14137260857a6f0&chksm=88ba4277bfcdcb615d746615c262927bf51c43c920ed93fa36274ef87c6264d6548c84647121&token=106920805&lang=zh_CN&scene=21#wechat_redirect)
  4. [谷歌联合创始人布林：巨头们打造的“上帝模型”几乎可以理解一切](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2649999870&idx=2&sn=0c714d804a72a52e002743d949e1685e&chksm=88ba40f9bfcdc9ef78749718480265922f4fba539abf6c9d62a6cd681f405dee9283d2429f84&token=106920805&lang=zh_CN&scene=21#wechat_redirect)
  5. [马斯克：AI将使商品和服务的成本趋近于零](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2649999870&idx=1&sn=752f000a117a705e77950c82bfc4a004&chksm=88ba40f9bfcdc9ef5a5afe4a3ae73d5247bd54ed525dbdbedee1fcf74a6c082165e664a5c4d0&token=106920805&lang=zh_CN&poc_token=HDp86Waj18SFm2Y-xnv_Vqd_4J6emFoh10eH48wg&scene=21#wechat_redirect)
  6. [Karpathy最新专访：人形机器人、特斯拉、数据墙与合成数据](https://mp.weixin.qq.com/s?__biz=MzA5NTU4NDM2MA==&mid=2649999613&idx=1&sn=b8bdda7afe4c3ca08e324ac5bbd5a2bd&chksm=88ba41fabfcdc8ec0e21dbf4c7eb4d33252da70f47e1cfc9f5e113717911c417c2aebb3d6180&token=106920805&lang=zh_CN&scene=21#wechat_redirect)

  

